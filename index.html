<html><head><meta charset='utf-8'><title>Keyword Plot</title></head><body><div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>                <div id="9b38e936-24ca-4b87-b2f1-59964be83b38" class="plotly-graph-div" style="height:700px; width:3200px;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("9b38e936-24ca-4b87-b2f1-59964be83b38")) {                    Plotly.newPlot(                        "9b38e936-24ca-4b87-b2f1-59964be83b38",                        [{"hoverinfo":"text","marker":{"line":{"width":0}},"text":["\u003cb\u003eQuestion Answering\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eEvaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection\u003cbr\u003eBlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering\u003cbr\u003eLookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps\u003cbr\u003eAligning Language Models to Explicitly Handle Ambiguity\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003eQUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios\u003cbr\u003ePhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study\u003cbr\u003eSEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation\u003cbr\u003e... and 155 more","\u003cb\u003eText Classification\u003c\u002fb\u003e\u003cbr\u003eOn Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003eText Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification\u003cbr\u003eIncubating Text Classifiers Following User Instruction with Nothing but LLM\u003cbr\u003eAdaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks\u003cbr\u003eThe Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse\u003cbr\u003eFine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates\u003cbr\u003ePERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts\u003cbr\u003eThe Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples\u003cbr\u003eStablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models\u003cbr\u003e... and 58 more","\u003cb\u003eLanguage Modeling\u003c\u002fb\u003e\u003cbr\u003eLanguage Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS\u003cbr\u003eRethinking Token Reduction for State Space Models\u003cbr\u003eInstruction Pre-Training: Language Models are Supervised Multitask Learners\u003cbr\u003eLeading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Probabilities\u003cbr\u003eWhen Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages\u003cbr\u003eKidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions\u003cbr\u003eContext-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models\u003cbr\u003eCItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling\u003cbr\u003eMitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing\u003cbr\u003eCan Large Language Models Learn Independent Causal Mechanisms?\u003cbr\u003e... and 54 more","\u003cb\u003eSentiment Analysis\u003c\u002fb\u003e\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eOvercome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue\u003cbr\u003eCryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading\u003cbr\u003eDA\u00b3: A Distribution-Aware Adversarial Attack against Language Models\u003cbr\u003eAdaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eFine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates\u003cbr\u003eDoes Large Language Model Contain Task-Specific Neurons?\u003cbr\u003eGetting More from Less: Large Language Models are Good Spontaneous Multilingual Learners\u003cbr\u003eMOSEL: Inference Serving Using Dynamic Modality Selection\u003cbr\u003e... and 53 more","\u003cb\u003eVisual Question Answering\u003c\u002fb\u003e\u003cbr\u003eExploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors\u003cbr\u003eMAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering\u003cbr\u003eSelf-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering\u003cbr\u003eTag-grounded Visual Instruction Tuning with Retrieval Augmentation\u003cbr\u003eDocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models\u003cbr\u003eWorld to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering\u003cbr\u003eFrom the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eHow Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?\u003cbr\u003eBenchmarking Vision Language Models for Cultural Understanding\u003cbr\u003e... and 47 more","\u003cb\u003eSummarization\u003c\u002fb\u003e\u003cbr\u003eLONGEMBED: Extending Embedding Models for Long Context Retrieval\u003cbr\u003eReuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment\u003cbr\u003eLookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps\u003cbr\u003eDancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models\u003cbr\u003eBayesian Calibration of Win Rate Estimation with LLM Evaluators\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eOptimized Speculative Sampling for GPU Hardware Accelerators\u003cbr\u003eA Survey of AMR Applications\u003cbr\u003eEAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees\u003cbr\u003eEvaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works\u003cbr\u003e... and 45 more","\u003cb\u003eNatural Language Inference\u003c\u002fb\u003e\u003cbr\u003eEffective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process\u003cbr\u003eLogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models\u003cbr\u003eFuseGen: PLM Fusion for Data-generation based Zero-shot Learning\u003cbr\u003eIn Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search\u003cbr\u003eQUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios\u003cbr\u003eVerification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving\u003cbr\u003eHow Hard is this Test Set?\nNLI Characterization by Exploiting Training Dynamics\u003cbr\u003eScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws\u003cbr\u003eAdaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks\u003cbr\u003eHow Do Humans Write Code? Large Models Do It the Same Way Too\u003cbr\u003e... and 44 more","\u003cb\u003eCommonsense Reasoning\u003c\u002fb\u003e\u003cbr\u003eParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eTeaching Small Language Models Reasoning through Counterfactual Distillation\u003cbr\u003eInvestigating Mysteries of CoT-Augmented Distillation\u003cbr\u003eFocused Large Language Models are Stable Many-Shot Learners\u003cbr\u003eMixture-of-Subspaces in Low-Rank Adaptation\u003cbr\u003e... and 43 more","\u003cb\u003eMachine Translation\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?\u003cbr\u003eChain-of-Dictionary Prompting Elicits Translation in Large Language Models\u003cbr\u003eWord Alignment as Preference for Machine Translation\u003cbr\u003eVoices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects\u003cbr\u003eAligning Translation-Specific Understanding to General Understanding in Large Language Models\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eAn Audit on the Perspectives and Challenges of Hallucinations in NLP\u003cbr\u003eA Survey of AMR Applications\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003eLexically Grounded Subword Segmentation\u003cbr\u003e... and 38 more","\u003cb\u003eText Generation\u003c\u002fb\u003e\u003cbr\u003ePrompts have evil twins\u003cbr\u003ePromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval\u003cbr\u003eKidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions\u003cbr\u003ePERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003eLLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives\u003cbr\u003ePrecise Model Benchmarking with Only a Few Observations\u003cbr\u003eIs It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?\u003cbr\u003eUnlocking Memorization in Large Language Models with Dynamic Soft Prompting\u003cbr\u003eIf CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions\u003cbr\u003e... and 29 more","\u003cb\u003eInstruction Following\u003c\u002fb\u003e\u003cbr\u003eSuccessfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eDirect Multi-Turn Preference Optimization for Language Agents\u003cbr\u003eInstruction Pre-Training: Language Models are Supervised Multitask Learners\u003cbr\u003eDancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models\u003cbr\u003eBayesian Calibration of Win Rate Estimation with LLM Evaluators\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003eGAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities\u003cbr\u003eAutomatic Instruction Evolving for Large Language Models\u003cbr\u003eEAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees\u003cbr\u003e... and 29 more","\u003cb\u003eMathematical Reasoning\u003c\u002fb\u003e\u003cbr\u003eLearning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing\u003cbr\u003eFrom Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning\u003cbr\u003eHow Do Humans Write Code? Large Models Do It the Same Way Too\u003cbr\u003eMuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003eAutomatic Instruction Evolving for Large Language Models\u003cbr\u003eFROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models\u003cbr\u003eEAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees\u003cbr\u003eADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning\u003cbr\u003eRetrieved In-Context Principles from Previous Mistakes\u003cbr\u003e... and 23 more","\u003cb\u003eIn-Context Learning\u003c\u002fb\u003e\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eTake Off the Training Wheels! Progressive In-Context Learning for Effective Alignment\u003cbr\u003eHow do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning\u003cbr\u003eInterpretability-based Tailored Knowledge Editing in Transformers\u003cbr\u003eEXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning\u003cbr\u003eSynergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems\u003cbr\u003eFocused Large Language Models are Stable Many-Shot Learners\u003cbr\u003eLearning to Retrieve Iteratively for In-Context Learning\u003cbr\u003ePosition Engineering: Boosting Large Language Models through Positional Information Manipulation\u003cbr\u003eStrategic Demonstration Selection for Improved Fairness in LLM In-Context Learning\u003cbr\u003e... and 23 more","\u003cb\u003eCode Generation\u003c\u002fb\u003e\u003cbr\u003eAMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation\u003cbr\u003eAutomatic Instruction Evolving for Large Language Models\u003cbr\u003eEAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees\u003cbr\u003eUnlocking Memorization in Large Language Models with Dynamic Soft Prompting\u003cbr\u003eI Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses\u003cbr\u003eOuroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding\u003cbr\u003eHow Do Your Code LLMs Perform? Empowering Code Instruction Tuning with Really Good Data\u003cbr\u003eSmall Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector\u003cbr\u003eLearn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning\u003cbr\u003eECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?\u003cbr\u003e... and 22 more","\u003cb\u003eNamed Entity Recognition\u003c\u002fb\u003e\u003cbr\u003eMTLS: Making Texts into Linguistic Symbols\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eADELIE: Aligning Large Language Models on Information Extraction\u003cbr\u003eBio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eNuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data\u003cbr\u003eSciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents\u003cbr\u003eZero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages\u003cbr\u003eLarge Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark\u003cbr\u003eA Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives\u003cbr\u003e... and 22 more","\u003cb\u003eReasoning\u003c\u002fb\u003e\u003cbr\u003ePhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study\u003cbr\u003eTo Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models\u003cbr\u003eMAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration\u003cbr\u003eMore Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs\u003cbr\u003eLearning to Correct for QA Reasoning with Black-box LLMs\u003cbr\u003eText-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction\u003cbr\u003eSelf-AMPLIFY : Improving Small Language Models with Self Post Hoc Explanations\u003cbr\u003eExperimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently\u003cbr\u003eDynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models\u003cbr\u003eImprove Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation\u003cbr\u003e... and 21 more","\u003cb\u003eRag\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eKnowledge Verification to Nip Hallucination in the Bud\u003cbr\u003eEfficientRAG: Efficient Retriever for Multi-Hop Question Answering\u003cbr\u003eCrafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs\u003cbr\u003eModel Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation\u003cbr\u003ePosition Engineering: Boosting Large Language Models through Positional Information Manipulation\u003cbr\u003eImproving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning\u003cbr\u003eAGRAME: Any-Granularity Ranking with Multi-Vector Embeddings\u003cbr\u003eMiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents\u003cbr\u003eAssessing \"Implicit\" Retrieval Robustness of Large Language Models\u003cbr\u003e... and 20 more","\u003cb\u003eInformation Retrieval\u003c\u002fb\u003e\u003cbr\u003eDo Large Language Models Know How Much They Know?\u003cbr\u003eGENRA: Enhancing Zero-shot Retrieval with Rank Aggregation\u003cbr\u003eFIRST: Faster Improved Listwise Reranking with Single Token Decoding\u003cbr\u003eOmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer\u003cbr\u003eMatryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions\u003cbr\u003eExploring the Practicality of Generative Retrieval on Dynamic Corpora\u003cbr\u003eMAIR: A Massive Benchmark for Evaluating Instructed Retrieval\u003cbr\u003ePcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity\u003cbr\u003eBridging Local Details and Global Context in Text-Attributed Graphs\u003cbr\u003eLitSearch: A Retrieval Benchmark for Scientific Literature Search\u003cbr\u003e... and 20 more","\u003cb\u003eImage Captioning\u003c\u002fb\u003e\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003eUniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation\u003cbr\u003eHELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding\u003cbr\u003eTag-grounded Visual Instruction Tuning with Retrieval Augmentation\u003cbr\u003eDoes Object Grounding Really Reduce Hallucination of Large Vision-Language Models?\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eHow Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?\u003cbr\u003eFiner: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models\u003cbr\u003eUOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models\u003cbr\u003eA Survey of AMR Applications\u003cbr\u003e... and 19 more","\u003cb\u003eNatural Language Processing\u003c\u002fb\u003e\u003cbr\u003eA Survey on In-context Learning\u003cbr\u003eModel Balancing Helps Low-data Training and Fine-tuning\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003eA Generic Method for Fine-grained Category Discovery in Natural Language Texts\u003cbr\u003eChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context\u003cbr\u003eKnowledge-Centric Hallucination Detection\u003cbr\u003eToward Compositional Behavior in Neural Models: A Survey of Current Views\u003cbr\u003eIntegrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation\u003cbr\u003ePragmatic Norms Are All You Need \u2013 Why The Symbol Grounding Problem Does Not Apply to LLMs\u003cbr\u003eUnraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications\u003cbr\u003e... and 18 more","\u003cb\u003eLlm Evaluation\u003c\u002fb\u003e\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eA User-Centric Multi-Intent Benchmark for Evaluating Large Language Models\u003cbr\u003eEvaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts\u003cbr\u003eChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context\u003cbr\u003eMAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration\u003cbr\u003ePARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data\u003cbr\u003eSplit and Merge: Aligning Position Biases in LLM-based Evaluators\u003cbr\u003eA Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations\u003cbr\u003eTowards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey\u003cbr\u003eFinding Blind Spots in Evaluator LLMs with Interpretable Checklists\u003cbr\u003e... and 16 more","\u003cb\u003eBias Detection\u003c\u002fb\u003e\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003eChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context\u003cbr\u003eTowards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights\u003cbr\u003eVoices in a Crowd: Searching for Clusters of Unique Perspectives\u003cbr\u003eTowards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey\u003cbr\u003eMoral Foundations of Large Language Models\u003cbr\u003eDiscovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation\u003cbr\u003e... and 11 more","\u003cb\u003eReading Comprehension\u003c\u002fb\u003e\u003cbr\u003eMitigating the Alignment Tax of RLHF\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003eLawBench: Benchmarking Legal Knowledge of Large Language Models\u003cbr\u003eMore Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation\u003cbr\u003eFAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition\u003cbr\u003eNull-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination\u003cbr\u003eShaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling\u003cbr\u003eDistractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation\u003cbr\u003eLayer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models\u003cbr\u003e... and 11 more","\u003cb\u003eClassification\u003c\u002fb\u003e\u003cbr\u003eEffective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process\u003cbr\u003eSURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information\u003cbr\u003eAttribute or Abstain: Large Language Models as Long Document Assistants\u003cbr\u003eIs It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?\u003cbr\u003eUnveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism\u003cbr\u003ePcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity\u003cbr\u003eHiFT: A Hierarchical Full Parameter Fine-Tuning Strategy\u003cbr\u003eFuse to Forget: Bias Reduction and Selective Memorization through Model Fusion\u003cbr\u003eComputational Meme Understanding: A Survey\u003cbr\u003eAdversarial Text Generation using Large Language Models for Dementia Detection\u003cbr\u003e... and 11 more","\u003cb\u003eData Augmentation\u003c\u002fb\u003e\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003eExploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors\u003cbr\u003eAn Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification\u003cbr\u003eMuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning\u003cbr\u003eAn Audit on the Perspectives and Challenges of Hallucinations in NLP\u003cbr\u003eCareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation\u003cbr\u003eControlMath: Controllable Data Generation Promotes Math Generalist Models\u003cbr\u003eCross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing\u003cbr\u003eGeneralizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4\u003cbr\u003eSMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support\u003cbr\u003e... and 10 more","\u003cb\u003eText Summarization\u003c\u002fb\u003e\u003cbr\u003eAligning Large Language Models with Diverse Political Viewpoints\u003cbr\u003eEnhancing Reinforcement Learning with Dense Rewards from Language Model Critic\u003cbr\u003eDetecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors\u003cbr\u003eOuroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding\u003cbr\u003eLeveraging Large Language Models for NLG Evaluation: Advances and Challenges\u003cbr\u003eCan We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?\u003cbr\u003eFFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping\u003cbr\u003eSemformer: Transformer Language Models with Semantic Planning\u003cbr\u003eDisordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts\u003cbr\u003eWalia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets\u003cbr\u003e... and 10 more","\u003cb\u003eInstruction Tuning\u003c\u002fb\u003e\u003cbr\u003eParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\u003cbr\u003eExploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights\u003cbr\u003eFast Forwarding Low-Rank Training\u003cbr\u003eIs It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?\u003cbr\u003eCommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions\u003cbr\u003eText2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback\u003cbr\u003eCurriculum Consistency Learning for Conditional Sentence Generation\u003cbr\u003eHow Susceptible are Large Language Models to Ideological Manipulation?\u003cbr\u003eInstruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks\u003cbr\u003eSymbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization\u003cbr\u003e... and 9 more","\u003cb\u003eArithmetic Reasoning\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eEmpowering Multi-step Reasoning across Languages via Program-Aided Language Models\u003cbr\u003eTowards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models\u003cbr\u003eLarge Language Models Can Self-Correct with Key Condition Verification\u003cbr\u003eNull-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination\u003cbr\u003eOuroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding\u003cbr\u003e... and 9 more","\u003cb\u003eNatural Language Understanding\u003c\u002fb\u003e\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eOn Training Data Influence of GPT Models\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003eFROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models\u003cbr\u003ePerceptions of Linguistic Uncertainty by Language Models and Humans\u003cbr\u003eMediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations\u003cbr\u003eEncouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\u003cbr\u003eApiQ: Finetuning of 2-Bit Quantized Large Language Model\u003cbr\u003eFAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding\u003cbr\u003e... and 9 more","\u003cb\u003eHallucination Detection\u003c\u002fb\u003e\u003cbr\u003eEmbedding and Gradient Say Wrong: A White-Box Method for Hallucination Detection\u003cbr\u003eWhen Context Leads but Parametric Memory Follows in Large Language Models\u003cbr\u003eKnowledge-Centric Hallucination Detection\u003cbr\u003ePelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification\u003cbr\u003eCliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios\u003cbr\u003eDetecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors\u003cbr\u003eNull-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination\u003cbr\u003eSmall Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector\u003cbr\u003eCan LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators\u003cbr\u003eOn the Universal Truthfulness Hyperplane Inside LLMS\u003cbr\u003e... and 9 more","\u003cb\u003eZero-Shot Learning\u003c\u002fb\u003e\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003eIs LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment\u003cbr\u003eExploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights\u003cbr\u003eInstruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks\u003cbr\u003eLink, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval\u003cbr\u003eEfficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models\u003cbr\u003eIFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning\u003cbr\u003eRetrieval-enriched zero-shot image classification in low-resource domains\u003cbr\u003eDiversity, Rationalize, and Combine: Ensembling Multiple QA Strategies for Zero-shot Knowledge-based VQA\u003cbr\u003eTROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning\u003cbr\u003e... and 9 more","\u003cb\u003eRelation Extraction\u003c\u002fb\u003e\u003cbr\u003eIntegrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training\u003cbr\u003eADELIE: Aligning Large Language Models on Information Extraction\u003cbr\u003eBio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints\u003cbr\u003eSciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents\u003cbr\u003eGrasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction\u003cbr\u003eLarge Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark\u003cbr\u003eTopic-Oriented Open Relation Extraction with A Priori Seed Generation\u003cbr\u003eSRF: Enhancing Document-Level Relation Extraction with a Novel Secondary Reasoning Framework\u003cbr\u003eFAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding\u003cbr\u003eA Survey on Natural Language Counterfactual Generation\u003cbr\u003e... and 9 more","\u003cb\u003eFew-Shot Learning\u003c\u002fb\u003e\u003cbr\u003eA Survey on In-context Learning\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003eINDUCT-LEARN: Short Phrase Prompting with Instruction Induction\u003cbr\u003eLearning to Retrieve Iteratively for In-Context Learning\u003cbr\u003eBridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning\u003cbr\u003eExploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights\u003cbr\u003eStrengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations\u003cbr\u003eNuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data\u003cbr\u003eOpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation\u003cbr\u003eFewer is More: Boosting Math Reasoning with Reinforced Context Pruning\u003cbr\u003e... and 8 more","\u003cb\u003eHate Speech Detection\u003c\u002fb\u003e\u003cbr\u003eHateful Word in Context Classification\u003cbr\u003eEyes Don't Lie: Subjective Hate Annotation and Detection with Gaze\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eBridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning\u003cbr\u003eThe Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification\u003cbr\u003eUnlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization\u003cbr\u003eMitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment\u003cbr\u003eUnveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003eHate Personified: Investigating the role of LLMs in content moderation\u003cbr\u003e... and 7 more","\u003cb\u003eGsm8K\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eMODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning\u003cbr\u003eFrom Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eAlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality\u003cbr\u003eTask Oriented In-Domain Data Augmentation\u003cbr\u003e... and 7 more","\u003cb\u003eBias Mitigation\u003c\u002fb\u003e\u003cbr\u003e\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models\u003cbr\u003eRSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework\u003cbr\u003eThe Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention\u003cbr\u003eLLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives\u003cbr\u003eSplit and Merge: Aligning Position Biases in LLM-based Evaluators\u003cbr\u003eBiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs\u003cbr\u003eApplying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation\u003cbr\u003eLocal Contrastive Editing of Gender Stereotypes\u003cbr\u003eOffsetBias: Leveraging Debiased Data for Tuning Evaluators\u003cbr\u003eCan AI Relate: Testing Large Language Model Response for Mental Health Support\u003cbr\u003e... and 6 more","\u003cb\u003eNatural Language Generation\u003c\u002fb\u003e\u003cbr\u003eUncertainty in Language Models: Assessment through Rank-Calibration\u003cbr\u003eOn Training Data Influence of GPT Models\u003cbr\u003eIs Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering\u003cbr\u003eStyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003eGenerative Models for Automatic Medical Decision Rule Extraction from Text\u003cbr\u003eOntologically Faithful Generation of Non-Player Character Dialogues\u003cbr\u003eContextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation\u003cbr\u003eMediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations\u003cbr\u003e... and 6 more","\u003cb\u003eLogical Reasoning\u003c\u002fb\u003e\u003cbr\u003eLearning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing\u003cbr\u003eLogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models\u003cbr\u003eScaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars\u003cbr\u003eRetrieved In-Context Principles from Previous Mistakes\u003cbr\u003eMETAREFLECTION: Learning Instructions for Language Agents using Past Reflections\u003cbr\u003ePuzzle Solving using Reasoning of Large Language Models: A Survey\u003cbr\u003eExploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems\u003cbr\u003eSymbolic Working Memory Enhances Language Models for Complex Rule Application\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eStep-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?\u003cbr\u003e... and 6 more","\u003cb\u003eReinforcement Learning From Human Feedback\u003c\u002fb\u003e\u003cbr\u003eEliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence\u003cbr\u003eWPO: Enhancing RLHF with Weighted Preference Optimization\u003cbr\u003eOptimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning\u003cbr\u003eA Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors\u003cbr\u003eImproving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003eGlobal Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents\u003cbr\u003eRethinking the Role of Proxy Rewards in Language Model Alignment\u003cbr\u003eFiltered Direct Preference Optimization\u003cbr\u003eReward Modeling Requires Automatic Adjustment Based on Data Quality\u003cbr\u003e... and 6 more","\u003cb\u003eLlms\u003c\u002fb\u003e\u003cbr\u003eEvaluating Readability and Faithfulness of Concept-based Explanations\u003cbr\u003eQUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models\u003cbr\u003ePTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL\u003cbr\u003eCOEVOL: Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation\u003cbr\u003eUNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models\u003cbr\u003eCommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions\u003cbr\u003eOpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation\u003cbr\u003eA Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models\u003cbr\u003eIn-Context Former: Lightning-fast Compressing Context for Large Language Model\u003cbr\u003eEvaluating Moral Beliefs across LLMs through a Pluralistic Framework\u003cbr\u003e... and 5 more","\u003cb\u003eKnowledge Editing\u003c\u002fb\u003e\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eInterpretability-based Tailored Knowledge Editing in Transformers\u003cbr\u003eEVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation\u003cbr\u003e\u003eWhy Does New Knowledge Create Messy Ripple Effects in LLMs?\u003cbr\u003eCommonsense Knowledge Editing Based on Free-Text in LLMs\u003cbr\u003eAKEW: Assessing Knowledge Editing in the Wild\u003cbr\u003eRebuilding ROME : Resolving Model Collapse during Sequential Model Editing\u003cbr\u003eEditing Conceptual Knowledge for Large Language Models\u003cbr\u003eKnowledge Editing in Language Models via Adapted Direct Preference Optimization\u003cbr\u003eRIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning\u003cbr\u003e... and 5 more","\u003cb\u003eDialogue Generation\u003c\u002fb\u003e\u003cbr\u003eSynergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems\u003cbr\u003eOntologically Faithful Generation of Non-Player Character Dialogues\u003cbr\u003eOptimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eBeyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models\u003cbr\u003eTransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities\u003cbr\u003eHow to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective\u003cbr\u003eLeveraging Large Language Models for NLG Evaluation: Advances and Challenges\u003cbr\u003eGDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets\u003cbr\u003eSMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support\u003cbr\u003e... and 5 more","\u003cb\u003eLanguage Understanding\u003c\u002fb\u003e\u003cbr\u003eNumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning\u003cbr\u003eAutoregressive Pre-Training on Pixels and Texts\u003cbr\u003eUnraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications\u003cbr\u003eLarge Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark\u003cbr\u003eLLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks\u003cbr\u003eConnecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game\u003cbr\u003eMalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language\u003cbr\u003eThe Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance\u003cbr\u003eEnhancing Agent Learning through World Dynamics Modeling\u003cbr\u003eMUSCLE: A Model Update Strategy for Compatible LLM Evolution\u003cbr\u003e... and 4 more","\u003cb\u003eOpen-Domain Question Answering\u003c\u002fb\u003e\u003cbr\u003eBlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering\u003cbr\u003eREAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering\u003cbr\u003eLarge Language Models Can Self-Correct with Key Condition Verification\u003cbr\u003eWhere am I? Large Language Models Wandering between Semantics and Structures in Long Contexts\u003cbr\u003eModel Editing Harms General Abilities of Large Language Models: Regularization to the Rescue\u003cbr\u003eImproving Zero-shot LLM Re-Ranker with Risk Minimization\u003cbr\u003eRE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation\u003cbr\u003eRaFe: Ranking Feedback Improves Query Rewriting for RAG\u003cbr\u003eChain-of-Rewrite: Aligning Question and Documents for Open-Domain Question Answering\u003cbr\u003eAdaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts\u003cbr\u003e... and 4 more","\u003cb\u003eKnowledge Distillation\u003c\u002fb\u003e\u003cbr\u003eAMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003eMTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval\u003cbr\u003eKnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server\u003cbr\u003eFIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation\u003cbr\u003eOptimizing Chinese Lexical Simplification Across Word Types: A Hybrid Approach\u003cbr\u003eImprove Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation\u003cbr\u003eDual-Space Knowledge Distillation for Large Language Models\u003cbr\u003ePAIRDISTILL: Pairwise Relevance Distillation for Dense Retrieval\u003cbr\u003exCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics\u003cbr\u003e... and 4 more","\u003cb\u003eLlm Alignment\u003c\u002fb\u003e\u003cbr\u003eTake Off the Training Wheels! Progressive In-Context Learning for Effective Alignment\u003cbr\u003eContrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion\u003cbr\u003eNegating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization\u003cbr\u003eReward Difference Optimization For Sample Reweighting In Offline RLHF\u003cbr\u003eSelf-Evolution Fine-Tuning for Policy Optimization\u003cbr\u003ePURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness\u003cbr\u003eTS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models\u003cbr\u003eOn Diversified Preferences of Large Language Model Alignment\u003cbr\u003eChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline\u003cbr\u003eEnhancing Alignment using Curriculum Learning & Ranked Preferences\u003cbr\u003e... and 4 more","\u003cb\u003eAutomatic Speech Recognition\u003c\u002fb\u003e\u003cbr\u003eVoices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eOn Mitigating Performance Disparities in Multilingual Speech Recognition\u003cbr\u003eOptimized Speculative Sampling for GPU Hardware Accelerators\u003cbr\u003eAdvancing Test-Time Adaptation in Wild Acoustic Test Settings\u003cbr\u003eMuting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models\u003cbr\u003eTask Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition\u003cbr\u003eUnveiling the Role of Pretraining in Direct Speech Translation\u003cbr\u003eInterventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding\u003cbr\u003eAre Modern Neural ASR Architectures Robust for Polysynthetic Languages?\u003cbr\u003e... and 4 more","\u003cb\u003eTopic Classification\u003c\u002fb\u003e\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eUniversal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003eCorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs\u003cbr\u003eSYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation\u003cbr\u003eMultilingual Topic Classification in X: Dataset and Analysis\u003cbr\u003eCoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage\u003cbr\u003eTransfer Learning for Text Classification via Model Risk Analysis\u003cbr\u003eGeneralists vs. Specialists: Evaluating Large Language Models for Urdu\u003cbr\u003eIn-Context Learning with Iterative Demonstration Selection\u003cbr\u003e... and 4 more","\u003cb\u003eLanguage Models\u003c\u002fb\u003e\u003cbr\u003ePrompts have evil twins\u003cbr\u003eTracking the perspectives of interacting language models\u003cbr\u003ePersonas as a Way to Model Truthfulness in Language Models\u003cbr\u003eToken Erasure as a Footprint of Implicit Vocabulary Items in LLMs\u003cbr\u003eBreaking ReLU Barrier: Generalized MoEfication for Dense Pretrained Models\u003cbr\u003eEncourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective\u003cbr\u003eFIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation\u003cbr\u003eCONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models\u003cbr\u003eLearning to Route for Dynamic Adapter Composition in Continual Learning with Language Models\u003cbr\u003eImproving LLM Attributions with Randomized Path-Integration\u003cbr\u003e... and 3 more","\u003cb\u003eTranslation\u003c\u002fb\u003e\u003cbr\u003eMitigating the Alignment Tax of RLHF\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003eA Thorough Examination of Decoding Methods in the Era of LLMs\u003cbr\u003eUnlocking Memorization in Large Language Models with Dynamic Soft Prompting\u003cbr\u003eAn Analysis and Mitigation of the Reversal Curse\u003cbr\u003eLayer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models\u003cbr\u003eLongAlign: A Recipe for Long Context Alignment of Large Language Models\u003cbr\u003eAC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models\u003cbr\u003eGeneralists vs. Specialists: Evaluating Large Language Models for Urdu\u003cbr\u003eMedINST: Meta Dataset of Biomedical Instructions\u003cbr\u003e... and 2 more","\u003cb\u003eFine-Tuning\u003c\u002fb\u003e\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eFast Forwarding Low-Rank Training\u003cbr\u003eBreaking ReLU Barrier: Generalized MoEfication for Dense Pretrained Models\u003cbr\u003eLarge Language Models Can Be Contextual Privacy Protection Learners\u003cbr\u003eMIXTURE-OF-SKILLS: Learning to Optimize Data Usage for Fine-Tuning Large Language Models\u003cbr\u003eGRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients\u003cbr\u003eOn the Empirical Complexity of Reasoning and Planning in LLMs\u003cbr\u003ePrivate prediction for large-scale synthetic text generation\u003cbr\u003eStep-level Value Preference Optimization for Mathematical Reasoning\u003cbr\u003eLeveraging Web-Crawled Data for High-Quality Fine-Tuning\u003cbr\u003e... and 2 more","\u003cb\u003eGeneralization\u003c\u002fb\u003e\u003cbr\u003eInstruction Pre-Training: Language Models are Supervised Multitask Learners\u003cbr\u003eControlMath: Controllable Data Generation Promotes Math Generalist Models\u003cbr\u003eMAIR: A Massive Benchmark for Evaluating Instructed Retrieval\u003cbr\u003eData Contamination Can Cross Language Barriers\u003cbr\u003eInstruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks\u003cbr\u003eInvestigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks\u003cbr\u003ePREDICT: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection\u003cbr\u003eKnowledge Graph Enhanced Large Language Model Editing\u003cbr\u003eCan LLM Graph Reasoning Generalize beyond Pattern Memorization?\u003cbr\u003eEditing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models\u003cbr\u003e... and 2 more","\u003cb\u003eVisual Reasoning\u003c\u002fb\u003e\u003cbr\u003eFiner: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models\u003cbr\u003eOmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer\u003cbr\u003eFineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension\u003cbr\u003eMultimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model\u003cbr\u003eWhiteboard-of-Thought: Thinking Step-by-Step Across Modalities\u003cbr\u003eTraining-free Deep Concept Injection Enables Language Models for Video Question Answering\u003cbr\u003eMMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems\u003cbr\u003eCONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models\u003cbr\u003ePlot Twist: Multimodal Models Don't Comprehend Simple Chart Details\u003cbr\u003eLarge Language Models Are Challenged by Habitat-Centered Reasoning\u003cbr\u003e... and 2 more","\u003cb\u003eMedical Question Answering\u003c\u002fb\u003e\u003cbr\u003eMedical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?\u003cbr\u003eLM\u00b2: A Simple Society of Language Models Solves Complex Reasoning\u003cbr\u003eCasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures\u003cbr\u003eAMPO: Automatic Multi-Branched Prompt Optimization\u003cbr\u003eFew shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering\u003cbr\u003eAugmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering\u003cbr\u003eEnhancing Healthcare LLM Trust with Atypical Presentations Recalibration\u003cbr\u003eMultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate\u003cbr\u003eMEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation\u003cbr\u003eLanguage Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks\u003cbr\u003e... and 2 more","\u003cb\u003eText-To-Image Generation\u003c\u002fb\u003e\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003ePre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation\u003cbr\u003eWords Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation\u003cbr\u003eEmpowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training\u003cbr\u003eThe Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention\u003cbr\u003eRe-ReST: Reflection-Reinforced Self-Training for Language Agents\u003cbr\u003eAltogether: Image Captioning via Re-aligning Alt-text\u003cbr\u003eLearning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training\u003cbr\u003ePrecision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model\u003cbr\u003eRepairing Catastrophic-Neglect in Text-to-Image Diffusion Models via Attention-Guided Feature Enhancement\u003cbr\u003e... and 1 more","\u003cb\u003eSpeech Recognition\u003c\u002fb\u003e\u003cbr\u003eScaling Properties of Speech Language Models\u003cbr\u003eEH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning\u003cbr\u003eMulti-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges\u003cbr\u003eMOSEL: Inference Serving Using Dynamic Modality Selection\u003cbr\u003eWhat is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations\u003cbr\u003e950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages\u003cbr\u003eVHASR: A Multimodal Speech Recognition System With Vision Hotwords\u003cbr\u003eBayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities\u003cbr\u003eTwists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps\u003cbr\u003eCasablanca: Data and Models for Multidialectal Arabic Speech Recognition\u003cbr\u003e... and 1 more","\u003cb\u003eEvaluation\u003c\u002fb\u003e\u003cbr\u003eLogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models\u003cbr\u003eWhat is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations\u003cbr\u003eAKEW: Assessing Knowledge Editing in the Wild\u003cbr\u003eTowards Benchmarking Situational Awareness of Large Language Models: Comprehensive Benchmark, Evaluation and Analysis\u003cbr\u003eCreative and Context-Aware Translation of East Asian Idioms with GPT-4\u003cbr\u003eA LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation\u003cbr\u003eNot (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition\u003cbr\u003eTuringQ: Benchmarking AI Comprehension in Theory of Computation\u003cbr\u003eBLADE: Benchmarking Language Model Agents for Data-Driven Science\u003cbr\u003eFactcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers\u003cbr\u003e... and 1 more","\u003cb\u003eEvent Extraction\u003c\u002fb\u003e\u003cbr\u003eIntegrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training\u003cbr\u003eADELIE: Aligning Large Language Models on Information Extraction\u003cbr\u003eExplicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments\u003cbr\u003eSPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness\u003cbr\u003eGeneral Collaborative Framework between Large Language Model and Experts for Universal Information Extraction\u003cbr\u003eDocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction\u003cbr\u003eEmploying Glyphic Information for Chinese Event Extraction with Vision-Language Model\u003cbr\u003eAC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models\u003cbr\u003eEvent-Keyed Summarization\u003cbr\u003eMedINST: Meta Dataset of Biomedical Instructions\u003cbr\u003e... and 1 more","\u003cb\u003eGeneration\u003c\u002fb\u003e\u003cbr\u003eFuseGen: PLM Fusion for Data-generation based Zero-shot Learning\u003cbr\u003eSafely Learning with Private Data: A Federated Learning Framework for Large Language Model\u003cbr\u003eUnveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism\u003cbr\u003eCOGEN: Learning from Feedback with Coupled Comprehension and Generation\u003cbr\u003eHiFT: A Hierarchical Full Parameter Fine-Tuning Strategy\u003cbr\u003eFuse to Forget: Bias Reduction and Selective Memorization through Model Fusion\u003cbr\u003eDynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models\u003cbr\u003eReformatted Alignment\u003cbr\u003eBSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain\u003cbr\u003eAre LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues\u003cbr\u003e... and 1 more","\u003cb\u003eInformation Extraction\u003c\u002fb\u003e\u003cbr\u003eA Survey of AMR Applications\u003cbr\u003eExploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights\u003cbr\u003eText-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction\u003cbr\u003eModeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eFINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents\u003cbr\u003eTKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs\u003cbr\u003eDe-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP\u003cbr\u003eWhen and Where Did It Happen? An Encoder-Decoder Model to Identify Scenario Context\u003cbr\u003eSchema-Driven Information Extraction from Heterogeneous Tables\u003cbr\u003e... and 1 more","\u003cb\u003eMath Problem Solving\u003c\u002fb\u003e\u003cbr\u003eA Thorough Examination of Decoding Methods in the Era of LLMs\u003cbr\u003eI Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses\u003cbr\u003eUnraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications\u003cbr\u003eAn Analysis and Mitigation of the Reversal Curse\u003cbr\u003eTools Fail: Detecting Silent Errors in Faulty Tools\u003cbr\u003eSmall Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector\u003cbr\u003eLLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks\u003cbr\u003eAssessing and Verifying Task Utility in LLM-Powered Applications\u003cbr\u003eMulti-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision\u003cbr\u003eChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline\u003cbr\u003e... and 1 more","\u003cb\u003eLanguage Model Alignment\u003c\u002fb\u003e\u003cbr\u003eLIONS: An Empirically Optimized Approach to Align Language Models\u003cbr\u003eReverse-Engineering the Reader\u003cbr\u003eImproving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning\u003cbr\u003eCOMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities\u003cbr\u003eRethinking the Role of Proxy Rewards in Language Model Alignment\u003cbr\u003eEnhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing\u003cbr\u003eFiltered Direct Preference Optimization\u003cbr\u003eOptimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search\u003cbr\u003eKnowledge Editing in Language Models via Adapted Direct Preference Optimization\u003cbr\u003eEvolutionary Contrastive Distillation for Language Model Alignment\u003cbr\u003e... and 1 more","\u003cb\u003eMath\u003c\u002fb\u003e\u003cbr\u003eParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003ePython is Not Always the Best Choice: Embracing Multilingual Program of Thoughts\u003cbr\u003eNash CoT: Multi-Path Inference with Preference Equilibrium\u003cbr\u003eReasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\u003cbr\u003eCan LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks\u003cbr\u003eAGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories\u003cbr\u003eDivide-or-Conquer? Which Part Should You Distill Your LLM?\u003cbr\u003eSkills-in-Context: Unlocking Compositionality in Large Language Models","\u003cb\u003eRte\u003c\u002fb\u003e\u003cbr\u003eAdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning\u003cbr\u003eOn Training Data Influence of GPT Models\u003cbr\u003eUnveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement\u003cbr\u003eWhen Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models\u003cbr\u003eFisher Information-based Efficient Curriculum Federated Learning with Large Language Models\u003cbr\u003eWhich Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?\u003cbr\u003eInitialization of Large Language Models via Reparameterization to Mitigate Loss Spikes\u003cbr\u003eIntermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models\u003cbr\u003eSkills-in-Context: Unlocking Compositionality in Large Language Models\u003cbr\u003eA Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune","\u003cb\u003eGlue\u003c\u002fb\u003e\u003cbr\u003eModel Balancing Helps Low-data Training and Fine-tuning\u003cbr\u003ePixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models\u003cbr\u003eUnveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement\u003cbr\u003eSparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers\u003cbr\u003eIM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method\u003cbr\u003eAlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality\u003cbr\u003eMixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules\u003cbr\u003eLeveraging Grammar Induction for Language Understanding and Generation\u003cbr\u003eIntermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models\u003cbr\u003eBIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models","\u003cb\u003eHallucination Mitigation\u003c\u002fb\u003e\u003cbr\u003eHELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding\u003cbr\u003eWhispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models\u003cbr\u003eWord Alignment as Preference for Machine Translation\u003cbr\u003eAnalysis of Plan-based Retrieval for Grounded Text Generation\u003cbr\u003eMitigating Open-Vocabulary Caption Hallucinations\u003cbr\u003eSH2: Self-Highlighted Hesitation Helps You Decode More Truthfully\u003cbr\u003eMechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations\u003cbr\u003eWhat if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models\u003cbr\u003eStructured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations\u003cbr\u003eDial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning","\u003cb\u003eMath Reasoning\u003c\u002fb\u003e\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eStepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors\u003cbr\u003eFewer is More: Boosting Math Reasoning with Reinforced Context Pruning\u003cbr\u003eSCIAGENT: Tool-augmented Language Models for Scientific Reasoning\u003cbr\u003eCoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage\u003cbr\u003eCan LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks\u003cbr\u003eMUSCLE: A Model Update Strategy for Compatible LLM Evolution\u003cbr\u003eUnlocking the Potential of Model Merging for Low-Resource Languages\u003cbr\u003eSQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models\u003cbr\u003eMiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning","\u003cb\u003eMulti-Hop Reasoning\u003c\u002fb\u003e\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003eFirst Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning\u003cbr\u003eAdaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations\u003cbr\u003eReasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\u003cbr\u003eAn Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models\u003cbr\u003eDetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?\u003cbr\u003eTRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation\u003cbr\u003eCross-Lingual Multi-Hop Knowledge Editing\u003cbr\u003eGraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models\u003cbr\u003eQuantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective","\u003cb\u003eSemantic Textual Similarity\u003c\u002fb\u003e\u003cbr\u003eSurveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese\u003cbr\u003eAdvancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss\u003cbr\u003ePcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity\u003cbr\u003eScaling Sentence Embeddings with Large Language Models\u003cbr\u003eVariational Language Concepts for Interpreting Foundation Language Models\u003cbr\u003eHit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention\u003cbr\u003eAre ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity\u003cbr\u003eMEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation\u003cbr\u003eRegression-aware Inference with LLMs\u003cbr\u003eRepresentational Isomorphism and Alignment of Multilingual Large Language Models","\u003cb\u003eTopic Modeling\u003c\u002fb\u003e\u003cbr\u003ePhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study\u003cbr\u003eNoise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature\u003cbr\u003eAutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments\u003cbr\u003eLLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement\u003cbr\u003eLLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement\u003cbr\u003eUnsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance\u003cbr\u003eNeuroMax: Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization\u003cbr\u003ePlatform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias\u003cbr\u003eTopic Modeling: Contextual Token Embeddings Are All You Need\u003cbr\u003eEnhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs","\u003cb\u003eQuantization\u003c\u002fb\u003e\u003cbr\u003eQUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models\u003cbr\u003eVPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models\u003cbr\u003exCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics\u003cbr\u003eMobileQuant: Mobile-friendly Quantization for On-device Language Models\u003cbr\u003ePromoting Data and Model Privacy in Federated Learning through Quantized LoRA\u003cbr\u003eOptimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs\u003cbr\u003eExploring Quantization for Efficient Pre-Training of Transformer Language Models\u003cbr\u003eQEFT: Quantization for Efficient Fine-Tuning of LLMs\u003cbr\u003eHow Does Quantization Affect Multilingual LLMs?\u003cbr\u003eATQ: Activation Transformation for Weight-Activation Quantization of Large Language Models","\u003cb\u003eVideo Question Answering\u003c\u002fb\u003e\u003cbr\u003eEmpowering Large Language Model for Continual Video Question Answering with Collaborative Prompting\u003cbr\u003eVideo-LLaVA: Learning United Visual Representation by Alignment Before Projection\u003cbr\u003eEncoding and Controlling Global Semantics for Long-form Video Question Answering\u003cbr\u003eTraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering\u003cbr\u003eEfficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge\u003cbr\u003eTV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning\u003cbr\u003eTraining-free Deep Concept Injection Enables Language Models for Video Question Answering\u003cbr\u003eEnhancing Temporal Modeling of Video LLMs via Time Gating\u003cbr\u003eExploring Question Guidance and Answer Calibration for Visually Grounded Video Question Answering\u003cbr\u003eALANAVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding","\u003cb\u003eLink Prediction\u003c\u002fb\u003e\u003cbr\u003eMQuinE: a Cure for \"Z-paradox\" in Knowledge Graph Embedding\u003cbr\u003eMoCoKGC: Momentum Contrast Entity Encoding for Knowledge Graph Completion\u003cbr\u003eATAP: Automatic Template-Augmented Commonsense Knowledge Graph Completion via Pre-Trained Language Models\u003cbr\u003eCan Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction\u003cbr\u003ePredictive Multiplicity of Knowledge Graph Embeddings in Link Prediction\u003cbr\u003eTemporal Fact Reasoning over Hyper-Relational Knowledge Graphs\u003cbr\u003eLet's Ask GNN: Empowering Large Language Model for Graph In-Context Learning\u003cbr\u003eOpenGraph: Towards Open Graph Foundation Models\u003cbr\u003eLlamipa: An Incremental Discourse Parser\u003cbr\u003eBlock-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding","\u003cb\u003eCalibration\u003c\u002fb\u003e\u003cbr\u003eEnhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding\u003cbr\u003eFIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation\u003cbr\u003eCONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models\u003cbr\u003eCalibrating Language Models with Adaptive Temperature Scaling\u003cbr\u003eReconfidencing LLMs from the Grouping Loss Perspective\u003cbr\u003eEnhancing Healthcare LLM Trust with Atypical Presentations Recalibration\u003cbr\u003eSelf-Consistency Boosts Calibration for Math Reasoning\u003cbr\u003eDistance-aware Calibration for Pre-trained Language Models\u003cbr\u003eCalibrating Long-form Generations from Large Language Models\u003cbr\u003eUncertainty Calibration for Tool-Using Language Agents","\u003cb\u003eEmotion Recognition\u003c\u002fb\u003e\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eVisual Prompting in LLMs for Enhancing Emotion Recognition\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eSelf-Powered LLM Modality Expansion for Large Speech-Text Models\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003ePALM: Few-Shot Prompt Learning for Audio Language Models\u003cbr\u003eEmotion Granularity from Text: An Aggregate-Level Indicator of Mental Health\u003cbr\u003eEmosical: An Emotion-Annotated Musical Theatre Dataset\u003cbr\u003eWavLLM: Towards Robust and Adaptive Speech Large Language Model","\u003cb\u003eSvamp\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eFrom Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eAlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality\u003cbr\u003eTask Oriented In-Domain Data Augmentation\u003cbr\u003eMirror-Consistency: Harnessing Inconsistency in Majority Voting","\u003cb\u003eQuestion Generation\u003c\u002fb\u003e\u003cbr\u003eQUDSELECT: Selective Decoding for Questions Under Discussion Parsing\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eMeasuring the Robustness of NLP Models to Domain Shifts\u003cbr\u003eTranslation of Multifaceted Data without Re-Training of Machine Translation Systems\u003cbr\u003eLearning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain\u003cbr\u003eEvaluation of Question Answer Generation for Portuguese: Insights and Datasets\u003cbr\u003eLearning to Ask Denotative and Connotative Questions for Knowledge-based VQA\u003cbr\u003eEnable Fast Sampling for Seq2Seq Text Diffusion\u003cbr\u003eReference-based Metrics Disprove Themselves in Question Generation","\u003cb\u003eTextual Entailment\u003c\u002fb\u003e\u003cbr\u003eDA\u00b3: A Distribution-Aware Adversarial Attack against Language Models\u003cbr\u003eEnhancing Systematic Decompositional Natural Language Inference Using Informal Logic\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eRepMatch: Quantifying Cross-Instance Similarities in Representation Space\u003cbr\u003eOpen-world Multi-label Text Classification with Extremely Weak Supervision\u003cbr\u003eMitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging\u003cbr\u003eMedINST: Meta Dataset of Biomedical Instructions\u003cbr\u003eVariational Language Concepts for Interpreting Foundation Language Models\u003cbr\u003eAre ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity","\u003cb\u003eToxicity Detection\u003c\u002fb\u003e\u003cbr\u003eToxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003eCan LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric\u003cbr\u003eModeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas\u003cbr\u003eBadFair: Backdoored Fairness Attacks with Group-conditioned Triggers\u003cbr\u003eHow to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models\u003cbr\u003eAttribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification\u003cbr\u003eBeyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression\u003cbr\u003eCROWD: Certified Robustness via Weight Distribution for Smoothed Classifiers against Backdoor Attack","\u003cb\u003eFact Verification\u003c\u002fb\u003e\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003eNormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization\u003cbr\u003eMolecular Facts: Desiderata for Decontextualization in LLM Fact Verification\u003cbr\u003ePROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context\u003cbr\u003eEvidence Retrieval for Fact Verification using Multi-stage Reranking\u003cbr\u003eDyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs\u003cbr\u003eHow Entangled is Factuality and Deception in German?\u003cbr\u003eClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs\u003cbr\u003eZero-Shot Fact Verification via Natural Logic and Large Language Models","\u003cb\u003eSentiment Classification\u003c\u002fb\u003e\u003cbr\u003eUNIGEN: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation\u003cbr\u003eAn Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification\u003cbr\u003eLLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History\u003cbr\u003eAMPO: Automatic Multi-Branched Prompt Optimization\u003cbr\u003eRevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference\u003cbr\u003eSCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models\u003cbr\u003eBadFair: Backdoored Fairness Attacks with Group-conditioned Triggers\u003cbr\u003eDual-Phase Accelerated Prompt Optimization","\u003cb\u003eDataset Creation\u003c\u002fb\u003e\u003cbr\u003eCOCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds\u003cbr\u003eClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures\u003cbr\u003eEmosical: An Emotion-Annotated Musical Theatre Dataset\u003cbr\u003eForecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling\u003cbr\u003eFrom Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues\u003cbr\u003eCACTUS: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory\u003cbr\u003eARTS: Assessing Readability & Text Simplicity\u003cbr\u003eCoCoHD: Congress Committee Hearing Dataset","\u003cb\u003eMulti-Task Learning\u003c\u002fb\u003e\u003cbr\u003eLarge Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment\u003cbr\u003eMetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003eGetting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection\u003cbr\u003eHit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention\u003cbr\u003eWhere Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing\u003cbr\u003eFine-Tuning Language Models on Multiple Datasets for Citation Intention Classification","\u003cb\u003eCoreference Resolution\u003c\u002fb\u003e\u003cbr\u003eScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eMajor Entity Identification: A Generalizable Alternative to Coreference Resolution\u003cbr\u003eLayer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models\u003cbr\u003eAre LLMs Good Annotators for Discourse-level Event Relation Extraction?\u003cbr\u003eMedINST: Meta Dataset of Biomedical Instructions\u003cbr\u003eHit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention\u003cbr\u003eEvaluating Differentially Private Synthetic Data Generation in High-Stakes Domains","\u003cb\u003eContinual Learning\u003c\u002fb\u003e\u003cbr\u003eSEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models\u003cbr\u003eEmpowering Large Language Model for Continual Video Question Answering with Collaborative Prompting\u003cbr\u003eLearning to Route for Dynamic Adapter Composition in Continual Learning with Language Models\u003cbr\u003eRevisiting Catastrophic Forgetting in Large Language Model Tuning\u003cbr\u003eUnlocking Continual Learning Abilities in Language Models\u003cbr\u003eICL: Iterative Continual Learning for Multi-domain Neural Machine Translation\u003cbr\u003eGradient Localization Improves Lifelong Pretraining of Language Models\u003cbr\u003eMitigating Catastrophic Forgetting in Language Transfer via Model Merging","\u003cb\u003eText-To-Sql\u003c\u002fb\u003e\u003cbr\u003ePTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL\u003cbr\u003eMiddleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments\u003cbr\u003eBayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities\u003cbr\u003eImproving Demonstration Diversity by Human-Free Fusing for Text-to-SQL\u003cbr\u003eSYNTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA\u003cbr\u003eLearning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL\u003cbr\u003eDTS-SQL: Decomposed Text-to-SQL with Small Large Language Models\u003cbr\u003eTAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning","\u003cb\u003eStory Generation\u003c\u002fb\u003e\u003cbr\u003eBayesian Calibration of Win Rate Estimation with LLM Evaluators\u003cbr\u003eThemis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability\u003cbr\u003eLeveraging Large Language Models for NLG Evaluation: Advances and Challenges\u003cbr\u003eMeasuring Psychological Depth in Language Models\u003cbr\u003eWalia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets\u003cbr\u003eBeyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models\u003cbr\u003eSWAG: Storytelling With Action Guidance\u003cbr\u003eExtrinsic Evaluation of Cultural Competence in Large Language Models","\u003cb\u003eDomain Adaptation\u003c\u002fb\u003e\u003cbr\u003eAdvancing Test-Time Adaptation in Wild Acoustic Test Settings\u003cbr\u003eTask Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition\u003cbr\u003eDomain adapted machine translation: What does catastrophic forgetting forget and why?\u003cbr\u003eGeneration with Dynamic Vocabulary\u003cbr\u003eAdaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?\u003cbr\u003eUnsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts\u003cbr\u003eA Survey on Natural Language Counterfactual Generation\u003cbr\u003eTogether We Can: Multilingual Automatic Post-Editing for Low-Resource Languages","\u003cb\u003eSentiment Analysis Natural Language Inference\u003c\u002fb\u003e\u003cbr\u003eGetting More from Less: Large Language Models are Good Spontaneous Multilingual Learners\u003cbr\u003eCoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage\u003cbr\u003eALVIN: Active Learning Via INterpolation\u003cbr\u003eMeasuring the Robustness of NLP Models to Domain Shifts\u003cbr\u003eA Survey on Natural Language Counterfactual Generation\u003cbr\u003eStablePT: Towards Stable Prompting for Few-shot Learning via Input Separation\u003cbr\u003eThink Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection\u003cbr\u003eLLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study","\u003cb\u003eModel Editing\u003c\u002fb\u003e\u003cbr\u003eConsecutive Batch Model Editing with HooK Layers\u003cbr\u003eOn the Robustness of Editing Large Language Models\u003cbr\u003eLocal Contrastive Editing of Gender Stereotypes\u003cbr\u003eRebuilding ROME : Resolving Model Collapse during Sequential Model Editing\u003cbr\u003eKnowledge Graph Enhanced Large Language Model Editing\u003cbr\u003eThe Fall of ROME: Understanding the Collapse of LLMs in Model Editing\u003cbr\u003eBetter Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization\u003cbr\u003eA Unified Framework for Model Editing","\u003cb\u003eBinary Classification\u003c\u002fb\u003e\u003cbr\u003eAn Experimental Analysis on Evaluating Patent Citations\u003cbr\u003eTROTR: A Framework for Evaluating the Recontextualization of Text\u003cbr\u003eStill Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis\u003cbr\u003eMulti-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models\u003cbr\u003eGraph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification\u003cbr\u003eESG-Kor: A Korean Dataset for ESG-related Information Extraction and Practical Use Cases\u003cbr\u003eThe Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI","\u003cb\u003eStrategyqa\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eMirror-Consistency: Harnessing Inconsistency in Majority Voting\u003cbr\u003eMaking Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning","\u003cb\u003eParaphrase Detection\u003c\u002fb\u003e\u003cbr\u003eWhat's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs\u003cbr\u003eLayer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models\u003cbr\u003eRevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference\u003cbr\u003eALVIN: Active Learning Via INterpolation\u003cbr\u003eMeasuring and Improving Attentiveness to Partial Inputs with Counterfactuals\u003cbr\u003eHit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention\u003cbr\u003eLoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters","\u003cb\u003eQnli\u003c\u002fb\u003e\u003cbr\u003eAdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning\u003cbr\u003eUnveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement\u003cbr\u003eFisher Information-based Efficient Curriculum Federated Learning with Large Language Models\u003cbr\u003eWhich Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?\u003cbr\u003eIntermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models\u003cbr\u003eFine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation\u003cbr\u003eA Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune","\u003cb\u003eMnli\u003c\u002fb\u003e\u003cbr\u003eAdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning\u003cbr\u003eUnveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement\u003cbr\u003eWhen Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models\u003cbr\u003eWhich Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?\u003cbr\u003eIntermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models\u003cbr\u003eFine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation\u003cbr\u003eA Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune","\u003cb\u003eBoolq\u003c\u002fb\u003e\u003cbr\u003eAdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning\u003cbr\u003eOn Training Data Influence of GPT Models\u003cbr\u003eWhen Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models\u003cbr\u003eFisher Information-based Efficient Curriculum Federated Learning with Large Language Models\u003cbr\u003eCHESS : Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification\u003cbr\u003eInitialization of Large Language Models via Reparameterization to Mitigate Loss Spikes\u003cbr\u003eDLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model","\u003cb\u003ePrompt Engineering\u003c\u002fb\u003e\u003cbr\u003eA Survey on In-context Learning\u003cbr\u003eMoral Foundations of Large Language Models\u003cbr\u003eAMPO: Automatic Multi-Branched Prompt Optimization\u003cbr\u003eThe Death and Life of Great Prompts: Analyzing the Evolution of LLM Prompts from the Structural Perspective\u003cbr\u003eFew shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering\u003cbr\u003eCreative and Context-Aware Translation of East Asian Idioms with GPT-4\u003cbr\u003eCan't Remember Details in Long Documents? You Need Some R&R","\u003cb\u003eBias Analysis\u003c\u002fb\u003e\u003cbr\u003eAGENTREVIEW: Exploring Peer Review Dynamics with LLM Agents\u003cbr\u003eHumans or LLMs as the Judge? A Study on Judgement Bias\u003cbr\u003eStatistical Uncertainty in Word Embeddings: GloVe-V\u003cbr\u003eHate Personified: Investigating the role of LLMs in content moderation\u003cbr\u003eFrom Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment\u003cbr\u003eWho is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models\u003cbr\u003eModeling Gender and Dialect Bias in Automatic Speech Recognition","\u003cb\u003eSuperglue\u003c\u002fb\u003e\u003cbr\u003eModel Balancing Helps Low-data Training and Fine-tuning\u003cbr\u003eSafely Learning with Private Data: A Federated Learning Framework for Large Language Model\u003cbr\u003eTEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models\u003cbr\u003eTurn Waste into Worth: Rectifying Top-k Router of MoE\u003cbr\u003eInitialization of Large Language Models via Reparameterization to Mitigate Loss Spikes\u003cbr\u003eBIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models\u003cbr\u003eLoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning","\u003cb\u003eLanguage Generation\u003c\u002fb\u003e\u003cbr\u003eCMD: a framework for Context-aware Model self-Detoxification\u003cbr\u003eLarge Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark\u003cbr\u003eIntrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis\u003cbr\u003eMake Large Language Model a Better Ranker\u003cbr\u003eBreaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling\u003cbr\u003eMoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition\u003cbr\u003eEigen Attention: Attention in Low-Rank Space for KV Cache Compression","\u003cb\u003eArithmetic Reasoning Commonsense Reasoning\u003c\u002fb\u003e\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eLarge Language Models Can Self-Correct with Key Condition Verification\u003cbr\u003eNull-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination\u003cbr\u003eRe-Reading Improves Reasoning in Large Language Models\u003cbr\u003eApiQ: Finetuning of 2-Bit Quantized Large Language Model\u003cbr\u003eMirror-Consistency: Harnessing Inconsistency in Majority Voting","\u003cb\u003eImage Classification\u003c\u002fb\u003e\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003eIf CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions\u003cbr\u003eVision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification\u003cbr\u003eRetrieval-enriched zero-shot image classification in low-resource domains\u003cbr\u003eUnveiling the mystery of visual attributes of concrete and abstract concepts: Variability, nearest neighbors, and challenging categories\u003cbr\u003eText2Model: Text-based Model Induction for Zero-shot Image Classification\u003cbr\u003eMed-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models","\u003cb\u003ePreference Learning\u003c\u002fb\u003e\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eVLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment\u003cbr\u003eWhat are the Generator Preferences for End-to-end Task-Oriented Dialog System?\u003cbr\u003eModeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation\u003cbr\u003eStep-level Value Preference Optimization for Mathematical Reasoning\u003cbr\u003eTS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models\u003cbr\u003eInterpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts","\u003cb\u003eSemantic Parsing\u003c\u002fb\u003e\u003cbr\u003eQUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios\u003cbr\u003eLearning to Retrieve Iteratively for In-Context Learning\u003cbr\u003eLanguage-to-Code Translation with a Single Labeled Example\u003cbr\u003eStrengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations\u003cbr\u003eCross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing\u003cbr\u003eScope-enhanced Compositional Semantic Parsing for DRT\u003cbr\u003ePredicting generalization performance with correctness discriminators","\u003cb\u003eMmlu\u003c\u002fb\u003e\u003cbr\u003eMODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning\u003cbr\u003eUnveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement\u003cbr\u003eTurn Waste into Worth: Rectifying Top-k Router of MoE\u003cbr\u003eThe Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance\u003cbr\u003eRoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization\u003cbr\u003eLLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement\u003cbr\u003eMiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning","\u003cb\u003eError Correction\u003c\u002fb\u003e\u003cbr\u003eVerification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving\u003cbr\u003eLearn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning\u003cbr\u003eEHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records\u003cbr\u003eBi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check\u003cbr\u003eBeyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models\u003cbr\u003eE2CL: Exploration-based Error Correction Learning for Embodied Agents\u003cbr\u003eResilience of Large Language Models for Noisy Instructions","\u003cb\u003eKnowledge Graph Question Answering\u003c\u002fb\u003e\u003cbr\u003eCoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering\u003cbr\u003eRight for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering\u003cbr\u003eGenerate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering\u003cbr\u003eRetrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering\u003cbr\u003eQuestion-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering\u003cbr\u003eA Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval\u003cbr\u003eLess is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA","\u003cb\u003ePart-Of-Speech Tagging\u003c\u002fb\u003e\u003cbr\u003eMTLS: Making Texts into Linguistic Symbols\u003cbr\u003eLexically Grounded Subword Segmentation\u003cbr\u003eA Morphology-Based Investigation of Positional Encodings\u003cbr\u003eGeneralists vs. Specialists: Evaluating Large Language Models for Urdu\u003cbr\u003ePredicting generalization performance with correctness discriminators\u003cbr\u003eAFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks\u003cbr\u003eTargeted Multilingual Adaptation for Low-resource Language Families","\u003cb\u003eEntity Linking\u003c\u002fb\u003e\u003cbr\u003eWhen Reasoning Meets Information Aggregation: A Case Study with Sports Narratives\u003cbr\u003eOneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting\u003cbr\u003eUnsupervised Named Entity Disambiguation for Low Resource Domains\u003cbr\u003eVisual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant\u003cbr\u003eBMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers\u003cbr\u003eComparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval\u003cbr\u003eOneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs","\u003cb\u003eAdversarial Attack\u003c\u002fb\u003e\u003cbr\u003eAdaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks\u003cbr\u003eIs LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment\u003cbr\u003eReasoning Robustness of LLMs to Adversarial Typographical Errors\u003cbr\u003eRAFT: Realistic Attacks to Fool Text Detectors\u003cbr\u003eRAt: Injecting Implicit Bias for Text-To-Image Prompt Refinement Models\u003cbr\u003eRobust Text Classification: Analyzing Prototype-Based Networks\u003cbr\u003eAttacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions","\u003cb\u003eMulti-Hop Qa\u003c\u002fb\u003e\u003cbr\u003eRetrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation\u003cbr\u003eTowards Verifiable Text Generation with Evolving Memory and Self-Reflection\u003cbr\u003eFine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together\u003cbr\u003eFAME: Towards Factual Multi-Task Model Editing\u003cbr\u003eOneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs\u003cbr\u003eRefiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities\u003cbr\u003eStyle-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles","\u003cb\u003eImage-Text Retrieval\u003c\u002fb\u003e\u003cbr\u003eQuantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval\u003cbr\u003eDistilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP\u003cbr\u003eFrom Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models\u003cbr\u003eFineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension\u003cbr\u003ePreserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality\u003cbr\u003eInfrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models\u003cbr\u003eTransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling","\u003cb\u003eText Classification Sentiment Analysis\u003c\u002fb\u003e\u003cbr\u003eFine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates\u003cbr\u003eUniversal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning\u003cbr\u003eRethinking the Evaluation of In-Context Learning for LLMs\u003cbr\u003eQuantum Recurrent Architectures for Text Classification\u003cbr\u003eBayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities\u003cbr\u003eNavigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models\u003cbr\u003eTransfer Learning for Text Classification via Model Risk Analysis","\u003cb\u003eText Simplification\u003c\u002fb\u003e\u003cbr\u003eEvaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003eDEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing\u003cbr\u003eImproving Minimum Bayes Risk Decoding with Multi-Prompt\u003cbr\u003eEnable Fast Sampling for Seq2Seq Text Diffusion\u003cbr\u003eExpertEase: A Multi-Agent Framework for Grade-Specific Document Simplification with Large Language Models\u003cbr\u003eREADME: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP","\u003cb\u003eEvent Detection\u003c\u002fb\u003e\u003cbr\u003eGAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities\u003cbr\u003eADELIE: Aligning Large Language Models on Information Extraction\u003cbr\u003eLawBench: Benchmarking Legal Knowledge of Large Language Models\u003cbr\u003eExplicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments\u003cbr\u003eGeneral Collaborative Framework between Large Language Model and Experts for Universal Information Extraction\u003cbr\u003eBeyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models\u003cbr\u003eDebate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction","\u003cb\u003eIntent Classification\u003c\u002fb\u003e\u003cbr\u003eEH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eSelf-Powered LLM Modality Expansion for Large Speech-Text Models\u003cbr\u003eLLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement\u003cbr\u003eMulti-dimensional Evaluation of Empathetic Dialogue Responses\u003cbr\u003eLinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization\u003cbr\u003eClass Name Guided Out-of-Scope Intent Classification","\u003cb\u003eSlot Filling\u003c\u002fb\u003e\u003cbr\u003eEH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eTransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities\u003cbr\u003eDC-Instruct: An Effective Framework for Generative Multi-intent Spoken Language Understanding\u003cbr\u003eFlee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling\u003cbr\u003eMELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science\u003cbr\u003eDesigning Logic Pattern Templates for Counter-Argument Logical Structure Analysis","\u003cb\u003ePreference Optimization\u003c\u002fb\u003e\u003cbr\u003eEPO: Hierarchical LLM Agents with Environment Preference Optimization\u003cbr\u003eAligning Large Language Models with Diverse Political Viewpoints\u003cbr\u003eRLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs\u003cbr\u003eModel-based Preference Optimization in Abstractive Summarization without Human Feedback\u003cbr\u003eBAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization\u003cbr\u003eEnhancing Alignment using Curriculum Learning & Ranked Preferences\u003cbr\u003eMargin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback","\u003cb\u003eGrammatical Error Correction\u003c\u002fb\u003e\u003cbr\u003eA Survey of AMR Applications\u003cbr\u003eMulti-pass Decoding for Grammatical Error Correction\u003cbr\u003eLLM-based Code-Switched Text Generation for Grammatical Error Correction\u003cbr\u003eTo Err Is Human, but Llamas Can Learn It Too\u003cbr\u003eTo Ask LLMs about English Grammaticality, Prompt Them in a Different Language\u003cbr\u003eGazelle: An Instruction Dataset for Arabic Writing Assistance\u003cbr\u003eEfficient and Interpretable Grammatical Error Correction with Mixture of Experts","\u003cb\u003eEvent Argument Extraction\u003c\u002fb\u003e\u003cbr\u003eADELIE: Aligning Large Language Models on Information Extraction\u003cbr\u003eExplicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments\u003cbr\u003eDocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction\u003cbr\u003eOEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary\u003cbr\u003eMMUTF: Multimodal Multimedia Event Argument Extraction with Unified Template Filling\u003cbr\u003eMELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science\u003cbr\u003eDebate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction","\u003cb\u003eParaphrase Identification\u003c\u002fb\u003e\u003cbr\u003eGetting More from Less: Large Language Models are Good Spontaneous Multilingual Learners\u003cbr\u003eDo LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models\u003cbr\u003eFAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding\u003cbr\u003eVariational Language Concepts for Interpreting Foundation Language Models\u003cbr\u003eAre ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity\u003cbr\u003eFunctionality learning through specification instructions\u003cbr\u003eUnlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization","\u003cb\u003eError Analysis\u003c\u002fb\u003e\u003cbr\u003eASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?\u003cbr\u003eNOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition\u003cbr\u003eDISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers\u003cbr\u003eError Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation\u003cbr\u003eStep-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?\u003cbr\u003eWhat's under the hood: Investigating Automatic Metrics on Meeting Summarization\u003cbr\u003eDifficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs","\u003cb\u003eStance Detection\u003c\u002fb\u003e\u003cbr\u003eThe Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification\u003cbr\u003eDiversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets\u003cbr\u003eEnhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research\u003cbr\u003eI love pineapple on pizza != I hate pineapple on pizza: Stance-Aware Sentence Transformers for Opinion Mining\u003cbr\u003eStanceformer: Target-Aware Transformer for Stance Detection\u003cbr\u003eToeing the Party Line: Election Manifestos as a Key to Understand Political Discourse on Twitter\u003cbr\u003eHow to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models","\u003cb\u003eKnowledge Graph Completion\u003c\u002fb\u003e\u003cbr\u003eImproving Knowledge Graph Completion with Structure-Aware Supervised Contrastive Learning\u003cbr\u003eMoCoKGC: Momentum Contrast Entity Encoding for Knowledge Graph Completion\u003cbr\u003eJoint Pre-Encoding Representation and Sturcture Embedding for Efficient and Low-Resource Knowledge Graph Completion\u003cbr\u003eVarying Sentence Representations via Condition-Specified Routers\u003cbr\u003eContext-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs\u003cbr\u003eTemporal Fact Reasoning over Hyper-Relational Knowledge Graphs\u003cbr\u003eSALMON: A Structure-Aware Language Model with logicality and densification strategy for Temporal Knowledge Graph Reasoning","\u003cb\u003ePrompt Optimization\u003c\u002fb\u003e\u003cbr\u003ePrompts have evil twins\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eOptimizing Instructions and Demonstrations for Multi-Stage Language Model Programs\u003cbr\u003eFewer is More: Boosting Math Reasoning with Reinforced Context Pruning\u003cbr\u003eSelf-Renewal Prompt Optimizing with Implicit Reasoning\u003cbr\u003eLLM as a metric critic for low resource relation identification","\u003cb\u003ePersuasion\u003c\u002fb\u003e\u003cbr\u003eLLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay\u003cbr\u003eAn LLM Feature-based Framework for Dialogue Constructiveness Assessment\u003cbr\u003eIntegrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation\u003cbr\u003eAutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments\u003cbr\u003eBeyond Persuasion: Towards Conversational Recommender System with Credible Explanations\u003cbr\u003eZero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval","\u003cb\u003eMultimodal Learning\u003c\u002fb\u003e\u003cbr\u003eWhen LLMs Meet Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection\u003cbr\u003eTowards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale\u003cbr\u003eMEANT: Multimodal Encoder for Antecedent Information\u003cbr\u003eMatryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions\u003cbr\u003eMMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems\u003cbr\u003eDifficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs","\u003cb\u003eKnowledge Injection\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eLEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models\u003cbr\u003eTowards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale\u003cbr\u003eLarge Language Models Can Be Contextual Privacy Protection Learners\u003cbr\u003eSynthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models\u003cbr\u003eLlama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection","\u003cb\u003ePlanning\u003c\u002fb\u003e\u003cbr\u003eMSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making\u003cbr\u003eRetrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation\u003cbr\u003eMAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration\u003cbr\u003eMathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models\u003cbr\u003eOn the Empirical Complexity of Reasoning and Planning in LLMs\u003cbr\u003eAn Evaluation Mechanism of LLM-based Agents on Manipulating APIs","\u003cb\u003eQa\u003c\u002fb\u003e\u003cbr\u003eLONGEMBED: Extending Embedding Models for Long Context Retrieval\u003cbr\u003eDancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models\u003cbr\u003eXplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs\u003cbr\u003eFAME: Towards Factual Multi-Task Model Editing\u003cbr\u003eA Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios\u003cbr\u003eA Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models","\u003cb\u003eEmotion Classification\u003c\u002fb\u003e\u003cbr\u003eIntegrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification\u003cbr\u003eMessage Passing on Semantic-Anchor-Graphs for Fine-grained Emotion Representation Learning and Classification\u003cbr\u003eDoes Large Language Model Contain Task-Specific Neurons?\u003cbr\u003eUnveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism\u003cbr\u003eUnderstanding Slang with LLMs: Modelling Cross-Cultural Nuances through Paraphrasing\u003cbr\u003eTUTOR-ICL: Guiding Large Language Models for Improved In-Context Learning Performance","\u003cb\u003eAbstractive Summarization\u003c\u002fb\u003e\u003cbr\u003eEffective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process\u003cbr\u003eSafely Learning with Private Data: A Federated Learning Framework for Large Language Model\u003cbr\u003eAn Audit on the Perspectives and Challenges of Hallucinations in NLP\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003eModel-based Preference Optimization in Abstractive Summarization without Human Feedback\u003cbr\u003eMeasuring the Robustness of NLP Models to Domain Shifts","\u003cb\u003eScienceqa\u003c\u002fb\u003e\u003cbr\u003eModel Balancing Helps Low-data Training and Fine-tuning\u003cbr\u003eARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback\u003cbr\u003eOn Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning\u003cbr\u003eAlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality\u003cbr\u003eAdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models\u003cbr\u003eTowards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks","\u003cb\u003eFact-Checking\u003c\u002fb\u003e\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003eMiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents\u003cbr\u003eUnknown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data\u003cbr\u003eM\u00b3D: MultiModal MultiDocument Fine-Grained Inconsistency Detection\u003cbr\u003eHow to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models\u003cbr\u003eFactcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers","\u003cb\u003eReinforcement Learning\u003c\u002fb\u003e\u003cbr\u003eVIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation\u003cbr\u003eTeaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use\u003cbr\u003eText2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback\u003cbr\u003eWeak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems\u003cbr\u003eCOFFEE-GYM: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code\u003cbr\u003eNavigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models","\u003cb\u003eMulti-Hop Question Answering\u003c\u002fb\u003e\u003cbr\u003eEfficientRAG: Efficient Retriever for Multi-Hop Question Answering\u003cbr\u003eFrom RAG to RICHES: Retrieval Interlaced with Sequence Generation\u003cbr\u003eRe-ReST: Reflection-Reinforced Self-Training for Language Agents\u003cbr\u003eRIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning\u003cbr\u003eAdaptive Token Biaser: Knowledge Editing via Biasing Key Entities\u003cbr\u003eLLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments","\u003cb\u003eMachine Translation Evaluation\u003c\u002fb\u003e\u003cbr\u003eWhat do Large Language Models Need for Machine Translation Evaluation?\u003cbr\u003eBeyond Reference: Evaluating High Quality Translations Better than Human References\u003cbr\u003eMMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language\u003cbr\u003ePrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation\u003cbr\u003eCan Automatic Metrics Assess High-Quality Translations?\u003cbr\u003eBLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation","\u003cb\u003eTask-Oriented Dialogue\u003c\u002fb\u003e\u003cbr\u003eBootstrapped Policy Learning for Task-oriented Dialogue through Goal Shaping\u003cbr\u003eUnsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of the Noisy Channel\u003cbr\u003eWhat are the Generator Preferences for End-to-end Task-Oriented Dialog System?\u003cbr\u003eTransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities\u003cbr\u003eLearning from Implicit User Feedback, Emotions and Demographic Information in Task-Oriented and Document-Grounded Dialogues\u003cbr\u003eRewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue","\u003cb\u003eVisual Grounding\u003c\u002fb\u003e\u003cbr\u003eWorld to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering\u003cbr\u003eUOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models\u003cbr\u003eShaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling\u003cbr\u003eInvestigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models\u003cbr\u003ePropTest: Automatic Property Testing for Improved Visual Programming\u003cbr\u003eVDebugger: Harnessing Execution Feedback for Debugging Visual Programs","\u003cb\u003eLong-Form Generation\u003c\u002fb\u003e\u003cbr\u003eRetrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation\u003cbr\u003eSynchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation\u003cbr\u003eAnalysis of Plan-based Retrieval for Grounded Text Generation\u003cbr\u003eMulti-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models\u003cbr\u003eCalibrating Long-form Generations from Large Language Models\u003cbr\u003eDownstream Trade-offs of a Family of Text Watermarks","\u003cb\u003eSynthetic Data Generation\u003c\u002fb\u003e\u003cbr\u003eA Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners\u003cbr\u003eLLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives\u003cbr\u003eSynthetic Multimodal Question Generation\u003cbr\u003eBetter Alignment with Instruction Back-and-Forth Translation\u003cbr\u003eAligners: Decoupling LLMs and Alignment\u003cbr\u003eToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information","\u003cb\u003eQqp\u003c\u002fb\u003e\u003cbr\u003eUnveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement\u003cbr\u003eWhen Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models\u003cbr\u003eWhich Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?\u003cbr\u003eIntermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models\u003cbr\u003eFine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation\u003cbr\u003eA Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune","\u003cb\u003eSentiment Analysis Topic Classification\u003c\u002fb\u003e\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eUniversal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning\u003cbr\u003eCorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs\u003cbr\u003ePrompt-Based Bias Calibration for Better Zero\u002fFew-Shot Learning of Language Models\u003cbr\u003eLexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons\u003cbr\u003eInference and Verbalization Functions During In-Context Learning","\u003cb\u003eResponse Generation\u003c\u002fb\u003e\u003cbr\u003eRelevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System\u003cbr\u003eWhat are the Generator Preferences for End-to-end Task-Oriented Dialog System?\u003cbr\u003eIntegrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation\u003cbr\u003eThoughts to Target: Enhance Planning for Target-driven Conversation\u003cbr\u003eRewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue\u003cbr\u003eSARCAT: Generative Span-Act Guided Response Generation Using Copy-Enhanced Target Augmentation","\u003cb\u003ePreference Alignment\u003c\u002fb\u003e\u003cbr\u003ePredicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model\u003cbr\u003eEncourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective\u003cbr\u003eORPO: Monolithic Preference Optimization without Reference Model\u003cbr\u003eFlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization\u003cbr\u003eSelf-Renewal Prompt Optimizing with Implicit Reasoning\u003cbr\u003eSelf-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness","\u003cb\u003eToxicity Reduction\u003c\u002fb\u003e\u003cbr\u003eRSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework\u003cbr\u003eUnlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding\u003cbr\u003eIntrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis\u003cbr\u003eTowards Aligning Language Models with Textual Feedback\u003cbr\u003eMulti-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models\u003cbr\u003ePromoting Constructive Deliberation: Reframing for Receptiveness","\u003cb\u003eQuestion Answering Summarization\u003c\u002fb\u003e\u003cbr\u003eA Survey of AMR Applications\u003cbr\u003eSynchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation\u003cbr\u003eLongAlign: A Recipe for Long Context Alignment of Large Language Models\u003cbr\u003eSH2: Self-Highlighted Hesitation Helps You Decode More Truthfully\u003cbr\u003eGeneralists vs. Specialists: Evaluating Large Language Models for Urdu\u003cbr\u003eCalibrating Long-form Generations from Large Language Models","\u003cb\u003eTransfer Learning\u003c\u002fb\u003e\u003cbr\u003eAdvancing Adversarial Suffix Transfer Learning on Aligned Large Language Models\u003cbr\u003eNuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data\u003cbr\u003eData Contamination Can Cross Language Barriers\u003cbr\u003eScaling Sentence Embeddings with Large Language Models\u003cbr\u003eTrain Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation\u003cbr\u003eMitigating Catastrophic Forgetting in Language Transfer via Model Merging","\u003cb\u003eVqa\u003c\u002fb\u003e\u003cbr\u003eSURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information\u003cbr\u003eCommVQA: Situating Visual Question Answering in Communicative Contexts\u003cbr\u003eVGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning\u003cbr\u003eMMedAgent: Learning to Use Medical Tools with Multi-modal Agent\u003cbr\u003eMobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding\u003cbr\u003eImproving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design","\u003cb\u003eCode Completion\u003c\u002fb\u003e\u003cbr\u003eCoBa: Convergence Balancer for Multitask Finetuning of Large Language Models\u003cbr\u003eIntroducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly\u003cbr\u003eKV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches\u003cbr\u003eAda-Instruct: Adapting Instruction Generators for Complex Reasoning\u003cbr\u003eLength Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding\u003cbr\u003eFrom Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression","\u003cb\u003eOpen-Domain Qa\u003c\u002fb\u003e\u003cbr\u003eTowards Verifiable Text Generation with Evolving Memory and Self-Reflection\u003cbr\u003eAGRAME: Any-Granularity Ranking with Multi-Vector Embeddings\u003cbr\u003eImprove Dense Passage Retrieval with Entailment Tuning\u003cbr\u003eDense X Retrieval: What Retrieval Granularity Should We Use?\u003cbr\u003eNot All Contexts Are Equal: Teaching LLMs Credibility-aware Generation\u003cbr\u003eLearning to Paraphrase for Alignment with LLM Preference","\u003cb\u003eMisinformation Detection\u003c\u002fb\u003e\u003cbr\u003eMisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"\u003cbr\u003eEnhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research\u003cbr\u003eSPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness\u003cbr\u003eDecoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach\u003cbr\u003eHow to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models\u003cbr\u003eMisinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation","\u003cb\u003eCoding\u003c\u002fb\u003e\u003cbr\u003eA Thorough Examination of Decoding Methods in the Era of LLMs\u003cbr\u003eHow to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective\u003cbr\u003eLongAlign: A Recipe for Long Context Alignment of Large Language Models\u003cbr\u003eAUTODETECT: Towards a Unified Framework for Automated Weakness Detection in Large Language Models\u003cbr\u003eLLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement\u003cbr\u003eMerge to Learn: Efficiently Adding Skills to Language Models with Model Merging","\u003cb\u003eMulti-Label Classification\u003c\u002fb\u003e\u003cbr\u003eMOSEL: Inference Serving Using Dynamic Modality Selection\u003cbr\u003eA Closer Look at Multidimensional Online Political Incivility\u003cbr\u003eStill Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis\u003cbr\u003eGraph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification\u003cbr\u003eRandom Label Forests: An Ensemble Method with Label Subsampling For Extreme Multi-Label Problems\u003cbr\u003eCHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models","\u003cb\u003eQuestion Answering Rag\u003c\u002fb\u003e\u003cbr\u003eAssessing \"Implicit\" Retrieval Robustness of Large Language Models\u003cbr\u003eATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR\u003cbr\u003eSearching for Best Practices in Retrieval-Augmented Generation\u003cbr\u003eRetrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation\u003cbr\u003eR2AG: Incorporating Retrieval Information into Retrieval Augmented Generation\u003cbr\u003eFrom Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression","\u003cb\u003eRecommendation Systems\u003c\u002fb\u003e\u003cbr\u003eEfficient Sequential Decision Making with Large Language Models\u003cbr\u003ePepRec: Progressive Enhancement of Prompting for Recommendation\u003cbr\u003eJump Starting Bandits with LLM-Generated Prior Knowledge\u003cbr\u003eI-AM-G: Interest Augmented Multimodal Generator for Item Personalization\u003cbr\u003eQUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware\u003cbr\u003eZero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval","\u003cb\u003eBenchmark\u003c\u002fb\u003e\u003cbr\u003eRuBLiMP: Russian Benchmark of Linguistic Minimal Pairs\u003cbr\u003eAKEW: Assessing Knowledge Editing in the Wild\u003cbr\u003eThe Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning\u003cbr\u003eForecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling\u003cbr\u003eFactcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers\u003cbr\u003eVarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation","\u003cb\u003eNamed Entity Recognition Relation Extraction\u003c\u002fb\u003e\u003cbr\u003eBio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints\u003cbr\u003eSciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents\u003cbr\u003eLarge Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark\u003cbr\u003eA Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction\u003cbr\u003eC-ICL: Contrastive In-context Learning for Information Extraction\u003cbr\u003eIs There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases","\u003cb\u003eCross-Lingual Transfer\u003c\u002fb\u003e\u003cbr\u003eBreaking the Curse of Multilinguality with Cross-lingual Expert Language Models\u003cbr\u003eREADME++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment\u003cbr\u003e\u003eWhy Does New Knowledge Create Messy Ripple Effects in LLMs?\u003cbr\u003eCross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing\u003cbr\u003eEfficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models\u003cbr\u003eSelf-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages","\u003cb\u003eModel Compression\u003c\u002fb\u003e\u003cbr\u003eStructured Optimal Brain Pruning for Large Language Models\u003cbr\u003ePruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging\u003cbr\u003ePromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning\u003cbr\u003eChange Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy\u003cbr\u003eOptimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs\u003cbr\u003eWhen Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models","\u003cb\u003eReward Modeling\u003c\u002fb\u003e\u003cbr\u003eImproving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003eReward Modeling Requires Automatic Adjustment Based on Data Quality\u003cbr\u003eSemi-Supervised Reward Modeling via Iterative Self-Training\u003cbr\u003eOn Diversified Preferences of Large Language Model Alignment\u003cbr\u003eInterpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts","\u003cb\u003eTemporal Reasoning\u003c\u002fb\u003e\u003cbr\u003eTemporally Consistent Factuality Probing for Large Language Models\u003cbr\u003eCAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans\u003cbr\u003eMIBench: Evaluating Multimodal Large Language Models over Multiple Images\u003cbr\u003eDyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs\u003cbr\u003eRemember This Event That Year? Assessing Temporal Information and Understanding in Large Language Models\u003cbr\u003eNARRATIVE-OF-THOUGHT: Improving Temporal Reasoning of Large Language Models via Recounted Narratives","\u003cb\u003eBenchmarking\u003c\u002fb\u003e\u003cbr\u003eClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures\u003cbr\u003eNOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition\u003cbr\u003eFlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents\u003cbr\u003eA Recipe to Train Powerful Romanian LLMs with English Instructions\u003cbr\u003eTowards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks\u003cbr\u003eBLADE: Benchmarking Language Model Agents for Data-Driven Science","\u003cb\u003eOpenbookqa\u003c\u002fb\u003e\u003cbr\u003eCHESS : Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification\u003cbr\u003eAlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality\u003cbr\u003eZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering\u003cbr\u003eAdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models\u003cbr\u003eDLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model\u003cbr\u003eMaking Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning","\u003cb\u003eData Annotation\u003c\u002fb\u003e\u003cbr\u003eMULTI-NEWS+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation\u003cbr\u003eLarge Language Models for Data Annotation and Synthesis: A Survey\u003cbr\u003eOATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants\u003cbr\u003eNeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries\u003cbr\u003eSelective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model","\u003cb\u003eData Generation\u003c\u002fb\u003e\u003cbr\u003eTable Question Answering for Low-resourced Indic Languages\u003cbr\u003eLeveraging pre-trained language models for linguistic analysis: A case of argument structure constructions\u003cbr\u003eEfficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts\u003cbr\u003eCreative and Context-Aware Translation of East Asian Idioms with GPT-4\u003cbr\u003eDiverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking","\u003cb\u003eWord Sense Disambiguation\u003c\u002fb\u003e\u003cbr\u003eHateful Word in Context Classification\u003cbr\u003eFOOL ME IF YOU CAN! An Adversarial Dataset to Investigate the Robustness of LMs in Word Sense Disambiguation\u003cbr\u003eMore DWUGs: Extending and Evaluating Word Usage Graph Datasets in Multiple Languages\u003cbr\u003eAC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models\u003cbr\u003eHit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention","\u003cb\u003eDownstream Task Performance\u003c\u002fb\u003e\u003cbr\u003e\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models\u003cbr\u003eMitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing\u003cbr\u003eTarget-Aware Language Modeling via Granular Data Sampling\u003cbr\u003eCultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies\u003cbr\u003eLPZero: Language Model Zero-cost Proxy Search from Zero","\u003cb\u003eRobustness Evaluation\u003c\u002fb\u003e\u003cbr\u003eRoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning\u003cbr\u003eAssessing \"Implicit\" Retrieval Robustness of Large Language Models\u003cbr\u003eOn the Robustness of Editing Large Language Models\u003cbr\u003eA Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios\u003cbr\u003eLanguage Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks","\u003cb\u003eRanking\u003c\u002fb\u003e\u003cbr\u003eConsolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing\u003cbr\u003eAGRAME: Any-Granularity Ranking with Multi-Vector Embeddings\u003cbr\u003eTROTR: A Framework for Evaluating the Recontextualization of Text\u003cbr\u003eMake Large Language Model a Better Ranker\u003cbr\u003eA Study of Implicit Ranking Unfairness in Large Language Models","\u003cb\u003ePiqa\u003c\u002fb\u003e\u003cbr\u003eTokenization Is More Than Compression\u003cbr\u003eCHESS : Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification\u003cbr\u003eZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering\u003cbr\u003eInstruction Fine-Tuning: Does Prompt Loss Matter?\u003cbr\u003eDLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model","\u003cb\u003eRed Teaming\u003c\u002fb\u003e\u003cbr\u003eFLIRT: Feedback Loop In-context Red Teaming\u003cbr\u003eHolistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction\u003cbr\u003eCoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference\u003cbr\u003eSTAR: SocioTechnical Approach to Red Teaming Language Models\u003cbr\u003eBe a Multitude to Itself: A Prompt Evolution Framework for Red Teaming","\u003cb\u003eHuman-Ai Collaboration\u003c\u002fb\u003e\u003cbr\u003eSuccessfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections\u003cbr\u003eOn Evaluating Explanation Utility for Human-AI Decision Making in NLP\u003cbr\u003eAsk the experts: Sourcing a high-quality nutrition counseling dataset through Human-AI collaboration\u003cbr\u003eREADME: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP\u003cbr\u003eSocratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation","\u003cb\u003eDocument Retrieval\u003c\u002fb\u003e\u003cbr\u003eDyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities\u003cbr\u003eUnifying Multimodal Retrieval via Document Screenshot Embedding\u003cbr\u003eMixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity\u003cbr\u003eThreshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval\u003cbr\u003eSeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation","\u003cb\u003eParameter-Efficient Fine-Tuning\u003c\u002fb\u003e\u003cbr\u003eLet the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models\u003cbr\u003eLearning to Route for Dynamic Adapter Composition in Continual Learning with Language Models\u003cbr\u003eLayer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models\u003cbr\u003eBIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models\u003cbr\u003ePromoting Data and Model Privacy in Federated Learning through Quantized LoRA","\u003cb\u003eAddsub\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eFrom Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eAlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality","\u003cb\u003eAqua\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eFrom Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eBeyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication","\u003cb\u003eCsqa\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering\u003cbr\u003eCan LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks","\u003cb\u003eDate Understanding\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eWorking Memory Identifies Reasoning Limits in Language Models\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eMirror-Consistency: Harnessing Inconsistency in Majority Voting\u003cbr\u003eAuto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework","\u003cb\u003eSst-2\u003c\u002fb\u003e\u003cbr\u003eAdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning\u003cbr\u003eOn Training Data Influence of GPT Models\u003cbr\u003eFisher Information-based Efficient Curriculum Federated Learning with Large Language Models\u003cbr\u003eIntermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models\u003cbr\u003eA Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune","\u003cb\u003eMedical Visual Question Answering\u003c\u002fb\u003e\u003cbr\u003eRULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models\u003cbr\u003eMedCoT: Medical Chain of Thought via Hierarchical Expert\u003cbr\u003eSelf-Training Large Language and Vision Assistant for Medical Question-Answering\u003cbr\u003eMed-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models\u003cbr\u003eLight-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models","\u003cb\u003eExplanation Generation\u003c\u002fb\u003e\u003cbr\u003eCryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading\u003cbr\u003eVerification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving\u003cbr\u003eXplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs\u003cbr\u003eEnhancing High-order Interaction Awareness in LLM-based Recommender Model\u003cbr\u003eBeyond Persuasion: Towards Conversational Recommender System with Credible Explanations","\u003cb\u003eBenchmark Creation\u003c\u002fb\u003e\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eThe Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention\u003cbr\u003eMalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language\u003cbr\u003eCONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules\u003cbr\u003eSedarEval: Automated Evaluation using Self-Adaptive Rubrics","\u003cb\u003eVisual Question Answering Image Captioning\u003c\u002fb\u003e\u003cbr\u003eTag-grounded Visual Instruction Tuning with Retrieval Augmentation\u003cbr\u003eUOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models\u003cbr\u003eTowards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale\u003cbr\u003eTraining-free Deep Concept Injection Enables Language Models for Video Question Answering\u003cbr\u003eInfrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models","\u003cb\u003eMultiarith\u003c\u002fb\u003e\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eFrom Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning\u003cbr\u003eAlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality","\u003cb\u003eInterpretability\u003c\u002fb\u003e\u003cbr\u003eBackward Lens: Projecting Language Model Gradients into the Vocabulary Space\u003cbr\u003eBeyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning\u003cbr\u003eAxis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings\u003cbr\u003eVariational Language Concepts for Interpreting Foundation Language Models\u003cbr\u003eSeeing Through VisualBERT: A Causal Adventure on Memetic Landscapes","\u003cb\u003eText Style Transfer\u003c\u002fb\u003e\u003cbr\u003eReusing Transferable Weight Increments for Low-resource Style Generation\u003cbr\u003eStyle-Specific Neurons for Steering LLMs in Text Style Transfer\u003cbr\u003eTowards Pareto-Efficient RLHF: Paying Attention to a Few High-Reward Samples with Reward Dropout\u003cbr\u003eMORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization\u003cbr\u003eTINYSTYLER: Efficient Few-Shot Text Style Transfer with Authorship Embeddings","\u003cb\u003eMultiple-Choice Question Answering\u003c\u002fb\u003e\u003cbr\u003eCalibrating the Confidence of Large Language Models by Eliciting Fidelity\u003cbr\u003eScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws\u003cbr\u003eGPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning\u003cbr\u003eIn-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models\u003cbr\u003eBiMediX: Bilingual Medical Mixture of Experts LLM","\u003cb\u003eDocument Classification\u003c\u002fb\u003e\u003cbr\u003eDocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models\u003cbr\u003eUnlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization\u003cbr\u003eLarge Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark\u003cbr\u003eDe-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP\u003cbr\u003eVE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models","\u003cb\u003eDependency Parsing\u003c\u002fb\u003e\u003cbr\u003ePixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models\u003cbr\u003eCSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free-Word-Ordered and Morphologically-Rich Low-Resource Languages\u003cbr\u003eContribution of Linguistic Typology to Universal Dependency Parsing: An Empirical Investigation\u003cbr\u003eA Morphology-Based Investigation of Positional Encodings\u003cbr\u003eRepresentation Alignment and Adversarial Networks for Cross-lingual Dependency Parsing","\u003cb\u003eActive Learning\u003c\u002fb\u003e\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003eSelf-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models\u003cbr\u003eAnnotator-Centric Active Learning for Subjective NLP Tasks\u003cbr\u003eOn the Fragility of Active Learners for Text Classification\u003cbr\u003eActive Learning for Abstractive Text Summarization via LLM-Determined Curriculum and Certainty Gain Maximization","\u003cb\u003eOpen-Ended Generation\u003c\u002fb\u003e\u003cbr\u003eEstimating Knowledge in Large Language Models Without Generating a Single Token\u003cbr\u003eRSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework\u003cbr\u003eRLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs\u003cbr\u003eFactuality of Large Language Models: A Survey\u003cbr\u003eAdaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation","\u003cb\u003eLanguage Model Evaluation\u003c\u002fb\u003e\u003cbr\u003ePROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models\u003cbr\u003eFirst Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning\u003cbr\u003eA Psycholinguistic Evaluation of Language Models' Sensitivity to Argument Roles\u003cbr\u003eEfficiently Computing Susceptibility to Context in Language Models\u003cbr\u003eVarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation","\u003cb\u003eOffensive Language Detection\u003c\u002fb\u003e\u003cbr\u003eLeveraging Conflicts in Social Media Posts: Unintended Offense Dataset\u003cbr\u003eToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations\u003cbr\u003eEnhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research\u003cbr\u003eGiving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases\u003cbr\u003eBanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla","\u003cb\u003eMrpc\u003c\u002fb\u003e\u003cbr\u003eUnveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement\u003cbr\u003eFisher Information-based Efficient Curriculum Federated Learning with Large Language Models\u003cbr\u003eWhich Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?\u003cbr\u003eIntermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models\u003cbr\u003eA Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune","\u003cb\u003eImage Captioning Visual Question Answering\u003c\u002fb\u003e\u003cbr\u003eHow Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?\u003cbr\u003eThe Instinctive Bias: Spurious Images lead to Illusion in MLLMs\u003cbr\u003eInvestigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models\u003cbr\u003eNearest Neighbor Normalization Improves Multimodal Retrieval\u003cbr\u003ePreserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models","\u003cb\u003eHallucination Reduction\u003c\u002fb\u003e\u003cbr\u003eVLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment\u003cbr\u003eMDPO: Conditional Preference Optimization for Multimodal Large Language Models\u003cbr\u003eSYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization\u003cbr\u003eSnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM\u003cbr\u003eSelf-training Large Language Models through Knowledge Detection","\u003cb\u003eAbstract Reasoning\u003c\u002fb\u003e\u003cbr\u003eCan Large Language Models Learn Independent Causal Mechanisms?\u003cbr\u003eANALOBENCH: Benchmarking the Identification of Abstract and Long-context Analogies\u003cbr\u003eConnecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game\u003cbr\u003eInference Helps PLMs' Conceptual Understanding: Improving the Abstract Inference Ability with Hierarchical Conceptual Entailment Graphs\u003cbr\u003eCELLO: Causal Evaluation of Large Vision-Language Models","\u003cb\u003eContinual Pre-Training\u003c\u002fb\u003e\u003cbr\u003eBreaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale\u003cbr\u003eA Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models\u003cbr\u003eCMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models\u003cbr\u003eImproving Referring Ability for Biomedical Language Models\u003cbr\u003eUnlocking Continual Learning Abilities in Language Models","\u003cb\u003eWatermarking\u003c\u002fb\u003e\u003cbr\u003ePOSTMARK: A Robust Blackbox Watermark for Large Language Models\u003cbr\u003eWhere Am I From? Identifying Origin of LLM-generated Content\u003cbr\u003eContext-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models\u003cbr\u003eGuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack\u003cbr\u003eCODEIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code","\u003cb\u003eDialogue Systems\u003c\u002fb\u003e\u003cbr\u003eEfficient Sequential Decision Making with Large Language Models\u003cbr\u003eIntegrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation\u003cbr\u003ePcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity\u003cbr\u003eFrom Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues\u003cbr\u003eCantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues","\u003cb\u003eTask Planning\u003c\u002fb\u003e\u003cbr\u003eOmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer\u003cbr\u003eSmall LLMs Are Weak Tool Learners: A Multi-LLM Agent\u003cbr\u003eE2CL: Exploration-based Error Correction Learning for Embodied Agents\u003cbr\u003eTrustAgent: Towards Safe and Trustworthy LLM-based Agents\u003cbr\u003eLearning to Use Tools via Cooperative and Interactive Agents with Large Language Models","\u003cb\u003eAsr\u003c\u002fb\u003e\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eSelf-Powered LLM Modality Expansion for Large Speech-Text Models\u003cbr\u003eContinual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech\u003cbr\u003eTokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR\u003cbr\u003eSTTATTS: Unified Speech-To-Text And Text-To-Speech Model","\u003cb\u003eNeural Machine Translation\u003c\u002fb\u003e\u003cbr\u003eDomain adapted machine translation: What does catastrophic forgetting forget and why?\u003cbr\u003eGranularity is crucial when applying differential privacy to text: An investigation for neural machine translation\u003cbr\u003eDICTDIS: Dictionary Constrained Disambiguation for Improved NMT\u003cbr\u003eFinding the Optimal Byte-Pair Encoding Merge Operations for Neural Machine Translation in a Low-Resource Setting\u003cbr\u003eDoes Context Help Mitigate Gender Bias in Neural Machine Translation?","\u003cb\u003eInstruction-Following\u003c\u002fb\u003e\u003cbr\u003eHow to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective\u003cbr\u003eMitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging\u003cbr\u003eEvolutionary Contrastive Distillation for Language Model Alignment\u003cbr\u003ePromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning\u003cbr\u003eAUTODETECT: Towards a Unified Framework for Automated Weakness Detection in Large Language Models","\u003cb\u003eBias Evaluation\u003c\u002fb\u003e\u003cbr\u003eBiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs\u003cbr\u003eMetrics for What, Metrics for Whom: Assessing Actionability of Bias Evaluation Metrics in NLP\\\u003cbr\u003eA Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models\u003cbr\u003eGender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts\u003cbr\u003eBeyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression","\u003cb\u003eInstruction Fine-Tuning\u003c\u002fb\u003e\u003cbr\u003eHiFT: A Hierarchical Full Parameter Fine-Tuning Strategy\u003cbr\u003eWalia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets\u003cbr\u003eLongAlign: A Recipe for Long Context Alignment of Large Language Models\u003cbr\u003eRevisiting Catastrophic Forgetting in Large Language Model Tuning\u003cbr\u003eLexMatcher: Dictionary-centric Data Curation for LLM-based Machine Translation","\u003cb\u003eMultimodal Reasoning\u003c\u002fb\u003e\u003cbr\u003eTV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning\u003cbr\u003eCONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models\u003cbr\u003eImproving Multi-Agent Debate with Sparse Communication Topology\u003cbr\u003eLarge Language Models Are Challenged by Habitat-Centered Reasoning\u003cbr\u003eKnowledge-Aware Reasoning over Multimodal Semi-structured Tables","\u003cb\u003eSpatial Reasoning\u003c\u002fb\u003e\u003cbr\u003eGrounding Language in Multi-Perspective Referential Communication\u003cbr\u003eWhiteboard-of-Thought: Thinking Step-by-Step Across Modalities\u003cbr\u003eBeyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models\u003cbr\u003eIntroducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation\u003cbr\u003eALANAVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding"],"textposition":"none","x":["Question Answering","Text Classification","Language Modeling","Sentiment Analysis","Visual Question Answering","Summarization","Natural Language Inference","Commonsense Reasoning","Machine Translation","Text Generation","Instruction Following","Mathematical Reasoning","In-Context Learning","Code Generation","Named Entity Recognition","Reasoning","Rag","Information Retrieval","Image Captioning","Natural Language Processing","Llm Evaluation","Bias Detection","Reading Comprehension","Classification","Data Augmentation","Text Summarization","Instruction Tuning","Arithmetic Reasoning","Natural Language Understanding","Hallucination Detection","Zero-Shot Learning","Relation Extraction","Few-Shot Learning","Hate Speech Detection","Gsm8K","Bias Mitigation","Natural Language Generation","Logical Reasoning","Reinforcement Learning From Human Feedback","Llms","Knowledge Editing","Dialogue Generation","Language Understanding","Open-Domain Question Answering","Knowledge Distillation","Llm Alignment","Automatic Speech Recognition","Topic Classification","Language Models","Translation","Fine-Tuning","Generalization","Visual Reasoning","Medical Question Answering","Text-To-Image Generation","Speech Recognition","Evaluation","Event Extraction","Generation","Information Extraction","Math Problem Solving","Language Model Alignment","Math","Rte","Glue","Hallucination Mitigation","Math Reasoning","Multi-Hop Reasoning","Semantic Textual Similarity","Topic Modeling","Quantization","Video Question Answering","Link Prediction","Calibration","Emotion Recognition","Svamp","Question Generation","Textual Entailment","Toxicity Detection","Fact Verification","Sentiment Classification","Dataset Creation","Multi-Task Learning","Coreference Resolution","Continual Learning","Text-To-Sql","Story Generation","Domain Adaptation","Sentiment Analysis Natural Language Inference","Model Editing","Binary Classification","Strategyqa","Paraphrase Detection","Qnli","Mnli","Boolq","Prompt Engineering","Bias Analysis","Superglue","Language Generation","Arithmetic Reasoning Commonsense Reasoning","Image Classification","Preference Learning","Semantic Parsing","Mmlu","Error Correction","Knowledge Graph Question Answering","Part-Of-Speech Tagging","Entity Linking","Adversarial Attack","Multi-Hop Qa","Image-Text Retrieval","Text Classification Sentiment Analysis","Text Simplification","Event Detection","Intent Classification","Slot Filling","Preference Optimization","Grammatical Error Correction","Event Argument Extraction","Paraphrase Identification","Error Analysis","Stance Detection","Knowledge Graph Completion","Prompt Optimization","Persuasion","Multimodal Learning","Knowledge Injection","Planning","Qa","Emotion Classification","Abstractive Summarization","Scienceqa","Fact-Checking","Reinforcement Learning","Multi-Hop Question Answering","Machine Translation Evaluation","Task-Oriented Dialogue","Visual Grounding","Long-Form Generation","Synthetic Data Generation","Qqp","Sentiment Analysis Topic Classification","Response Generation","Preference Alignment","Toxicity Reduction","Question Answering Summarization","Transfer Learning","Vqa","Code Completion","Open-Domain Qa","Misinformation Detection","Coding","Multi-Label Classification","Question Answering Rag","Recommendation Systems","Benchmark","Named Entity Recognition Relation Extraction","Cross-Lingual Transfer","Model Compression","Reward Modeling","Temporal Reasoning","Benchmarking","Openbookqa","Data Annotation","Data Generation","Word Sense Disambiguation","Downstream Task Performance","Robustness Evaluation","Ranking","Piqa","Red Teaming","Human-Ai Collaboration","Document Retrieval","Parameter-Efficient Fine-Tuning","Addsub","Aqua","Csqa","Date Understanding","Sst-2","Medical Visual Question Answering","Explanation Generation","Benchmark Creation","Visual Question Answering Image Captioning","Multiarith","Interpretability","Text Style Transfer","Multiple-Choice Question Answering","Document Classification","Dependency Parsing","Active Learning","Open-Ended Generation","Language Model Evaluation","Offensive Language Detection","Mrpc","Image Captioning Visual Question Answering","Hallucination Reduction","Abstract Reasoning","Continual Pre-Training","Watermarking","Dialogue Systems","Task Planning","Asr","Neural Machine Translation","Instruction-Following","Bias Evaluation","Instruction Fine-Tuning","Multimodal Reasoning","Spatial Reasoning"],"y":[165,68,64,63,57,55,54,53,48,39,39,33,33,32,32,31,30,30,29,28,26,21,21,21,20,20,19,19,19,19,19,19,18,17,17,16,16,16,16,15,15,15,14,14,14,14,14,14,13,12,12,12,12,12,11,11,11,11,11,11,11,11,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5],"type":"bar"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"tickfont":{"size":12},"tickmode":"linear","tickangle":-60},"margin":{"l":50,"r":50,"t":80,"b":300},"font":{"size":14},"title":{"text":"EMNLP2024 Keyword"},"yaxis":{"title":{"text":"Number of papers"}},"height":700,"width":3200,"bargap":0.25},                        {"responsive": true}                    )                };            </script>        </div>
    <script>
    const ngramMap = {"Question Answering": ["Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection", "BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "Aligning Language Models to Explicitly Handle Ambiguity", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "PhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study", "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation", "Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism", "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation", "Where is the signal in tokenization space?", "Estimating Knowledge in Large Language Models Without Generating a Single Token", "When Context Leads but Parametric Memory Follows in Large Language Models", "I Could've Asked That: Reformulating Unanswerable Questions", "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs", "EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation", "Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "Teaching Small Language Models Reasoning through Counterfactual Distillation", "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation", "Investigating Mysteries of CoT-Augmented Distillation", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "A Survey of AMR Applications", "Does Large Language Model Contain Task-Specific Neurons?", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "\u201cImage, Tell me your story!\" Predicting the original meta-context of visual misinformation", "Language-to-Code Translation with a Single Labeled Example", "Attribute or Abstain: Large Language Models as Long Document Assistants", "Learning to Correct for QA Reasoning with Black-box LLMs", "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation", "MixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity", "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering", "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMS", "TroL: Traversal of Layers for Large Language and Vision Models", "Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in LLMs", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "Rethinking the Evaluation of In-Context Learning for LLMs", "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "RepMatch: Quantifying Cross-Instance Similarities in Representation Space", "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "LONGAGENT: Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration", "Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "Searching for Best Practices in Retrieval-Augmented Generation", "Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition", "Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks", "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering", "Generation with Dynamic Vocabulary", "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "Can LLMs Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "Measuring the Robustness of NLP Models to Domain Shifts", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment", "API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access", "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models", "Large Language Model-based Human-Agent Collaboration for Complex Task Solving", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models", "AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses", "Abstraction-of-Thought Makes Language Models Better Reasoners", "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Finance Communities in Long Form Question Answering", "Are LLMs Aware that Some Questions are not Open-ended?", "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature", "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations", "A Notion of Complexity for Theory of Mind via Discrete World Models", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "HoneyComb: A Flexible LLM-Based Agent System for Materials Science", "Characterizing LLM Abstention Behavior in Science QA with Context Perturbations", "Language Models Still Struggle to Zero-shot Reason about Time Series", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "Self-Contradictory Reasoning Evaluation and Detection", "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "RoQLlama: A Lightweight Romanian Adapted Language Model", "A Survey on Natural Language Counterfactual Generation", "Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "M2QA: Multi-domain Multilingual Question Answering", "LumberChunker: Long-Form Narrative Document Segmentation", "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs", "Learning Semantic Structure through First-Order-Logic Translation", "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "In-Context Learning with Iterative Demonstration Selection", "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation", "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "MedINST: Meta Dataset of Biomedical Instructions", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Datasets for Multilingual Answer Sentence Selection", "LoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "Detecting Temporal Ambiguity in Questions", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice", "Query Routing for Homogeneous Tools: An Instantiation in the RAG Scenario", "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation", "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation", "MDCR: A Dataset for Multi-Document Conditional Reasoning", "R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation", "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity", "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning", "Dual-Phase Accelerated Prompt Optimization", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "Can't Remember Details in Long Documents? You Need Some R&R", "More Bang for your Context: Virtual Documents for Question Answering over Long Documents", "Synthetic Multimodal Question Generation", "Calibrating Long-form Generations from Large Language Models", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "XC-CACHE: Cross-Attending to Cached Context for Efficient LLM Inference", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "M3SCIQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "Zero-Shot Fact Verification via Natural Logic and Large Language Models"], "Text Classification": ["On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification", "Incubating Text Classifiers Following User Instruction with Nothing but LLM", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse", "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates", "PERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts", "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction", "Towards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs", "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "A Bayesian Approach to Harnessing the Power of LLMs in Authorship Attribution", "Rethinking the Evaluation of In-Context Learning for LLMs", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Quantum Recurrent Architectures for Text Classification", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "AMPO: Automatic Multi-Branched Prompt Optimization", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "A Morphology-Based Investigation of Positional Encodings", "ADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers", "GottBERT: a pure German Language Model", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "On the Fragility of Active Learners for Text Classification", " 'Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated Peer Reviews ", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "Transfer Learning for Text Classification via Model Risk Analysis", "LoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "Rethinking Evaluation Methods for Machine Unlearning", "A Survey on Natural Language Counterfactual Generation", "Enhancing Byzantine-Resistant Aggregations with Client Embedding", "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models", "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference", "On the Generalization of Training-based ChatGPT Detection Methods", "MedINST: Meta Dataset of Biomedical Instructions", "Unlocking the Potential of Model Merging for Low-Resource Languages", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "CERT-ED: Certifiably Robust Text Classification for Edit Distance", "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation", "Logits Reranking via Semantic Labels for Hard Samples in Text Classification", "Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment", "Distance-aware Calibration for Pre-trained Language Models", "Textual Dataset Distillation via Language Model Embedding", "Robust Text Classification: Analyzing Prototype-Based Networks", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "Random Label Forests: An Ensemble Method with Label Subsampling For Extreme Multi-Label Problems", "Efficient Active Learning with Adapters", "CED: Comparing Embedding Differences for Detecting Out-of-Distribution and Hallucinated Text", "Characterizing Text Datasets with Psycholinguistic Features", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information"], "Language Modeling": ["Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS", "Rethinking Token Reduction for State Space Models", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "Leading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Probabilities", "When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages", "KidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions", "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models", "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling", "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing", "Can Large Language Models Learn Independent Causal Mechanisms?", "Extending Context Window of Large Language Models from a Distributional Perspective", "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models", "Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives", "Can Transformers Learn n-gram Language Models?", "PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "Demystifying Verbatim Memorization in Large Language Models", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Stable Language Model Pre-training by Reducing Embedding Variability", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "Target-Aware Language Modeling via Granular Data Sampling", "Chain and Causal Attention for Efficient Entity Tracking", "Turn Waste into Worth: Rectifying Top-k Router of MoE", "Evaluating n-Gram Novelty of Language Models Using RUSTY-DAWG", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients", "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training", "Scaling Laws for Linear Complexity Language Models", "Do LLMs learn a true syntactic universal?", "Calibrating Language Models with Adaptive Temperature Scaling", "How to Compute the Probability of a Word", "Language models and brains align due to more than next-word prediction and word-level information", "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models", "Semformer: Transformer Language Models with Semantic Planning", "Transformers are Multi-State RNNS", "Generation with Dynamic Vocabulary", "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging", "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings", "Is Child-Directed Speech Effective Training Data for Language Models?", "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "Tending Towards Stability: Convergence Challenges in Small Language Models", "Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens", "On the token distance modeling ability of higher RoPE attention dimension", "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models", "LONGHEADS: Multi-Head Attention is Secretly a Long Context Processor", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "Generalized Measures of Anticipation and Responsivity in Online Language Processing", "Securing Multi-turn Conversational Language Models From Distributed Backdoor Triggers", "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation", "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "Empirical Prior for Text Autoencoders", "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning", "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models", "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models"], "Sentiment Analysis": ["A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue", "CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading", "DA\u00b3: A Distribution-Aware Adversarial Attack against Language Models", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates", "Does Large Language Model Contain Task-Specific Neurons?", "Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners", "MOSEL: Inference Serving Using Dynamic Modality Selection", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Latent Concept-based Explanation of NLP Models", "Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research", "Rethinking the Evaluation of In-Context Learning for LLMs", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "RepMatch: Quantifying Cross-Instance Similarities in Representation Space", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Revisiting Supervised Contrastive Learning for Microblog Classification", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training", "Quantum Recurrent Architectures for Text Classification", "Semantics and Sentiment: Cross-lingual Variations in Emoji Use", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "ALVIN: Active Learning Via INterpolation", "Measuring the Robustness of NLP Models to Domain Shifts", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "Transfer Learning for Text Classification via Model Risk Analysis", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "A Survey on Natural Language Counterfactual Generation", "DADEE: Unsupervised Domain Adaptation in Early Exit PLMS", "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists", "Crisis counselor language and perceived genuine concern in crisis conversations", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "LoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis", "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models", "Exploring Design Choices for Building Language-Specific LLMs", "Functionality learning through specification instructions", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Regression-aware Inference with LLMs", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla", "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study", "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization", "The Overlooked Repetitive Lengthening Form in Sentiment Analysis", "Inference and Verbalization Functions During In-Context Learning", "CROWD: Certified Robustness via Weight Distribution for Smoothed Classifiers against Backdoor Attack"], "Visual Question Answering": ["Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering", "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "Tag-grounded Visual Instruction Tuning with Retrieval Augmentation", "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering", "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "Benchmarking Vision Language Models for Cultural Understanding", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model", "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "Attribute Diversity Determines the Systematicity Gap in VQA", "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "CommVQA: Situating Visual Question Answering in Communicative Contexts", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments", "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models", "In-Context Compositional Generalization for Large Vision-Language Models", "Game on Tree: Visual Hallucination Mitigation via Coarse-to-Fine View Tree and Game Theory", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models", "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "Nearest Neighbor Normalization Improves Multimodal Retrieval", "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "A Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect", "M5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "PropTest: Automatic Property Testing for Improved Visual Programming", "Learning to Ask Denotative and Connotative Questions for Knowledge-based VQA", "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models", "Zero-shot Commonsense Reasoning over Machine Imagination", "\u201cWhat is the value of {templates}?\u201d Rethinking Document Information Extraction Datasets for LLMs", "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "Why do LLaVA Vision-Language Models Reply to Images in English?", "Query-based Cross-Modal Projector Bolstering Mamba Multimodal LLM", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "Towards One-to-Many Visual Question Answering"], "Summarization": ["LONGEMBED: Extending Embedding Models for Long Context Retrieval", "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Optimized Speculative Sampling for GPU Hardware Accelerators", "A Survey of AMR Applications", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works", "Attribute or Abstain: Large Language Models as Long Document Assistants", "A Thorough Examination of Decoding Methods in the Era of LLMs", "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "Paraphrase Types Elicit Prompt Engineering Capabilities", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability", "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization", "Re-Evaluating Evaluation for Multilingual Summarization", "Towards Aligning Language Models with Textual Feedback", "Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion", "Reformatted Alignment", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning", "LoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation", "Inference-Time Language Model Alignment via Integrated Value Guidance", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Event-Keyed Summarization", "MedINST: Meta Dataset of Biomedical Instructions", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity", "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts", "Calibrating Long-form Generations from Large Language Models", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "A Critical Look at Meta-evaluating Summarisation Evaluation Metrics", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS"], "Natural Language Inference": ["Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search", "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving", "How Hard is this Test Set?\nNLI Characterization by Exploiting Training Dynamics", "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "How Do Humans Write Code? Large Models Do It the Same Way Too", "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars", "Revealing the Parallel Multilingual Learning within Large Language Models", "Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners", "Attribute or Abstain: Large Language Models as Long Document Assistants", "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic", "Atomic Inference for NLI with Generated Facts as Atoms", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "Latent Concept-based Explanation of NLP Models", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "VerifyMatch: A Semi-Supervised Learning Paradigm for Natural Language Inference with Confidence-Aware MixUp", "A Morphology-Based Investigation of Positional Encodings", "GottBERT: a pure German Language Model", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference", "Inference Helps PLMs' Conceptual Understanding: Improving the Abstract Inference Ability with Hierarchical Conceptual Entailment Graphs", "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets", "Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text", "ALVIN: Active Learning Via INterpolation", "Measuring the Robustness of NLP Models to Domain Shifts", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals", "Self-Contradictory Reasoning Evaluation and Detection", "A Survey on Natural Language Counterfactual Generation", "DADEE: Unsupervised Domain Adaptation in Early Exit PLMS", "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "How Entangled is Factuality and Deception in German?", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "\u201cSeeing the Big through the Small\u201d: Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?", "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study", "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization", "CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models", "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "Inference and Verbalization Functions During In-Context Learning"], "Commonsense Reasoning": ["Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "Teaching Small Language Models Reasoning through Counterfactual Distillation", "Investigating Mysteries of CoT-Augmented Distillation", "Focused Large Language Models are Stable Many-Shot Learners", "Mixture-of-Subspaces in Low-Rank Adaptation", "Retrieved In-Context Principles from Previous Mistakes", "A Thorough Examination of Decoding Methods in the Era of LLMs", "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Large Language Models Can Self-Correct with Key Condition Verification", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "Commonsense Knowledge Editing Based on Free-Text in LLMs", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Re-Reading Improves Reasoning in Large Language Models", "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training", "Scaling Laws for Linear Complexity Language Models", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "Learning to Paraphrase for Alignment with LLM Preference", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information", "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning", "MUSCLE: A Model Update Strategy for Compatible LLM Evolution", "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization", "The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning", "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "Exploring Design Choices for Building Language-Specific LLMs", "Zero-shot Commonsense Reasoning over Machine Imagination", "PizzaCommonSense: Learning to Model Commonsense Reasoning about Intermediate Steps in Cooking Recipes", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "Large Language Models are In-context Teachers for Knowledge Reasoning", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning"], "Machine Translation": ["Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?", "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models", "Word Alignment as Preference for Machine Translation", "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects", "Aligning Translation-Specific Understanding to General Understanding in Large Language Models", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "An Audit on the Perspectives and Challenges of Hallucinations in NLP", "A Survey of AMR Applications", "Revealing the Parallel Multilingual Learning within Large Language Models", "Lexically Grounded Subword Segmentation", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation", "Distributional Properties of Subword Regularization", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation", "Building Resources for Emakhuwa: Machine Translation and News Classification Benchmarks", "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation", "Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level", "Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training", "What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study", "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "Back to School: Translation Using Grammar Books", "Improving Minimum Bayes Risk Decoding with Multi-Prompt", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "Evaluating Automatic Metrics with Incremental Machine Translation Systems", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation", "Leveraging Grammar Induction for Language Understanding and Generation", "Dual-teacher Knowledge Distillation for Low-frequency Word Translation", "Visual Pivoting Unsupervised Multimodal Machine Translation in Low-Resource Distant Language Pairs", "On Creating an English-Thai Code-switched Machine Translation in Medical Domain", "Cross-lingual Contextualized Phrase Retrieval", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization", "Exploring Design Choices for Building Language-Specific LLMs", "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "Benchmarking Machine Translation with Cultural Awareness", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Representational Isomorphism and Alignment of Multilingual Large Language Models", "LexMatcher: Dictionary-centric Data Curation for LLM-based Machine Translation", "Analyzing Context Contributions in LLM-based Machine Translation", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS"], "Text Generation": ["Prompts have evil twins", "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval", "KidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions", "PERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "Precise Model Benchmarking with Only a Few Observations", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "Analysis of Plan-based Retrieval for Grounded Text Generation", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "Evaluating n-Gram Novelty of Language Models Using RUSTY-DAWG", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "Scaling Laws for Linear Complexity Language Models", "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions", "Transformers are Multi-State RNNS", "An Empirical Analysis of the Writing Styles of Persona-Assigned LLMs", "Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?", "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules", "Adversarial Text Generation using Large Language Models for Dementia Detection", "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models", "Filtered Direct Preference Optimization", "Are LLMs Aware that Some Questions are not Open-ended?", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "LongForm: Effective Instruction Tuning with Reverse Instructions", "Achieving Stronger Generation via Simple Contrastive Tuning", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models", "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation", "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "Empirical Prior for Text Autoencoders", "Local and Global Decoding in Text Generation", "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains", "Extrinsic Evaluation of Cultural Competence in Large Language Models"], "Instruction Following": ["Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "Direct Multi-Turn Preference Optimization for Language Agents", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "Automatic Instruction Evolving for Large Language Models", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs", "WPO: Enhancing RLHF with Weighted Preference Optimization", "A Thorough Examination of Decoding Methods in the Era of LLMs", "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "Dual-Space Knowledge Distillation for Large Language Models", "BLSP-Emo: Towards Empathetic Large Speech-Language Models", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks", "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?", "Preference-Guided Reflective Sampling for Aligning Language Models", "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "Inference-Time Language Model Alignment via Integrated Value Guidance", "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation\\", "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models", "LongForm: Effective Instruction Tuning with Reverse Instructions", "LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "Achieving Stronger Generation via Simple Contrastive Tuning", "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "Resilience of Large Language Models for Noisy Instructions", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression"], "Mathematical Reasoning": ["Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "How Do Humans Write Code? Large Models Do It the Same Way Too", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "Revealing the Parallel Multilingual Learning within Large Language Models", "Automatic Instruction Evolving for Large Language Models", "FROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "Retrieved In-Context Principles from Previous Mistakes", "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "LM\u00b2: A Simple Society of Language Models Solves Complex Reasoning", "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "Mixed Distillation Helps Smaller Language Models Reason Better", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts", "Self-Consistency Boosts Calibration for Math Reasoning", "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information", "Step-level Value Preference Optimization for Mathematical Reasoning", "Weak-to-Strong Reasoning", "Learning to Plan by Updating Natural Language", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation"], "In-Context Learning": ["Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning", "Interpretability-based Tailored Knowledge Editing in Transformers", "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems", "Focused Large Language Models are Stable Many-Shot Learners", "Learning to Retrieve Iteratively for In-Context Learning", "Position Engineering: Boosting Large Language Models through Positional Information Manipulation", "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation", "On the In-context Generation of Language Models", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Self-AMPLIFY : Improving Small Language Models with Self Post Hoc Explanations", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach", "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences", "Generating Demonstrations for In-Context Compositional Generalization in Grounded Language Learning", "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!", "Semformer: Transformer Language Models with Semantic Planning", "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL", "On the Empirical Complexity of Reasoning and Planning in LLMs", "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "Private prediction for large-scale synthetic text generation", "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "Large Language Models are In-context Teachers for Knowledge Reasoning"], "Code Generation": ["AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation", "Automatic Instruction Evolving for Large Language Models", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with Really Good Data", "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "Free your mouse! Command Large Language Models to Generate Code to Format Word Documents", "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing", "RAR: Retrieval-augmented retrieval for code generation in low-resource languages", "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "Improving Minimum Bayes Risk Decoding with Multi-Prompt", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "Large Language Model-based Human-Agent Collaboration for Complex Task Solving", "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation", "EvOR: Evolving Retrieval for Code Generation", "CodeFort: Robust Training for Code Generation Models", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision", "CODEIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code", "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft", "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "On Leakage of Code Generation Evaluation Datasets", "SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement", "One-to-many testing for code generation from (just) natural language", "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation", "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs"], "Named Entity Recognition": ["MTLS: Making Texts into Linguistic Symbols", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "ADELIE: Aligning Large Language Models on Information Extraction", "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints", "Paraphrase Types Elicit Prompt Engineering Capabilities", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents", "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Embedded Named Entity Recognition using Probing Classifiers", "NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?", "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries", "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR", "A Morphology-Based Investigation of Positional Encodings", "Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4", "GottBERT: a pure German Language Model", "General Collaborative Framework between Large Language Model and Experts for Universal Information Extraction", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting", "MedINST: Meta Dataset of Biomedical Instructions", "C-ICL: Contrastive In-context Learning for Information Extraction", "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases", "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Efficient Active Learning with Adapters", "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models"], "Reasoning": ["PhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study", "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models", "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs", "Learning to Correct for QA Reasoning with Black-box LLMs", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "Self-AMPLIFY : Improving Small Language Models with Self Post Hoc Explanations", "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently", "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "One Thousand and One Pairs: A \u201cnovel\u201d challenge for long-context language models", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "Head-wise Shareable Attention for Large Language Models", "On the Empirical Complexity of Reasoning and Planning in LLMs", "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models", "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate", "Improving Multi-Agent Debate with Sparse Communication Topology", "In-Context Learning with Iterative Demonstration Selection", "Weak-to-Strong Reasoning", "Navigating Hallucinations for Reasoning of Unintentional Activities", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models", "Will LLMs Sink or Swim? Exploring Decision-Making Under Pressure", "ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering", "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework", "Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning"], "Rag": ["Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "Knowledge Verification to Nip Hallucination in the Bud", "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering", "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs", "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation", "Position Engineering: Boosting Large Language Models through Positional Information Manipulation", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "AGRAME: Any-Granularity Ranking with Multi-Vector Embeddings", "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR", "Improve Dense Passage Retrieval with Entailment Tuning", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "Analysis of Plan-based Retrieval for Grounded Text Generation", "Searching for Best Practices in Retrieval-Augmented Generation", "Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "RAR: Retrieval-augmented retrieval for code generation in low-resource languages", "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation", "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts", "EvOR: Evolving Retrieval for Code Generation", "LumberChunker: Long-Form Narrative Document Segmentation", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation", "R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation", "Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA", "Unified Active Retrieval for Retrieval Augmented Generation"], "Information Retrieval": ["Do Large Language Models Know How Much They Know?", "GENRA: Enhancing Zero-shot Retrieval with Rank Aggregation", "FIRST: Faster Improved Listwise Reranking with Single Token Decoding", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions", "Exploring the Practicality of Generative Retrieval on Dynamic Corpora", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Bridging Local Details and Global Context in Text-Attributed Graphs", "LitSearch: A Retrieval Benchmark for Scientific Literature Search", "Scaling Laws for Linear Complexity Language Models", "One Thousand and One Pairs: A \u201cnovel\u201d challenge for long-context language models", "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement", "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "MedAdapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "Document Hashing with Multi-Grained Prototype-Induced Hierarchical Generative Model", "Exploring the Best Practices of Query Expansion with Large Language Models", "Revisiting Query Variation Robustness of Transformer Models", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations", "HyQE: Ranking Contexts with Hypothetical Query Embeddings"], "Image Captioning": ["ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation", "HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding", "Tag-grounded Visual Instruction Tuning with Retrieval Augmentation", "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "A Survey of AMR Applications", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes", "Precise Model Benchmarking with Only a Few Observations", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Altogether: Image Captioning via Re-aligning Alt-text", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "Nearest Neighbor Normalization Improves Multimodal Retrieval", "Mitigating Open-Vocabulary Caption Hallucinations", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "M5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks", "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "Improving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design"], "Natural Language Processing": ["A Survey on In-context Learning", "Model Balancing Helps Low-data Training and Fine-tuning", "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "A Generic Method for Fine-grained Category Discovery in Natural Language Texts", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "Knowledge-Centric Hallucination Detection", "Toward Compositional Behavior in Neural Models: A Survey of Current Views", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "Pragmatic Norms Are All You Need \u2013 Why The Symbol Grounding Problem Does Not Apply to LLMs", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models", "Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models", "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Calibrating Language Models with Adaptive Temperature Scaling", "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference", "InsertGNN: A Hierarchical Graph Neural Network for the TOEFL Sentence Insertion Problem", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Enhancing Byzantine-Resistant Aggregations with Client Embedding", "Crisis counselor language and perceived genuine concern in crisis conversations", "Large Language Models are Students at Various Levels: Zero-shot Question Difficulty Estimation", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Resilience of Large Language Models for Noisy Instructions", "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch"], "Llm Evaluation": ["Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models", "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations", "Towards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey", "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs", "Assessing and Verifying Task Utility in LLM-Powered Applications", "An Open-Source Data Contamination Report for Large Language Models", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Adversarial Math Word Problem Generation", "Self-Evaluation of Large Language Model based on Glass-box Features", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation", "Knowledge-based Consistency Testing of Large Language Models", "SODA-EVAL: Open-Domain Dialogue Evaluation in the age of LLMs", "Cognitive Bias in Decision-Making with LLMs", "Compare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY", "TOWER: Tree Organized Weighting for Evaluating Complex Instructions", "Are Large Language Models Consistent over Value-laden Questions?", "SedarEval: Automated Evaluation using Self-Adaptive Rubrics"], "Bias Detection": ["A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights", "Voices in a Crowd: Searching for Clusters of Unique Perspectives", "Towards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey", "Moral Foundations of Large Language Models", "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation", "The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models", "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models", "Evaluating Biases in Context-Dependent Sexual and Reproductive Health Questions", "PclGPT: A Large Language Model for Patronizing and Condescending Language Detection", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models", "Gender Identity in Pretrained Language Models: An Inclusive Approach to Data Creation and Probing", "Cognitive Bias in Decision-Making with LLMs", "Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models"], "Reading Comprehension": ["Mitigating the Alignment Tax of RLHF", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "Revealing the Parallel Multilingual Learning within Large Language Models", "LawBench: Benchmarking Legal Knowledge of Large Language Models", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks", "AMPO: Automatic Multi-Branched Prompt Optimization", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals", "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models", "Functionality learning through specification instructions", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States"], "Classification": ["Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "Attribute or Abstain: Large Language Models as Long Document Assistants", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Computational Meme Understanding: A Survey", "Adversarial Text Generation using Large Language Models for Dementia Detection", "Reformatted Alignment", "Categorial Grammar Supertagging via Large Language Models", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "Rethinking Code Refinement: Learning to Judge Code Efficiency", "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability", "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models", "Downstream Trade-offs of a Family of Text Watermarks", "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains", "Unified Active Retrieval for Retrieval Augmented Generation"], "Data Augmentation": ["ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "An Audit on the Perspectives and Challenges of Hallucinations in NLP", "CareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing", "Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4", "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "A Study of Implicit Ranking Unfairness in Large Language Models", "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification", "Controlled Transformation of Text-Attributed Graphs", "Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information"], "Text Summarization": ["Aligning Large Language Models with Diverse Political Viewpoints", "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?", "FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping", "Semformer: Transformer Language Models with Semantic Planning", "Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Divide and Conquer: Legal Concept-guided Criminal Court View Generation", "RoQLlama: A Lightweight Romanian Adapted Language Model", "ALIGNSUM: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "HealthAlignSumm: Utilizing Alignment for Multimodal Summarization of Code-Mixed Healthcare Dialogues", "A Decoding Algorithm for Length-Control Summarization Based on Directed Acyclic Transformers", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions", "Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles"], "Instruction Tuning": ["Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Fast Forwarding Low-Rank Training", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions", "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback", "Curriculum Consistency Learning for Conditional Sentence Generation", "How Susceptible are Large Language Models to Ideological Manipulation?", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization", "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search", "Data Diversity Matters for Robust Instruction Tuning", "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach", "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "Better Alignment with Instruction Back-and-Forth Translation", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning"], "Arithmetic Reasoning": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models", "Large Language Models Can Self-Correct with Key Condition Verification", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning", "Re-Reading Improves Reasoning in Large Language Models", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "Self-training Language Models for Arithmetic Reasoning"], "Natural Language Understanding": ["RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "On Training Data Influence of GPT Models", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "FROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models", "Perceptions of Linguistic Uncertainty by Language Models and Humans", "MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations", "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "Head-wise Shareable Attention for Large Language Models", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "Leveraging Grammar Induction for Language Understanding and Generation", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "Zero-shot Commonsense Reasoning over Machine Imagination", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation"], "Hallucination Detection": ["Embedding and Gradient Say Wrong: A White-Box Method for Hallucination Detection", "When Context Leads but Parametric Memory Follows in Large Language Models", "Knowledge-Centric Hallucination Detection", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "On the Universal Truthfulness Hyperplane Inside LLMS", "Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation", "Factuality of Large Language Models: A Survey", "Reference-free Hallucination Detection for Large Vision-Language Models", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "CED: Comparing Embedding Differences for Detecting Out-of-Distribution and Hallucinated Text", "Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning", "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation"], "Zero-Shot Learning": ["Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "Retrieval-enriched zero-shot image classification in low-resource domains", "Diversity, Rationalize, and Combine: Ensembling Multiple QA Strategies for Zero-shot Knowledge-based VQA", "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "Instance-Level Dynamic LoRAs Composition for Cross-Task Generalization", "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs", "Navigating Hallucinations for Reasoning of Unintentional Activities", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "Zero-shot Commonsense Reasoning over Machine Imagination", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting"], "Relation Extraction": ["Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "ADELIE: Aligning Large Language Models on Information Extraction", "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints", "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents", "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "Topic-Oriented Open Relation Extraction with A Priori Seed Generation", "SRF: Enhancing Document-Level Relation Extraction with a Novel Secondary Reasoning Framework", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "A Survey on Natural Language Counterfactual Generation", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "Entity or Relation Embeddings? An Analysis of Encoding Strategies for Relation Extraction", "MedINST: Meta Dataset of Biomedical Instructions", "C-ICL: Contrastive In-context Learning for Information Extraction", "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "AliGATr: Graph-based layout generation for form understanding", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models"], "Few-Shot Learning": ["A Survey on In-context Learning", "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "INDUCT-LEARN: Short Phrase Prompting with Instruction Induction", "Learning to Retrieve Iteratively for In-Context Learning", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "OpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs", "Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting", "Scalable and Domain-General Abstractive Proposition Segmentation", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression"], "Hate Speech Detection": ["Hateful Word in Context Classification", "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze", "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization", "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Hate Personified: Investigating the role of LLMs in content moderation", "Annotator-Centric Active Learning for Subjective NLP Tasks", "Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection", "PREDICT: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection", "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation", "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models", "Functionality learning through specification instructions", "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation"], "Gsm8K": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "MODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "Task Oriented In-Domain Data Augmentation", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning", "Skills-in-Context: Unlocking Compositionality in Large Language Models", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning"], "Bias Mitigation": ["\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework", "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation", "Local Contrastive Editing of Gender Stereotypes", "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "Cognitive Bias in Decision-Making with LLMs", "Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models"], "Natural Language Generation": ["Uncertainty in Language Models: Assessment through Rank-Calibration", "On Training Data Influence of GPT Models", "Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering", "StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Generative Models for Automatic Medical Decision Rule Extraction from Text", "Ontologically Faithful Generation of Non-Player Character Dialogues", "Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation", "MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations", "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "Evaluating Diversity in Automatic Poetry Generation", "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "SODA-EVAL: Open-Domain Dialogue Evaluation in the age of LLMs", "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts"], "Logical Reasoning": ["Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars", "Retrieved In-Context Principles from Previous Mistakes", "METAREFLECTION: Learning Instructions for Language Agents using Past Reflections", "Puzzle Solving using Reasoning of Large Language Models: A Survey", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "Symbolic Working Memory Enhances Language Models for Complex Rule Application", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "Abstraction-of-Thought Makes Language Models Better Reasoners", "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning", "Learning to Plan by Updating Natural Language", "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains"], "Reinforcement Learning From Human Feedback": ["Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence", "WPO: Enhancing RLHF with Weighted Preference Optimization", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents", "Rethinking the Role of Proxy Rewards in Language Model Alignment", "Filtered Direct Preference Optimization", "Reward Modeling Requires Automatic Adjustment Based on Data Quality", "Semi-Supervised Reward Modeling via Iterative Self-Training", "Step-level Value Preference Optimization for Mathematical Reasoning", "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models without Preference Data", "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback", "DEFT: Distribution-guided Efficient Fine-Tuning for Human Alignment", "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models"], "Llms": ["Evaluating Readability and Faithfulness of Concept-based Explanations", "QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models", "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL", "COEVOL: Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation", "UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models", "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions", "OpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation", "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models", "In-Context Former: Lightning-fast Compressing Context for Large Language Model", "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "LaCo: Large Language Model Pruning via Layer Collapse", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Better Alignment with Instruction Back-and-Forth Translation", "ATQ: Activation Transformation for Weight-Activation Quantization of Large Language Models"], "Knowledge Editing": ["RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "Interpretability-based Tailored Knowledge Editing in Transformers", "EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation", ">Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "Commonsense Knowledge Editing Based on Free-Text in LLMs", "AKEW: Assessing Knowledge Editing in the Wild", "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing", "Editing Conceptual Knowledge for Large Language Models", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities", "Cross-Lingual Multi-Hop Knowledge Editing", "Updating Large Language Models' Memories with Time Constraints", "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments"], "Dialogue Generation": ["Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems", "Ontologically Faithful Generation of Non-Player Character Dialogues", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Mixed-Session Conversation with Egocentric Memory", "Securing Multi-turn Conversational Language Models From Distributed Backdoor Triggers", "CACTUS: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory", "TRIP NEGOTIATOR: A Travel Persona-aware Reinforced Dialogue Generation Model for Personalized Integrative Negotiation in Tourism"], "Language Understanding": ["NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning", "Autoregressive Pre-Training on Pixels and Texts", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "LLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks", "Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game", "MalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language", "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance", "Enhancing Agent Learning through World Dynamics Modeling", "MUSCLE: A Model Update Strategy for Compatible LLM Evolution", "Large Language Models are Students at Various Levels: Zero-shot Question Difficulty Estimation", "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models"], "Open-Domain Question Answering": ["BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering", "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering", "Large Language Models Can Self-Correct with Key Condition Verification", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Improving Zero-shot LLM Re-Ranker with Risk Minimization", "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation", "RaFe: Ranking Feedback Improves Query Rewriting for RAG", "Chain-of-Rewrite: Aligning Question and Documents for Open-Domain Question Answering", "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts", "Evidence Retrieval for Fact Verification using Multi-stage Reranking", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs", "Exploring Hint Generation Approaches in Open-Domain Question Answering"], "Knowledge Distillation": ["AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "MTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval", "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "Optimizing Chinese Lexical Simplification Across Word Types: A Hybrid Approach", "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation", "Dual-Space Knowledge Distillation for Large Language Models", "PAIRDISTILL: Pairwise Relevance Distillation for Dense Retrieval", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation"], "Llm Alignment": ["Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion", "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization", "Reward Difference Optimization For Sample Reweighting In Offline RLHF", "Self-Evolution Fine-Tuning for Policy Optimization", "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "On Diversified Preferences of Large Language Model Alignment", "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch", "Aligners: Decoupling LLMs and Alignment", "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks", "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization"], "Automatic Speech Recognition": ["Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "On Mitigating Performance Disparities in Multilingual Speech Recognition", "Optimized Speculative Sampling for GPU Hardware Accelerators", "Advancing Test-Time Adaptation in Wild Acoustic Test Settings", "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "Unveiling the Role of Pretraining in Direct Speech Translation", "Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding", "Are Modern Neural ASR Architectures Robust for Polysynthetic Languages?", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models", "Modeling Gender and Dialect Bias in Automatic Speech Recognition", "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper"], "Topic Classification": ["SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "Revisiting Supervised Contrastive Learning for Microblog Classification", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "Multilingual Topic Classification in X: Dataset and Analysis", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "Transfer Learning for Text Classification via Model Risk Analysis", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "In-Context Learning with Iterative Demonstration Selection", "Dual-Phase Accelerated Prompt Optimization", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "Inference and Verbalization Functions During In-Context Learning"], "Language Models": ["Prompts have evil twins", "Tracking the perspectives of interacting language models", "Personas as a Way to Model Truthfulness in Language Models", "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs", "Breaking ReLU Barrier: Generalized MoEfication for Dense Pretrained Models", "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models", "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "Improving LLM Attributions with Randomized Path-Integration", "MobileQuant: Mobile-friendly Quantization for On-device Language Models", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data"], "Translation": ["Mitigating the Alignment Tax of RLHF", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "A Thorough Examination of Decoding Methods in the Era of LLMs", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "An Analysis and Mitigation of the Reversal Curse", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "MedINST: Meta Dataset of Biomedical Instructions", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models"], "Fine-Tuning": ["Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "Fast Forwarding Low-Rank Training", "Breaking ReLU Barrier: Generalized MoEfication for Dense Pretrained Models", "Large Language Models Can Be Contextual Privacy Protection Learners", "MIXTURE-OF-SKILLS: Learning to Optimize Data Usage for Fine-Tuning Large Language Models", "GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients", "On the Empirical Complexity of Reasoning and Planning in LLMs", "Private prediction for large-scale synthetic text generation", "Step-level Value Preference Optimization for Mathematical Reasoning", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "QEFT: Quantization for Efficient Fine-Tuning of LLMs"], "Generalization": ["Instruction Pre-Training: Language Models are Supervised Multitask Learners", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Data Contamination Can Cross Language Barriers", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks", "PREDICT: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection", "Knowledge Graph Enhanced Large Language Model Editing", "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation"], "Visual Reasoning": ["Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model", "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "Large Language Models Are Challenged by Habitat-Centered Reasoning", "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables", "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks"], "Medical Question Answering": ["Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "LM\u00b2: A Simple Society of Language Models Solves Complex Reasoning", "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures", "AMPO: Automatic Multi-Branched Prompt Optimization", "Few shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering", "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering", "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration", "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate", "MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks", "Large Language Models are In-context Teachers for Knowledge Reasoning", "MedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures"], "Text-To-Image Generation": ["ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation", "Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation", "Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training", "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "Altogether: Image Captioning via Re-aligning Alt-text", "Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training", "Precision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model", "Repairing Catastrophic-Neglect in Text-to-Image Diffusion Models via Attention-Guided Feature Enhancement", "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation"], "Speech Recognition": ["Scaling Properties of Speech Language Models", "EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning", "Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges", "MOSEL: Inference Serving Using Dynamic Modality Selection", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages", "VHASR: A Multimodal Speech Recognition System With Vision Hotwords", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition"], "Evaluation": ["LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "AKEW: Assessing Knowledge Editing in the Wild", "Towards Benchmarking Situational Awareness of Large Language Models: Comprehensive Benchmark, Evaluation and Analysis", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation", "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "BLADE: Benchmarking Language Model Agents for Data-Driven Science", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "Knowledge-Centric Templatic Views of Documents"], "Event Extraction": ["Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "ADELIE: Aligning Large Language Models on Information Extraction", "Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments", "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness", "General Collaborative Framework between Large Language Model and Experts for Universal Information Extraction", "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction", "Employing Glyphic Information for Chinese Event Extraction with Vision-Language Model", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "Event-Keyed Summarization", "MedINST: Meta Dataset of Biomedical Instructions", "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction"], "Generation": ["FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "COGEN: Learning from Feedback with Coupled Comprehension and Generation", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "Reformatted Alignment", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues", "Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas"], "Information Extraction": ["A Survey of AMR Applications", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "Paraphrase Types Elicit Prompt Engineering Capabilities", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "TKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs", "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP", "When and Where Did It Happen? An Encoder-Decoder Model to Identify Scenario Context", "Schema-Driven Information Extraction from Heterogeneous Tables", "SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement"], "Math Problem Solving": ["A Thorough Examination of Decoding Methods in the Era of LLMs", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "An Analysis and Mitigation of the Reversal Curse", "Tools Fail: Detecting Silent Errors in Faulty Tools", "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector", "LLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks", "Assessing and Verifying Task Utility in LLM-Powered Applications", "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision", "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline", "Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?"], "Language Model Alignment": ["LIONS: An Empirically Optimized Approach to Align Language Models", "Reverse-Engineering the Reader", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities", "Rethinking the Role of Proxy Rewards in Language Model Alignment", "Enhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing", "Filtered Direct Preference Optimization", "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "Evolutionary Contrastive Distillation for Language Model Alignment", "FACTALIGN: Long-form Factuality Alignment of Large Language Models"], "Math": ["Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts", "Nash CoT: Multi-Path Inference with Preference Equilibrium", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "Skills-in-Context: Unlocking Compositionality in Large Language Models"], "Rte": ["AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning", "On Training Data Influence of GPT Models", "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement", "When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models", "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models", "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models", "Skills-in-Context: Unlocking Compositionality in Large Language Models", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune"], "Glue": ["Model Balancing Helps Low-data Training and Fine-tuning", "Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models", "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement", "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers", "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules", "Leveraging Grammar Induction for Language Understanding and Generation", "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models", "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models"], "Hallucination Mitigation": ["HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding", "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models", "Word Alignment as Preference for Machine Translation", "Analysis of Plan-based Retrieval for Grounded Text Generation", "Mitigating Open-Vocabulary Caption Hallucinations", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations", "Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning"], "Math Reasoning": ["Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "SCIAGENT: Tool-augmented Language Models for Scientific Reasoning", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "MUSCLE: A Model Update Strategy for Compatible LLM Evolution", "Unlocking the Potential of Model Merging for Low-Resource Languages", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning"], "Multi-Hop Reasoning": ["Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "Cross-Lingual Multi-Hop Knowledge Editing", "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective"], "Semantic Textual Similarity": ["Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese", "Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Scaling Sentence Embeddings with Large Language Models", "Variational Language Concepts for Interpreting Foundation Language Models", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "Regression-aware Inference with LLMs", "Representational Isomorphism and Alignment of Multilingual Large Language Models"], "Topic Modeling": ["PhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study", "Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature", "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments", "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement", "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement", "Unsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance", "NeuroMax: Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "Topic Modeling: Contextual Token Embeddings Are All You Need", "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs"], "Quantization": ["QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models", "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "MobileQuant: Mobile-friendly Quantization for On-device Language Models", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "QEFT: Quantization for Efficient Fine-Tuning of LLMs", "How Does Quantization Affect Multilingual LLMs?", "ATQ: Activation Transformation for Weight-Activation Quantization of Large Language Models"], "Video Question Answering": ["Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting", "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection", "Encoding and Controlling Global Semantics for Long-form Video Question Answering", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge", "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "Enhancing Temporal Modeling of Video LLMs via Time Gating", "Exploring Question Guidance and Answer Calibration for Visually Grounded Video Question Answering", "ALANAVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding"], "Link Prediction": ["MQuinE: a Cure for \"Z-paradox\" in Knowledge Graph Embedding", "MoCoKGC: Momentum Contrast Entity Encoding for Knowledge Graph Completion", "ATAP: Automatic Template-Augmented Commonsense Knowledge Graph Completion via Pre-Trained Language Models", "Can Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction", "Predictive Multiplicity of Knowledge Graph Embeddings in Link Prediction", "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs", "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning", "OpenGraph: Towards Open Graph Foundation Models", "Llamipa: An Incremental Discourse Parser", "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding"], "Calibration": ["Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models", "Calibrating Language Models with Adaptive Temperature Scaling", "Reconfidencing LLMs from the Grouping Loss Perspective", "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration", "Self-Consistency Boosts Calibration for Math Reasoning", "Distance-aware Calibration for Pre-trained Language Models", "Calibrating Long-form Generations from Large Language Models", "Uncertainty Calibration for Tool-Using Language Agents"], "Emotion Recognition": ["A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "Visual Prompting in LLMs for Enhancing Emotion Recognition", "Towards Robust Speech Representation Learning for Thousands of Languages", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "Revisiting Supervised Contrastive Learning for Microblog Classification", "PALM: Few-Shot Prompt Learning for Audio Language Models", "Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health", "Emosical: An Emotion-Annotated Musical Theatre Dataset", "WavLLM: Towards Robust and Adaptive Speech Large Language Model"], "Svamp": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "Task Oriented In-Domain Data Augmentation", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting"], "Question Generation": ["QUDSELECT: Selective Decoding for Questions Under Discussion Parsing", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Measuring the Robustness of NLP Models to Domain Shifts", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "Learning to Ask Denotative and Connotative Questions for Knowledge-based VQA", "Enable Fast Sampling for Seq2Seq Text Diffusion", "Reference-based Metrics Disprove Themselves in Question Generation"], "Textual Entailment": ["DA\u00b3: A Distribution-Aware Adversarial Attack against Language Models", "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic", "Paraphrase Types Elicit Prompt Engineering Capabilities", "RepMatch: Quantifying Cross-Instance Similarities in Representation Space", "Open-world Multi-label Text Classification with Extremely Weak Supervision", "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging", "MedINST: Meta Dataset of Biomedical Instructions", "Variational Language Concepts for Interpreting Foundation Language Models", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity"], "Toxicity Detection": ["Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric", "Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas", "BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "CROWD: Certified Robustness via Weight Distribution for Smoothed Classifiers against Backdoor Attack"], "Fact Verification": ["\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization", "Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context", "Evidence Retrieval for Fact Verification using Multi-stage Reranking", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "How Entangled is Factuality and Deception in German?", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "Zero-Shot Fact Verification via Natural Logic and Large Language Models"], "Sentiment Classification": ["UNIGEN: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation", "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History", "AMPO: Automatic Multi-Branched Prompt Optimization", "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference", "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models", "BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers", "Dual-Phase Accelerated Prompt Optimization"], "Dataset Creation": ["COCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds", "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "Emosical: An Emotion-Annotated Musical Theatre Dataset", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues", "CACTUS: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory", "ARTS: Assessing Readability & Text Simplicity", "CoCoHD: Congress Committee Hearing Dataset"], "Multi-Task Learning": ["Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment", "MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing", "Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification"], "Coreference Resolution": ["ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Major Entity Identification: A Generalizable Alternative to Coreference Resolution", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?", "MedINST: Meta Dataset of Biomedical Instructions", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains"], "Continual Learning": ["SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models", "Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting", "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "Unlocking Continual Learning Abilities in Language Models", "ICL: Iterative Continual Learning for Multi-domain Neural Machine Translation", "Gradient Localization Improves Lifelong Pretraining of Language Models", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging"], "Text-To-Sql": ["PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL", "Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL", "SYNTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA", "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL", "DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models", "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning"], "Story Generation": ["Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "Measuring Psychological Depth in Language Models", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models", "SWAG: Storytelling With Action Guidance", "Extrinsic Evaluation of Cultural Competence in Large Language Models"], "Domain Adaptation": ["Advancing Test-Time Adaptation in Wild Acoustic Test Settings", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "Domain adapted machine translation: What does catastrophic forgetting forget and why?", "Generation with Dynamic Vocabulary", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts", "A Survey on Natural Language Counterfactual Generation", "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages"], "Sentiment Analysis Natural Language Inference": ["Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "ALVIN: Active Learning Via INterpolation", "Measuring the Robustness of NLP Models to Domain Shifts", "A Survey on Natural Language Counterfactual Generation", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study"], "Model Editing": ["Consecutive Batch Model Editing with HooK Layers", "On the Robustness of Editing Large Language Models", "Local Contrastive Editing of Gender Stereotypes", "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing", "Knowledge Graph Enhanced Large Language Model Editing", "The Fall of ROME: Understanding the Collapse of LLMs in Model Editing", "Better Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization", "A Unified Framework for Model Editing"], "Binary Classification": ["An Experimental Analysis on Evaluating Patent Citations", "TROTR: A Framework for Evaluating the Recontextualization of Text", "Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification", "ESG-Kor: A Korean Dataset for ESG-related Information Extraction and Practical Use Cases", "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI"], "Strategyqa": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning"], "Paraphrase Detection": ["What's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference", "ALVIN: Active Learning Via INterpolation", "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "LoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters"], "Qnli": ["AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning", "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement", "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models", "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models", "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune"], "Mnli": ["AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning", "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement", "When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models", "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models", "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune"], "Boolq": ["AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning", "On Training Data Influence of GPT Models", "When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models", "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models", "CHESS : Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification", "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model"], "Prompt Engineering": ["A Survey on In-context Learning", "Moral Foundations of Large Language Models", "AMPO: Automatic Multi-Branched Prompt Optimization", "The Death and Life of Great Prompts: Analyzing the Evolution of LLM Prompts from the Structural Perspective", "Few shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "Can't Remember Details in Long Documents? You Need Some R&R"], "Bias Analysis": ["AGENTREVIEW: Exploring Peer Review Dynamics with LLM Agents", "Humans or LLMs as the Judge? A Study on Judgement Bias", "Statistical Uncertainty in Word Embeddings: GloVe-V", "Hate Personified: Investigating the role of LLMs in content moderation", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models", "Modeling Gender and Dialect Bias in Automatic Speech Recognition"], "Superglue": ["Model Balancing Helps Low-data Training and Fine-tuning", "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model", "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models", "Turn Waste into Worth: Rectifying Top-k Router of MoE", "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning"], "Language Generation": ["CMD: a framework for Context-aware Model self-Detoxification", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis", "Make Large Language Model a Better Ranker", "Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"], "Arithmetic Reasoning Commonsense Reasoning": ["GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Large Language Models Can Self-Correct with Key Condition Verification", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Re-Reading Improves Reasoning in Large Language Models", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting"], "Image Classification": ["Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "Retrieval-enriched zero-shot image classification in low-resource domains", "Unveiling the mystery of visual attributes of concrete and abstract concepts: Variability, nearest neighbors, and challenging categories", "Text2Model: Text-based Model Induction for Zero-shot Image Classification", "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models"], "Preference Learning": ["Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "What are the Generator Preferences for End-to-end Task-Oriented Dialog System?", "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation", "Step-level Value Preference Optimization for Mathematical Reasoning", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"], "Semantic Parsing": ["QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "Learning to Retrieve Iteratively for In-Context Learning", "Language-to-Code Translation with a Single Labeled Example", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing", "Scope-enhanced Compositional Semantic Parsing for DRT", "Predicting generalization performance with correctness discriminators"], "Mmlu": ["MODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning", "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement", "Turn Waste into Worth: Rectifying Top-k Router of MoE", "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance", "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning"], "Error Correction": ["Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check", "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models", "E2CL: Exploration-based Error Correction Learning for Embodied Agents", "Resilience of Large Language Models for Noisy Instructions"], "Knowledge Graph Question Answering": ["CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering", "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering", "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering", "A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval", "Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA"], "Part-Of-Speech Tagging": ["MTLS: Making Texts into Linguistic Symbols", "Lexically Grounded Subword Segmentation", "A Morphology-Based Investigation of Positional Encodings", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Predicting generalization performance with correctness discriminators", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Targeted Multilingual Adaptation for Low-resource Language Families"], "Entity Linking": ["When Reasoning Meets Information Aggregation: A Case Study with Sports Narratives", "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting", "Unsupervised Named Entity Disambiguation for Low Resource Domains", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval", "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs"], "Adversarial Attack": ["Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "Reasoning Robustness of LLMs to Adversarial Typographical Errors", "RAFT: Realistic Attacks to Fool Text Detectors", "RAt: Injecting Implicit Bias for Text-To-Image Prompt Refinement Models", "Robust Text Classification: Analyzing Prototype-Based Networks", "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions"], "Multi-Hop Qa": ["Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "Towards Verifiable Text Generation with Evolving Memory and Self-Reflection", "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together", "FAME: Towards Factual Multi-Task Model Editing", "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs", "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles"], "Image-Text Retrieval": ["Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP", "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling"], "Text Classification Sentiment Analysis": ["Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "Rethinking the Evaluation of In-Context Learning for LLMs", "Quantum Recurrent Architectures for Text Classification", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "Transfer Learning for Text Classification via Model Risk Analysis"], "Text Simplification": ["Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts", "Revealing the Parallel Multilingual Learning within Large Language Models", "DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing", "Improving Minimum Bayes Risk Decoding with Multi-Prompt", "Enable Fast Sampling for Seq2Seq Text Diffusion", "ExpertEase: A Multi-Agent Framework for Grade-Specific Document Simplification with Large Language Models", "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP"], "Event Detection": ["GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "ADELIE: Aligning Large Language Models on Information Extraction", "LawBench: Benchmarking Legal Knowledge of Large Language Models", "Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments", "General Collaborative Framework between Large Language Model and Experts for Universal Information Extraction", "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models", "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction"], "Intent Classification": ["EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning", "Towards Robust Speech Representation Learning for Thousands of Languages", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement", "Multi-dimensional Evaluation of Empathetic Dialogue Responses", "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization", "Class Name Guided Out-of-Scope Intent Classification"], "Slot Filling": ["EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning", "Towards Robust Speech Representation Learning for Thousands of Languages", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "DC-Instruct: An Effective Framework for Generative Multi-intent Spoken Language Understanding", "Flee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling", "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science", "Designing Logic Pattern Templates for Counter-Argument Logical Structure Analysis"], "Preference Optimization": ["EPO: Hierarchical LLM Agents with Environment Preference Optimization", "Aligning Large Language Models with Diverse Political Viewpoints", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback"], "Grammatical Error Correction": ["A Survey of AMR Applications", "Multi-pass Decoding for Grammatical Error Correction", "LLM-based Code-Switched Text Generation for Grammatical Error Correction", "To Err Is Human, but Llamas Can Learn It Too", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language", "Gazelle: An Instruction Dataset for Arabic Writing Assistance", "Efficient and Interpretable Grammatical Error Correction with Mixture of Experts"], "Event Argument Extraction": ["ADELIE: Aligning Large Language Models on Information Extraction", "Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments", "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction", "OEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary", "MMUTF: Multimodal Multimedia Event Argument Extraction with Unified Template Filling", "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science", "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction"], "Paraphrase Identification": ["Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "Variational Language Concepts for Interpreting Foundation Language Models", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "Functionality learning through specification instructions", "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization"], "Error Analysis": ["ASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?", "NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?", "What's under the hood: Investigating Automatic Metrics on Meeting Summarization", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs"], "Stance Detection": ["The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets", "Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research", "I love pineapple on pizza != I hate pineapple on pizza: Stance-Aware Sentence Transformers for Opinion Mining", "Stanceformer: Target-Aware Transformer for Stance Detection", "Toeing the Party Line: Election Manifestos as a Key to Understand Political Discourse on Twitter", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models"], "Knowledge Graph Completion": ["Improving Knowledge Graph Completion with Structure-Aware Supervised Contrastive Learning", "MoCoKGC: Momentum Contrast Entity Encoding for Knowledge Graph Completion", "Joint Pre-Encoding Representation and Sturcture Embedding for Efficient and Low-Resource Knowledge Graph Completion", "Varying Sentence Representations via Condition-Specified Routers", "Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs", "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs", "SALMON: A Structure-Aware Language Model with logicality and densification strategy for Temporal Knowledge Graph Reasoning"], "Prompt Optimization": ["Prompts have evil twins", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "LLM as a metric critic for low resource relation identification"], "Persuasion": ["LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay", "An LLM Feature-based Framework for Dialogue Constructiveness Assessment", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments", "Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations", "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval"], "Multimodal Learning": ["When LLMs Meet Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "MEANT: Multimodal Encoder for Antecedent Information", "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs"], "Knowledge Injection": ["Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "LEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "Large Language Models Can Be Contextual Privacy Protection Learners", "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection"], "Planning": ["MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making", "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models", "On the Empirical Complexity of Reasoning and Planning in LLMs", "An Evaluation Mechanism of LLM-based Agents on Manipulating APIs"], "Qa": ["LONGEMBED: Extending Embedding Models for Long Context Retrieval", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs", "FAME: Towards Factual Multi-Task Model Editing", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models"], "Emotion Classification": ["Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification", "Message Passing on Semantic-Anchor-Graphs for Fine-grained Emotion Representation Learning and Classification", "Does Large Language Model Contain Task-Specific Neurons?", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Understanding Slang with LLMs: Modelling Cross-Cultural Nuances through Paraphrasing", "TUTOR-ICL: Guiding Large Language Models for Improved In-Context Learning Performance"], "Abstractive Summarization": ["Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model", "An Audit on the Perspectives and Challenges of Hallucinations in NLP", "Revealing the Parallel Multilingual Learning within Large Language Models", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Measuring the Robustness of NLP Models to Domain Shifts"], "Scienceqa": ["Model Balancing Helps Low-data Training and Fine-tuning", "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback", "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "AdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks"], "Fact-Checking": ["Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "Unknown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data", "M\u00b3D: MultiModal MultiDocument Fine-Grained Inconsistency Detection", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers"], "Reinforcement Learning": ["VIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation", "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use", "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "COFFEE-GYM: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code", "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models"], "Multi-Hop Question Answering": ["EfficientRAG: Efficient Retriever for Multi-Hop Question Answering", "From RAG to RICHES: Retrieval Interlaced with Sequence Generation", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities", "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments"], "Machine Translation Evaluation": ["What do Large Language Models Need for Machine Translation Evaluation?", "Beyond Reference: Evaluating High Quality Translations Better than Human References", "MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language", "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation", "Can Automatic Metrics Assess High-Quality Translations?", "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation"], "Task-Oriented Dialogue": ["Bootstrapped Policy Learning for Task-oriented Dialogue through Goal Shaping", "Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of the Noisy Channel", "What are the Generator Preferences for End-to-end Task-Oriented Dialog System?", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "Learning from Implicit User Feedback, Emotions and Demographic Information in Task-Oriented and Document-Grounded Dialogues", "Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue"], "Visual Grounding": ["World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "PropTest: Automatic Property Testing for Improved Visual Programming", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs"], "Long-Form Generation": ["Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "Analysis of Plan-based Retrieval for Grounded Text Generation", "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models", "Calibrating Long-form Generations from Large Language Models", "Downstream Trade-offs of a Family of Text Watermarks"], "Synthetic Data Generation": ["A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "Synthetic Multimodal Question Generation", "Better Alignment with Instruction Back-and-Forth Translation", "Aligners: Decoupling LLMs and Alignment", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information"], "Qqp": ["Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement", "When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models", "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models", "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune"], "Sentiment Analysis Topic Classification": ["SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "Inference and Verbalization Functions During In-Context Learning"], "Response Generation": ["Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System", "What are the Generator Preferences for End-to-end Task-Oriented Dialog System?", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "Thoughts to Target: Enhance Planning for Target-driven Conversation", "Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue", "SARCAT: Generative Span-Act Guided Response Generation Using Copy-Enhanced Target Augmentation"], "Preference Alignment": ["Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model", "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective", "ORPO: Monolithic Preference Optimization without Reference Model", "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness"], "Toxicity Reduction": ["RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework", "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding", "Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis", "Towards Aligning Language Models with Textual Feedback", "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models", "Promoting Constructive Deliberation: Reframing for Receptiveness"], "Question Answering Summarization": ["A Survey of AMR Applications", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Calibrating Long-form Generations from Large Language Models"], "Transfer Learning": ["Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "Data Contamination Can Cross Language Barriers", "Scaling Sentence Embeddings with Large Language Models", "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging"], "Vqa": ["SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "CommVQA: Situating Visual Question Answering in Communicative Contexts", "VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding", "Improving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design"], "Code Completion": ["CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models", "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression"], "Open-Domain Qa": ["Towards Verifiable Text Generation with Evolving Memory and Self-Reflection", "AGRAME: Any-Granularity Ranking with Multi-Vector Embeddings", "Improve Dense Passage Retrieval with Entailment Tuning", "Dense X Retrieval: What Retrieval Granularity Should We Use?", "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation", "Learning to Paraphrase for Alignment with LLM Preference"], "Misinformation Detection": ["MisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"", "Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research", "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness", "Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation"], "Coding": ["A Thorough Examination of Decoding Methods in the Era of LLMs", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "AUTODETECT: Towards a Unified Framework for Automated Weakness Detection in Large Language Models", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging"], "Multi-Label Classification": ["MOSEL: Inference Serving Using Dynamic Modality Selection", "A Closer Look at Multidimensional Online Political Incivility", "Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis", "Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification", "Random Label Forests: An Ensemble Method with Label Subsampling For Extreme Multi-Label Problems", "CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models"], "Question Answering Rag": ["Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR", "Searching for Best Practices in Retrieval-Augmented Generation", "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation", "R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression"], "Recommendation Systems": ["Efficient Sequential Decision Making with Large Language Models", "PepRec: Progressive Enhancement of Prompting for Recommendation", "Jump Starting Bandits with LLM-Generated Prior Knowledge", "I-AM-G: Interest Augmented Multimodal Generator for Item Personalization", "QUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware", "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval"], "Benchmark": ["RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs", "AKEW: Assessing Knowledge Editing in the Wild", "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation"], "Named Entity Recognition Relation Extraction": ["Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints", "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "C-ICL: Contrastive In-context Learning for Information Extraction", "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases"], "Cross-Lingual Transfer": ["Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", ">Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages"], "Model Compression": ["Structured Optimal Brain Pruning for Large Language Models", "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models"], "Reward Modeling": ["Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "Reward Modeling Requires Automatic Adjustment Based on Data Quality", "Semi-Supervised Reward Modeling via Iterative Self-Training", "On Diversified Preferences of Large Language Model Alignment", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"], "Temporal Reasoning": ["Temporally Consistent Factuality Probing for Large Language Models", "CAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "Remember This Event That Year? Assessing Temporal Information and Understanding in Large Language Models", "NARRATIVE-OF-THOUGHT: Improving Temporal Reasoning of Large Language Models via Recounted Narratives"], "Benchmarking": ["ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks", "BLADE: Benchmarking Language Model Agents for Data-Driven Science"], "Openbookqa": ["CHESS : Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "AdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning"], "Data Annotation": ["MULTI-NEWS+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation", "Large Language Models for Data Annotation and Synthesis: A Survey", "OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants", "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries", "Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model"], "Data Generation": ["Table Question Answering for Low-resourced Indic Languages", "Leveraging pre-trained language models for linguistic analysis: A case of argument structure constructions", "Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking"], "Word Sense Disambiguation": ["Hateful Word in Context Classification", "FOOL ME IF YOU CAN! An Adversarial Dataset to Investigate the Robustness of LMs in Word Sense Disambiguation", "More DWUGs: Extending and Evaluating Word Usage Graph Datasets in Multiple Languages", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention"], "Downstream Task Performance": ["\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing", "Target-Aware Language Modeling via Granular Data Sampling", "CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies", "LPZero: Language Model Zero-cost Proxy Search from Zero"], "Robustness Evaluation": ["RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning", "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "On the Robustness of Editing Large Language Models", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks"], "Ranking": ["Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing", "AGRAME: Any-Granularity Ranking with Multi-Vector Embeddings", "TROTR: A Framework for Evaluating the Recontextualization of Text", "Make Large Language Model a Better Ranker", "A Study of Implicit Ranking Unfairness in Large Language Models"], "Piqa": ["Tokenization Is More Than Compression", "CHESS : Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification", "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "Instruction Fine-Tuning: Does Prompt Loss Matter?", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model"], "Red Teaming": ["FLIRT: Feedback Loop In-context Red Teaming", "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction", "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "STAR: SocioTechnical Approach to Red Teaming Language Models", "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming"], "Human-Ai Collaboration": ["Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "On Evaluating Explanation Utility for Human-AI Decision Making in NLP", "Ask the experts: Sourcing a high-quality nutrition counseling dataset through Human-AI collaboration", "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP", "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation"], "Document Retrieval": ["DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities", "Unifying Multimodal Retrieval via Document Screenshot Embedding", "MixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity", "Threshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval", "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation"], "Parameter-Efficient Fine-Tuning": ["Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models", "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA"], "Addsub": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality"], "Aqua": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication"], "Csqa": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks"], "Date Understanding": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "Working Memory Identifies Reasoning Limits in Language Models", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework"], "Sst-2": ["AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning", "On Training Data Influence of GPT Models", "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models", "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune"], "Medical Visual Question Answering": ["RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "MedCoT: Medical Chain of Thought via Hierarchical Expert", "Self-Training Large Language and Vision Assistant for Medical Question-Answering", "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models", "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models"], "Explanation Generation": ["CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading", "Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving", "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs", "Enhancing High-order Interaction Awareness in LLM-based Recommender Model", "Beyond Persuasion: Towards Conversational Recommender System with Credible Explanations"], "Benchmark Creation": ["Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "MalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language", "CONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules", "SedarEval: Automated Evaluation using Self-Adaptive Rubrics"], "Visual Question Answering Image Captioning": ["Tag-grounded Visual Instruction Tuning with Retrieval Augmentation", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models"], "Multiarith": ["GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality"], "Interpretability": ["Backward Lens: Projecting Language Model Gradients into the Vocabulary Space", "Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning", "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings", "Variational Language Concepts for Interpreting Foundation Language Models", "Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes"], "Text Style Transfer": ["Reusing Transferable Weight Increments for Low-resource Style Generation", "Style-Specific Neurons for Steering LLMs in Text Style Transfer", "Towards Pareto-Efficient RLHF: Paying Attention to a Few High-Reward Samples with Reward Dropout", "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization", "TINYSTYLER: Efficient Few-Shot Text Style Transfer with Authorship Embeddings"], "Multiple-Choice Question Answering": ["Calibrating the Confidence of Large Language Models by Eliciting Fidelity", "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models", "BiMediX: Bilingual Medical Mixture of Experts LLM"], "Document Classification": ["DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models"], "Dependency Parsing": ["Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models", "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free-Word-Ordered and Morphologically-Rich Low-Resource Languages", "Contribution of Linguistic Typology to Universal Dependency Parsing: An Empirical Investigation", "A Morphology-Based Investigation of Positional Encodings", "Representation Alignment and Adversarial Networks for Cross-lingual Dependency Parsing"], "Active Learning": ["Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "Annotator-Centric Active Learning for Subjective NLP Tasks", "On the Fragility of Active Learners for Text Classification", "Active Learning for Abstractive Text Summarization via LLM-Determined Curriculum and Certainty Gain Maximization"], "Open-Ended Generation": ["Estimating Knowledge in Large Language Models Without Generating a Single Token", "RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "Factuality of Large Language Models: A Survey", "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation"], "Language Model Evaluation": ["PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models", "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning", "A Psycholinguistic Evaluation of Language Models' Sensitivity to Argument Roles", "Efficiently Computing Susceptibility to Context in Language Models", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation"], "Offensive Language Detection": ["Leveraging Conflicts in Social Media Posts: Unintended Offense Dataset", "ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations", "Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla"], "Mrpc": ["Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement", "Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models", "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune"], "Image Captioning Visual Question Answering": ["How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Nearest Neighbor Normalization Improves Multimodal Retrieval", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models"], "Hallucination Reduction": ["VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "MDPO: Conditional Preference Optimization for Multimodal Large Language Models", "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization", "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM", "Self-training Large Language Models through Knowledge Detection"], "Abstract Reasoning": ["Can Large Language Models Learn Independent Causal Mechanisms?", "ANALOBENCH: Benchmarking the Identification of Abstract and Long-context Analogies", "Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game", "Inference Helps PLMs' Conceptual Understanding: Improving the Abstract Inference Ability with Hierarchical Conceptual Entailment Graphs", "CELLO: Causal Evaluation of Large Vision-Language Models"], "Continual Pre-Training": ["Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models", "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models", "Improving Referring Ability for Biomedical Language Models", "Unlocking Continual Learning Abilities in Language Models"], "Watermarking": ["POSTMARK: A Robust Blackbox Watermark for Large Language Models", "Where Am I From? Identifying Origin of LLM-generated Content", "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models", "GuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack", "CODEIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code"], "Dialogue Systems": ["Efficient Sequential Decision Making with Large Language Models", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues"], "Task Planning": ["OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent", "E2CL: Exploration-based Error Correction Learning for Embodied Agents", "TrustAgent: Towards Safe and Trustworthy LLM-based Agents", "Learning to Use Tools via Cooperative and Interactive Agents with Large Language Models"], "Asr": ["Towards Robust Speech Representation Learning for Thousands of Languages", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech", "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR", "STTATTS: Unified Speech-To-Text And Text-To-Speech Model"], "Neural Machine Translation": ["Domain adapted machine translation: What does catastrophic forgetting forget and why?", "Granularity is crucial when applying differential privacy to text: An investigation for neural machine translation", "DICTDIS: Dictionary Constrained Disambiguation for Improved NMT", "Finding the Optimal Byte-Pair Encoding Merge Operations for Neural Machine Translation in a Low-Resource Setting", "Does Context Help Mitigate Gender Bias in Neural Machine Translation?"], "Instruction-Following": ["How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging", "Evolutionary Contrastive Distillation for Language Model Alignment", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "AUTODETECT: Towards a Unified Framework for Automated Weakness Detection in Large Language Models"], "Bias Evaluation": ["BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "Metrics for What, Metrics for Whom: Assessing Actionability of Bias Evaluation Metrics in NLP\\", "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models", "Gender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression"], "Instruction Fine-Tuning": ["HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "LexMatcher: Dictionary-centric Data Curation for LLM-based Machine Translation"], "Multimodal Reasoning": ["TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "Improving Multi-Agent Debate with Sparse Communication Topology", "Large Language Models Are Challenged by Habitat-Centered Reasoning", "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables"], "Spatial Reasoning": ["Grounding Language in Multi-Perspective Referential Communication", "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models", "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation", "ALANAVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding"]};
    window.addEventListener('DOMContentLoaded', () => {
        const plotDiv = document.querySelector(".plotly-graph-div");

        const titleBox = document.createElement("div");
        titleBox.id = "titleBox";
        titleBox.style.width = "90%";
        titleBox.style.maxHeight = "300px";
        titleBox.style.margin = "20px auto";
        titleBox.style.border = "1px solid gray";
        titleBox.style.overflowY = "scroll";
        titleBox.style.padding = "10px";
        titleBox.style.fontSize = "16px";
        titleBox.innerHTML = "Click Bars! You can see papers of the specific keyword";
        document.body.appendChild(titleBox);

        plotDiv.on('plotly_click', function(data) {
            const displayLabel = data.points[0].x;
            const titles = ngramMap[displayLabel] || [];
            let content = `<b style='font-size:18px;'>${displayLabel}</b><br><br>`;
            titles.forEach((t, i) => {
                content += `${i+1}. ${t}<br>`;
            });
            titleBox.innerHTML = content;
        });
    });
    </script>
    </body></html>
