<html><head><meta charset='utf-8'><title>Keyword Plot</title></head><body><div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>                <div id="bf81d4ae-d4f3-495f-90b6-743b6167afa1" class="plotly-graph-div" style="height:700px; width:3200px;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("bf81d4ae-d4f3-495f-90b6-743b6167afa1")) {                    Plotly.newPlot(                        "bf81d4ae-d4f3-495f-90b6-743b6167afa1",                        [{"hoverinfo":"text","marker":{"line":{"width":0}},"text":["\u003cb\u003eQA\u003c\u002fb\u003e\u003cbr\u003eLLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eDatasets for Multilingual Answer Sentence Selection\u003cbr\u003eTypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eWhen Context Leads but Parametric Memory Follows in Large Language Models\u003cbr\u003eSelf-Powered LLM Modality Expansion for Large Speech-Text Models\u003cbr\u003eCharacterizing LLM Abstention Behavior in Science QA with Context Perturbations\u003cbr\u003eUnraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003e... and 198 more","\u003cb\u003eIn-context Learning\u003c\u002fb\u003e\u003cbr\u003eEliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties\u003cbr\u003eBeyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs.\u003cbr\u003eOn the In-context Generation of Language Models\u003cbr\u003eWhich Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?\u003cbr\u003eWalking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias\u003cbr\u003eIn-Context Compositional Generalization for Large Vision-Language Models\u003cbr\u003eHow Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment\u003cbr\u003eStyle-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles\u003cbr\u003eExploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003e... and 133 more","\u003cb\u003eFine-tuning\u003c\u002fb\u003e\u003cbr\u003eEvaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding\u003cbr\u003eBreaking ReLU Barrier: Generalized MoEfication for Dense Pretrained Models\u003cbr\u003eALANAVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding\u003cbr\u003eAttribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification\u003cbr\u003eReasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths\u003cbr\u003eA Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives\u003cbr\u003eAda-Instruct: Adapting Instruction Generators for Complex Reasoning\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003eFast Forwarding Low-Rank Training\u003cbr\u003eTo Err Is Human, but Llamas Can Learn It Too\u003cbr\u003e... and 116 more","\u003cb\u003eZero-shot Learning\u003c\u002fb\u003e\u003cbr\u003eZero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages\u003cbr\u003eEfficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models\u003cbr\u003eFunctionality learning through specification instructions\u003cbr\u003eIs LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment\u003cbr\u003ePredicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement\u003cbr\u003ePALM: Few-Shot Prompt Learning for Audio Language Models\u003cbr\u003eHow Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment\u003cbr\u003eGenerate then Refine: Data Augmentation for Zero-shot Intent Detection\u003cbr\u003eSPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness\u003cbr\u003eYou Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions\u003cbr\u003e... and 101 more","\u003cb\u003eRAG\u003c\u002fb\u003e\u003cbr\u003eREADME: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP\u003cbr\u003eMAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering\u003cbr\u003eTypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\u003cbr\u003eFree your mouse! Command Large Language Models to Generate Code to Format Word Documents\u003cbr\u003eExtract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction\u003cbr\u003eMiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents\u003cbr\u003eLLOCO: Learning Long Contexts Offline\u003cbr\u003eSEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation\u003cbr\u003eBSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain\u003cbr\u003eImprove Dense Passage Retrieval with Entailment Tuning\u003cbr\u003e... and 92 more","\u003cb\u003eReasoning\u003c\u002fb\u003e\u003cbr\u003eMerely Judging Metaphor is Not Enough: Research on Reasonable Metaphor Detection\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003ePython is Not Always the Best Choice: Embracing Multilingual Program of Thoughts\u003cbr\u003eSOCIALGAZE: Improving the Integration of Human Social Norms in Large Language Models\u003cbr\u003eAfrican or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003eIn-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models\u003cbr\u003eSelf-AMPLIFY : Improving Small Language Models with Self Post Hoc Explanations\u003cbr\u003eMore Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs\u003cbr\u003eFirst Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning\u003cbr\u003e... and 92 more","\u003cb\u003eInstruction Tuning\u003c\u002fb\u003e\u003cbr\u003eTextLap: Customizing Language Models for Text-to-Layout Planning\u003cbr\u003eHow Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment\u003cbr\u003eSelf-Powered LLM Modality Expansion for Large Speech-Text Models\u003cbr\u003eUnraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications\u003cbr\u003eAmbigNLG: Addressing Task Ambiguity in Instruction for NLG\u003cbr\u003eFast Forwarding Low-Rank Training\u003cbr\u003eParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\u003cbr\u003eXRec: Large Language Models for Explainable Recommendation\u003cbr\u003eTowards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks\u003cbr\u003eAFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks\u003cbr\u003e... and 81 more","\u003cb\u003eGeneralization\u003c\u002fb\u003e\u003cbr\u003eMerely Judging Metaphor is Not Enough: Research on Reasonable Metaphor Detection\u003cbr\u003eOn the Generalization of Training-based ChatGPT Detection Methods\u003cbr\u003eFunctionality learning through specification instructions\u003cbr\u003eBreaking ReLU Barrier: Generalized MoEfication for Dense Pretrained Models\u003cbr\u003eParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\u003cbr\u003eRethinking Evaluation Methods for Machine Unlearning\u003cbr\u003eMorpheus: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space\u003cbr\u003eCross-Domain Audio Deepfake Detection: Dataset and Analysis\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003ePreserving Generalization of Language Models in Few-shot Continual Relation Extraction\u003cbr\u003e... and 81 more","\u003cb\u003eBenchmarks\u003c\u002fb\u003e\u003cbr\u003eCan visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!\u003cbr\u003eFactcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers\u003cbr\u003eKV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches\u003cbr\u003eEmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models\u003cbr\u003eEU DisinfoTest: a Benchmark for Evaluating Language Models' Ability to Detect Disinformation Narratives\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003ePSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer\u003cbr\u003eDocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction\u003cbr\u003eThe Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance\u003cbr\u003eINTENTIONQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce\u003cbr\u003e... and 80 more","\u003cb\u003eEvaluation\u003c\u002fb\u003e\u003cbr\u003eSonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets\u003cbr\u003eFactcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers\u003cbr\u003eKV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches\u003cbr\u003eMiTTenS: A Dataset for Evaluating Gender Mistranslation\u003cbr\u003eA Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations\u003cbr\u003eEmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models\u003cbr\u003eCliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios\u003cbr\u003ePSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer\u003cbr\u003eProSA: Assessing and Understanding the Prompt Sensitivity of LLMs\u003cbr\u003eTOWER: Tree Organized Weighting for Evaluating Complex Instructions\u003cbr\u003e... and 79 more","\u003cb\u003eFew-shot Learning\u003c\u002fb\u003e\u003cbr\u003eEliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties\u003cbr\u003eINDUCT-LEARN: Short Phrase Prompting with Instruction Induction\u003cbr\u003ePALM: Few-Shot Prompt Learning for Audio Language Models\u003cbr\u003eKV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eGenerate then Refine: Data Augmentation for Zero-shot Intent Detection\u003cbr\u003eAda-Instruct: Adapting Instruction Generators for Complex Reasoning\u003cbr\u003eDual-Phase Accelerated Prompt Optimization\u003cbr\u003eBeyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models\u003cbr\u003eSanitizing Large Language Models in Bug Detection with Data-Flow\u003cbr\u003e... and 79 more","\u003cb\u003eData Augmentation\u003c\u002fb\u003e\u003cbr\u003eREADME: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP\u003cbr\u003eHUMVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid\u003cbr\u003eLLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study\u003cbr\u003eRWKV-CLIP: A Robust Vision-Language Representation Learner\u003cbr\u003eADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eMeasuring and Improving Attentiveness to Partial Inputs with Counterfactuals\u003cbr\u003eGenerate then Refine: Data Augmentation for Zero-shot Intent Detection\u003cbr\u003eTogether We Can: Multilingual Automatic Post-Editing for Low-Resource Languages\u003cbr\u003eYou Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions\u003cbr\u003e... and 72 more","\u003cb\u003eText Classification\u003c\u002fb\u003e\u003cbr\u003eOn the Generalization of Training-based ChatGPT Detection Methods\u003cbr\u003eEfficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models\u003cbr\u003eSCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models\u003cbr\u003eTextual Dataset Distillation via Language Model Embedding\u003cbr\u003eADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers\u003cbr\u003eFine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003eMental Disorder Classification via Temporal Representation of Text\u003cbr\u003eEfficient Active Learning with Adapters\u003cbr\u003eFighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation\u003cbr\u003e... and 71 more","\u003cb\u003eRobustness\u003c\u002fb\u003e\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eTypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\u003cbr\u003eADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eA Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations\u003cbr\u003eIM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method\u003cbr\u003eInternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States\u003cbr\u003eProSA: Assessing and Understanding the Prompt Sensitivity of LLMs\u003cbr\u003eRethinking Evaluation Methods for Machine Unlearning\u003cbr\u003eContext-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models\u003cbr\u003e... and 68 more","\u003cb\u003eSummarization\u003c\u002fb\u003e\u003cbr\u003eSYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization\u003cbr\u003eCalibrating Long-form Generations from Large Language Models\u003cbr\u003eLearning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA\u003cbr\u003eTowards Aligning Language Models with Textual Feedback\u003cbr\u003eRLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs\u003cbr\u003eOptimized Speculative Sampling for GPU Hardware Accelerators\u003cbr\u003eSearching for Best Practices in Retrieval-Augmented Generation\u003cbr\u003eSummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization\u003cbr\u003eModel Editing Harms General Abilities of Large Language Models: Regularization to the Rescue\u003cbr\u003eKV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches\u003cbr\u003e... and 66 more","\u003cb\u003eMachine Translation\u003c\u002fb\u003e\u003cbr\u003eLadder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level\u003cbr\u003eBack to School: Translation Using Grammar Books\u003cbr\u003eMultiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing\u003cbr\u003eDatasets for Multilingual Answer Sentence Selection\u003cbr\u003eUnveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism\u003cbr\u003eMMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language\u003cbr\u003eLeveraging Grammar Induction for Language Understanding and Generation\u003cbr\u003eLLMs Are Zero-Shot Context-Aware Simultaneous Translators\u003cbr\u003eMiTTenS: A Dataset for Evaluating Gender Mistranslation\u003cbr\u003eWomen Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling\u003cbr\u003e... and 63 more","\u003cb\u003eContrastive Learning\u003c\u002fb\u003e\u003cbr\u003eAn Instruction Tuning-Based Contrastive Learning Framework for Aspect Sentiment Quad Prediction with Implicit Aspects and Opinions\u003cbr\u003eDeciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning\u003cbr\u003eInvestigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models\u003cbr\u003eVideo-Text Prompting for Weakly Supervised Spatio-Temporal Video Grounding\u003cbr\u003eCan Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs\u003cbr\u003eSignCLIP: Connecting Text and Sign Language by Contrastive Learning\u003cbr\u003eVanessa : Visual Connotation and Aesthetic Attributes Understanding Network for Multimodal Aspect-based Sentiment Analysis\u003cbr\u003eInterpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding\u003cbr\u003ePseudo-Label Enhanced Prototypical Contrastive Learning for Uniformed Intent Discovery\u003cbr\u003eShortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning\u003cbr\u003e... and 63 more","\u003cb\u003eSentiment Analysis\u003c\u002fb\u003e\u003cbr\u003eRethinking the Evaluation of In-Context Learning for LLMs\u003cbr\u003eAn Instruction Tuning-Based Contrastive Learning Framework for Aspect Sentiment Quad Prediction with Implicit Aspects and Opinions\u003cbr\u003eThe Overlooked Repetitive Lengthening Form in Sentiment Analysis\u003cbr\u003eQuantum Recurrent Architectures for Text Classification\u003cbr\u003eCrisis counselor language and perceived genuine concern in crisis conversations\u003cbr\u003eFunctionality learning through specification instructions\u003cbr\u003eUnveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism\u003cbr\u003eUnleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training\u003cbr\u003eDA\u00b3: A Distribution-Aware Adversarial Attack against Language Models\u003cbr\u003eLLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study\u003cbr\u003e... and 62 more","\u003cb\u003eLanguage Modeling\u003c\u002fb\u003e\u003cbr\u003eEvaluating n-Gram Novelty of Language Models Using RUSTY-DAWG\u003cbr\u003eBreaking the Curse of Multilinguality with Cross-lingual Expert Language Models\u003cbr\u003eTeaching LLMs to Abstain across Languages via Multilingual Feedback\u003cbr\u003eSCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models\u003cbr\u003eLanguage Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS\u003cbr\u003eBirdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives\u003cbr\u003eHop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models\u003cbr\u003eKidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions\u003cbr\u003eLeading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Probabilities\u003cbr\u003eCan Large Language Models Learn Independent Causal Mechanisms?\u003cbr\u003e... and 62 more","\u003cb\u003eText Generation\u003c\u002fb\u003e\u003cbr\u003ePrecise Model Benchmarking with Only a Few Observations\u003cbr\u003eEvaluating n-Gram Novelty of Language Models Using RUSTY-DAWG\u003cbr\u003eReverse-Engineering the Reader\u003cbr\u003eEnable Fast Sampling for Seq2Seq Text Diffusion\u003cbr\u003eControl Large Language Models via Divide and Conquer\u003cbr\u003eIs LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment\u003cbr\u003eAdversarial Text Generation using Large Language Models for Dementia Detection\u003cbr\u003eImproving Diversity of Commonsense Generation by Large Language Models via In-Context Learning\u003cbr\u003ePrompts have evil twins\u003cbr\u003eKidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions\u003cbr\u003e... and 60 more","\u003cb\u003eInterpretability\u003c\u002fb\u003e\u003cbr\u003eInterpretability-based Tailored Knowledge Editing in Transformers\u003cbr\u003eRethinking the Role of Proxy Rewards in Language Model Alignment\u003cbr\u003eEfficient and Interpretable Grammatical Error Correction with Mixture of Experts\u003cbr\u003eInformation Flow Routes: Automatically Interpreting Language Models at Scale\u003cbr\u003eCan AI Relate: Testing Large Language Model Response for Mental Health Support\u003cbr\u003eAdversarial Text Generation using Large Language Models for Dementia Detection\u003cbr\u003eStyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements\u003cbr\u003eBackward Lens: Projecting Language Model Gradients into the Vocabulary Space\u003cbr\u003eInterpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding\u003cbr\u003eNALA: an Effective and Interpretable Entity Alignment Method\u003cbr\u003e... and 59 more","\u003cb\u003eNatural Language Inference\u003c\u002fb\u003e\u003cbr\u003eSelf-Contradictory Reasoning Evaluation and Detection\u003cbr\u003eLLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study\u003cbr\u003eHow Hard is this Test Set?\nNLI Characterization by Exploiting Training Dynamics\u003cbr\u003eIs This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text\u003cbr\u003eModel Editing Harms General Abilities of Large Language Models: Regularization to the Rescue\u003cbr\u003eDADEE: Unsupervised Domain Adaptation in Early Exit PLMS\u003cbr\u003eMeasuring and Improving Attentiveness to Partial Inputs with Counterfactuals\u003cbr\u003e\u201cSeeing the Big through the Small\u201d: Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?\u003cbr\u003eHyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation\u003cbr\u003eECON: On the Detection and Resolution of Evidence Conflicts\u003cbr\u003e... and 56 more","\u003cb\u003eCommonsense Reasoning\u003c\u002fb\u003e\u003cbr\u003eGuided Knowledge Generation with Language Models for Commonsense Reasoning\u003cbr\u003eLarge Language Models Can Self-Correct with Key Condition Verification\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eMaking Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eCommonsense Knowledge Editing Based on Free-Text in LLMs\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eRoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization\u003cbr\u003eImproving Diversity of Commonsense Generation by Large Language Models via In-Context Learning\u003cbr\u003e... and 53 more","\u003cb\u003eDataset\u003c\u002fb\u003e\u003cbr\u003eTowards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs\u003cbr\u003eMMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems\u003cbr\u003eSonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets\u003cbr\u003eMediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eFOLIO: Natural Language Reasoning with First-Order Logic\u003cbr\u003eCOCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds\u003cbr\u003eCasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures\u003cbr\u003eSummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization\u003cbr\u003eTextLap: Customizing Language Models for Text-to-Layout Planning\u003cbr\u003e... and 52 more","\u003cb\u003eVisual QA\u003c\u002fb\u003e\u003cbr\u003eM5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eInvestigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models\u003cbr\u003eAUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models\u003cbr\u003eCan CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP\u003cbr\u003eTransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling\u003cbr\u003eAn Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models\u003cbr\u003eTowards One-to-Many Visual Question Answering\u003cbr\u003eMAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering\u003cbr\u003eThe Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems\u003cbr\u003e... and 51 more","\u003cb\u003eVision-language Models\u003c\u002fb\u003e\u003cbr\u003eModSCAN: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities\u003cbr\u003eFoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture\u003cbr\u003eEliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eInvestigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models\u003cbr\u003eDoes Object Grounding Really Reduce Hallucination of Large Vision-Language Models?\u003cbr\u003eCan CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP\u003cbr\u003eAUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models\u003cbr\u003ePredicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement\u003cbr\u003eDecompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison\u003cbr\u003e... and 49 more","\u003cb\u003eReinforcement Learning\u003c\u002fb\u003e\u003cbr\u003eRaFe: Ranking Feedback Improves Query Rewriting for RAG\u003cbr\u003eReward Difference Optimization For Sample Reweighting In Offline RLHF\u003cbr\u003eTowards Aligning Language Models with Textual Feedback\u003cbr\u003eE2CL: Exploration-based Error Correction Learning for Embodied Agents\u003cbr\u003eBirdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives\u003cbr\u003eText2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback\u003cbr\u003eUsing RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media\u003cbr\u003eWeak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems\u003cbr\u003eNavigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models\u003cbr\u003eHolistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction\u003cbr\u003e... and 48 more","\u003cb\u003eInstruction Following\u003c\u002fb\u003e\u003cbr\u003eFAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition\u003cbr\u003eAutomatic Instruction Evolving for Large Language Models\u003cbr\u003eHow Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment\u003cbr\u003ePreference-Guided Reflective Sampling for Aligning Language Models\u003cbr\u003eEliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation\u003cbr\u003eInvestigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?\u003cbr\u003eDual-Space Knowledge Distillation for Large Language Models\u003cbr\u003eNebula: A Discourse-Aware Minecraft Builder\u003cbr\u003eGAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities\u003cbr\u003eMathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models\u003cbr\u003e... and 47 more","\u003cb\u003eBenchmarksing\u003c\u002fb\u003e\u003cbr\u003ePrecise Model Benchmarking with Only a Few Observations\u003cbr\u003eTowards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs\u003cbr\u003eMMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems\u003cbr\u003eSonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets\u003cbr\u003eECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eFOLIO: Natural Language Reasoning with First-Order Logic\u003cbr\u003eRevisiting Automated Evaluation for Long-form Table Question Answering\u003cbr\u003eClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures\u003cbr\u003eDifficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs\u003cbr\u003e... and 43 more","\u003cb\u003eAlignment\u003c\u002fb\u003e\u003cbr\u003eLet Me Teach You: Pedagogical Foundations of Feedback for Language Models\u003cbr\u003eARM: An Alignment-and-Replacement Module for Chinese Spelling Check Based on LLMs\u003cbr\u003eTowards Aligning Language Models with Textual Feedback\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eHow Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States\u003cbr\u003eLLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification\u003cbr\u003eEnhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing\u003cbr\u003eReward Modeling Requires Automatic Adjustment Based on Data Quality\u003cbr\u003eSplit and Merge: Aligning Position Biases in LLM-based Evaluators\u003cbr\u003eHouseholder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective\u003cbr\u003e... and 41 more","\u003cb\u003eChain-of-thought\u003c\u002fb\u003e\u003cbr\u003eGuided Knowledge Generation with Language Models for Commonsense Reasoning\u003cbr\u003eMaking Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning\u003cbr\u003eLaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eCoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering\u003cbr\u003eINDUCT-LEARN: Short Phrase Prompting with Instruction Induction\u003cbr\u003eIs This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text\u003cbr\u003eWrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information\u003cbr\u003eI Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003e... and 40 more","\u003cb\u003eLow-resource Languages\u003c\u002fb\u003e\u003cbr\u003eBack to School: Translation Using Grammar Books\u003cbr\u003eM5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks\u003cbr\u003eMultiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing\u003cbr\u003eTargeted Multilingual Adaptation for Low-resource Language Families\u003cbr\u003eTeaching LLMs to Abstain across Languages via Multilingual Feedback\u003cbr\u003eZero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages\u003cbr\u003eEfficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models\u003cbr\u003eDatasets for Multilingual Answer Sentence Selection\u003cbr\u003eRepresentation Alignment and Adversarial Networks for Cross-lingual Dependency Parsing\u003cbr\u003eTEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models\u003cbr\u003e... and 40 more","\u003cb\u003eTransfer Learning\u003c\u002fb\u003e\u003cbr\u003eEfficient and Interpretable Grammatical Error Correction with Mixture of Experts\u003cbr\u003eMerge to Learn: Efficiently Adding Skills to Language Models with Model Merging\u003cbr\u003eAuto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents\u003cbr\u003eDatasets for Multilingual Answer Sentence Selection\u003cbr\u003eMitigating Catastrophic Forgetting in Language Transfer via Model Merging\u003cbr\u003eLanguage Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS\u003cbr\u003eMulti-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation\u003cbr\u003eM2QA: Multi-domain Multilingual Question Answering\u003cbr\u003eWeak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems\u003cbr\u003eRepresentation Alignment and Adversarial Networks for Cross-lingual Dependency Parsing\u003cbr\u003e... and 39 more","\u003cb\u003eHallucination\u003c\u002fb\u003e\u003cbr\u003eUnified Active Retrieval for Retrieval Augmented Generation\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eDoes Object Grounding Really Reduce Hallucination of Large Vision-Language Models?\u003cbr\u003eAUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models\u003cbr\u003eCoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering\u003cbr\u003eEnhancing Training Data Attribution for Large Language Models with Fitting Error Consideration\u003cbr\u003eWhen Context Leads but Parametric Memory Follows in Large Language Models\u003cbr\u003eFactuality of Large Language Models: A Survey\u003cbr\u003eModel-based Preference Optimization in Abstractive Summarization without Human Feedback\u003cbr\u003eNavigating Hallucinations for Reasoning of Unintentional Activities\u003cbr\u003e... and 39 more","\u003cb\u003eCode Generation\u003c\u002fb\u003e\u003cbr\u003eSciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement\u003cbr\u003eMMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems\u003cbr\u003eAutomatic Instruction Evolving for Large Language Models\u003cbr\u003eECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?\u003cbr\u003eLlama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection\u003cbr\u003eCoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing\u003cbr\u003eGeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation\u003cbr\u003eFree your mouse! Command Large Language Models to Generate Code to Format Word Documents\u003cbr\u003eI Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses\u003cbr\u003eUnlocking Memorization in Large Language Models with Dynamic Soft Prompting\u003cbr\u003e... and 37 more","\u003cb\u003eInformation Retrieval\u003c\u002fb\u003e\u003cbr\u003eMEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation\u003cbr\u003eEvaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark\u003cbr\u003eFew-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model\u003cbr\u003eZero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval\u003cbr\u003eClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures\u003cbr\u003eDiscovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation\u003cbr\u003eScaling Laws for Linear Complexity Language Models\u003cbr\u003eLLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement\u003cbr\u003eOmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer\u003cbr\u003eDense X Retrieval: What Retrieval Granularity Should We Use?\u003cbr\u003e... and 36 more","\u003cb\u003eParameter-efficient Fine-tuning\u003c\u002fb\u003e\u003cbr\u003eADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers\u003cbr\u003eHeterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models\u003cbr\u003eDLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model\u003cbr\u003eAdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning\u003cbr\u003eHyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation\u003cbr\u003eLayer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models\u003cbr\u003eLet the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models\u003cbr\u003eNot Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment\u003cbr\u003eFast Forwarding Low-Rank Training\u003cbr\u003eOPEN-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models\u003cbr\u003e... and 33 more","\u003cb\u003eBias\u003c\u002fb\u003e\u003cbr\u003eDelving into Qualitative Implications of Synthetic Data for Hate Speech Detection\u003cbr\u003eTowards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey\u003cbr\u003e\u201cThey are uncultured\u201d: Unveiling Covert Harms and Social Threats in LLM Generated Conversations\u003cbr\u003eCan AI Relate: Testing Large Language Model Response for Mental Health Support\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003ePARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data\u003cbr\u003eMiTTenS: A Dataset for Evaluating Gender Mistranslation\u003cbr\u003eSYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation\u003cbr\u003eSOCIALGAZE: Improving the Integration of Human Social Norms in Large Language Models\u003cbr\u003eHouseholder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective\u003cbr\u003e... and 32 more","\u003cb\u003eSynthetic Data\u003c\u002fb\u003e\u003cbr\u003eEffective Synthetic Data and Test-Time Adaptation for OCR Correction\u003cbr\u003ePersonas as a Way to Model Truthfulness in Language Models\u003cbr\u003eLLMs Are Prone to Fallacies in Causal Inference\u003cbr\u003eDelving into Qualitative Implications of Synthetic Data for Hate Speech Detection\u003cbr\u003ePlot Twist: Multimodal Models Don't Comprehend Simple Chart Details\u003cbr\u003eOn the In-context Generation of Language Models\u003cbr\u003eDe-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP\u003cbr\u003eI Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses\u003cbr\u003eCACTUS: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory\u003cbr\u003eMiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents\u003cbr\u003e... and 32 more","\u003cb\u003eMathematical Reasoning\u003c\u002fb\u003e\u003cbr\u003eMaking Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning\u003cbr\u003eAutomatic Instruction Evolving for Large Language Models\u003cbr\u003eAlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations\u003cbr\u003eWeak-to-Strong Reasoning\u003cbr\u003eWrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information\u003cbr\u003eHow Does Quantization Affect Multilingual LLMs?\u003cbr\u003eExploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions\u003cbr\u003eLearning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing\u003cbr\u003eLM\u00b2: A Simple Society of Language Models Solves Complex Reasoning\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003e... and 31 more","\u003cb\u003eExplainability\u003c\u002fb\u003e\u003cbr\u003eThe Overlooked Repetitive Lengthening Form in Sentiment Analysis\u003cbr\u003eRevealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eREADME: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP\u003cbr\u003eHow Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States\u003cbr\u003eCURE: Context- and Uncertainty-Aware Mental Disorder Detection\u003cbr\u003eLLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study\u003cbr\u003eSummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization\u003cbr\u003eEnhancing Healthcare LLM Trust with Atypical Presentations Recalibration\u003cbr\u003eIf CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions\u003cbr\u003e... and 30 more","\u003cb\u003eEvaluation Metrics\u003c\u002fb\u003e\u003cbr\u003eEvaluating Readability and Faithfulness of Concept-based Explanations\u003cbr\u003eIs LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment\u003cbr\u003eMMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language\u003cbr\u003eAPPLS: Evaluating Evaluation Metrics for Plain Language Summarization\u003cbr\u003eRecent Trends in Linear Text Segmentation: A Survey\u003cbr\u003eEfficiently Computing Susceptibility to Context in Language Models\u003cbr\u003eCan Automatic Metrics Assess High-Quality Translations?\u003cbr\u003eDistractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation\u003cbr\u003eA LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation\u003cbr\u003eScalable and Domain-General Abstractive Proposition Segmentation\u003cbr\u003e... and 30 more","\u003cb\u003eKnowledge Distillation\u003c\u002fb\u003e\u003cbr\u003eTransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling\u003cbr\u003eRefiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities\u003cbr\u003eMulti-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation\u003cbr\u003eDiscovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation\u003cbr\u003eDADEE: Unsupervised Domain Adaptation in Early Exit PLMS\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eA Coarse-to-Fine Prototype Learning Approach for Multi-Label Few-Shot Intent Detection\u003cbr\u003eDual-Space Knowledge Distillation for Large Language Models\u003cbr\u003eLM\u00b2: A Simple Society of Language Models Solves Complex Reasoning\u003cbr\u003eDual-teacher Knowledge Distillation for Low-frequency Word Translation\u003cbr\u003e... and 30 more","\u003cb\u003eDirect Preference Optimization\u003c\u002fb\u003e\u003cbr\u003eMaking Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eFlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization\u003cbr\u003eAdaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers\u003cbr\u003eEnhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing\u003cbr\u003eModel-based Preference Optimization in Abstractive Summarization without Human Feedback\u003cbr\u003eLearning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing\u003cbr\u003eClosing the Loop: Learning to Generate Writing Feedback via Language Model Simulated Student Revisions\u003cbr\u003eLearning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain\u003cbr\u003ePedagogical Alignment of Large Language Models\u003cbr\u003e... and 29 more","\u003cb\u003eClassification\u003c\u002fb\u003e\u003cbr\u003eUnified Active Retrieval for Retrieval Augmented Generation\u003cbr\u003eAdversarial Text Generation using Large Language Models for Dementia Detection\u003cbr\u003eUnveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism\u003cbr\u003eComputational Meme Understanding: A Survey\u003cbr\u003eA Closer Look at Multidimensional Online Political Incivility\u003cbr\u003eSYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation\u003cbr\u003e\"I Never Said That\": A dataset, taxonomy and baselines on response clarity classification\u003cbr\u003eIs It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?\u003cbr\u003eQuantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles\u003cbr\u003eEvaluating Differentially Private Synthetic Data Generation in High-Stakes Domains\u003cbr\u003e... and 29 more","\u003cb\u003eImage Captioning\u003c\u002fb\u003e\u003cbr\u003ePrecise Model Benchmarking with Only a Few Observations\u003cbr\u003eM5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks\u003cbr\u003eInvestigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models\u003cbr\u003eDoes Object Grounding Really Reduce Hallucination of Large Vision-Language Models?\u003cbr\u003eTransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling\u003cbr\u003eIFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning\u003cbr\u003eBeyond Embeddings: The Promise of Visual Table in Visual Reasoning\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eImproving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design\u003cbr\u003eUOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models\u003cbr\u003e... and 26 more","\u003cb\u003eMultimodal Learning\u003c\u002fb\u003e\u003cbr\u003eMMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems\u003cbr\u003eMEANT: Multimodal Encoder for Antecedent Information\u003cbr\u003eFrom LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking\u003cbr\u003eVIEWS: Entity-Aware News Video Captioning\u003cbr\u003eHope 'The Paragraph Guy' explains the rest : Introducing MeSum, the Meme Summarizer\u003cbr\u003eSelf-Powered LLM Modality Expansion for Large Speech-Text Models\u003cbr\u003eGlobal Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents\u003cbr\u003eTrain Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation\u003cbr\u003eWhen LLMs Meet Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection\u003cbr\u003eMMUTF: Multimodal Multimedia Event Argument Extraction with Unified Template Filling\u003cbr\u003e... and 26 more","\u003cb\u003eEfficiency\u003c\u002fb\u003e\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eInstruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks\u003cbr\u003eLLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning\u003cbr\u003eChain and Causal Attention for Efficient Entity Tracking\u003cbr\u003eMulti-pass Decoding for Grammatical Error Correction\u003cbr\u003eBridging Local Details and Global Context in Text-Attributed Graphs\u003cbr\u003eMedAdapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning\u003cbr\u003ePURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness\u003cbr\u003eTrain Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003e... and 26 more","\u003cb\u003eNamed Entity Recognition\u003c\u002fb\u003e\u003cbr\u003eEmbedded Named Entity Recognition using Probing Classifiers\u003cbr\u003eZero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages\u003cbr\u003eHop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models\u003cbr\u003eModel Editing Harms General Abilities of Large Language Models: Regularization to the Rescue\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eFew-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eMedINST: Meta Dataset of Biomedical Instructions\u003cbr\u003eMELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science\u003cbr\u003eReap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora\u003cbr\u003e... and 26 more","\u003cb\u003eFairness\u003c\u002fb\u003e\u003cbr\u003eDelving into Qualitative Implications of Synthetic Data for Hate Speech Detection\u003cbr\u003eCan AI Relate: Testing Large Language Model Response for Mental Health Support\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eReconfidencing LLMs from the Grouping Loss Perspective\u003cbr\u003eTWBias: A Benchmark for Assessing Social Bias in Traditional Chinese Large Language Models through a Taiwan Cultural Lens\u003cbr\u003eJobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models\u003cbr\u003eSOCIALGAZE: Improving the Integration of Human Social Norms in Large Language Models\u003cbr\u003e\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models\u003cbr\u003eEvaluating Differentially Private Synthetic Data Generation in High-Stakes Domains\u003cbr\u003e... and 25 more","\u003cb\u003eTransformer\u003c\u002fb\u003e\u003cbr\u003eLeveraging Grammar Induction for Language Understanding and Generation\u003cbr\u003eUnlocking Memorization in Large Language Models with Dynamic Soft Prompting\u003cbr\u003eGeneration with Dynamic Vocabulary\u003cbr\u003eLaCo: Large Language Model Pruning via Layer Collapse\u003cbr\u003eSeeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients\u003cbr\u003eRationalizing Transformer Predictions via End-To-End Differentiable Self-Training\u003cbr\u003eNormalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction\u003cbr\u003eInsights into LLM Long-Context Failures: When Transformers Know but Don't Tell\u003cbr\u003eVarying Sentence Representations via Condition-Specified Routers\u003cbr\u003eInitialization of Large Language Models via Reparameterization to Mitigate Loss Spikes\u003cbr\u003e... and 25 more","\u003cb\u003eMulti-task Learning\u003c\u002fb\u003e\u003cbr\u003eMEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation\u003cbr\u003eDeciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eWhere Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing\u003cbr\u003eBiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks\u003cbr\u003eLarge Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment\u003cbr\u003eEnhancing Emotion-Cause Pair Extraction in Conversations via Center Event Detection and Reasoning\u003cbr\u003eMETAKP: On-Demand Keyphrase Generation\u003cbr\u003eMixed Distillation Helps Smaller Language Models Reason Better\u003cbr\u003eConditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning\u003cbr\u003e... and 24 more","\u003cb\u003eMultilingual\u003c\u002fb\u003e\u003cbr\u003eMMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eDatasets for Multilingual Answer Sentence Selection\u003cbr\u003ePython is Not Always the Best Choice: Embracing Multilingual Program of Thoughts\u003cbr\u003eM2QA: Multi-domain Multilingual Question Answering\u003cbr\u003eUnlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering\u003cbr\u003eCasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures\u003cbr\u003ePARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data\u003cbr\u003eSignCLIP: Connecting Text and Sign Language by Contrastive Learning\u003cbr\u003eSPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness\u003cbr\u003e... and 22 more","\u003cb\u003eCross-lingual Transfer\u003c\u002fb\u003e\u003cbr\u003eMMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos\u003cbr\u003eBreaking the Curse of Multilinguality with Cross-lingual Expert Language Models\u003cbr\u003eEfficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models\u003cbr\u003eRLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs\u003cbr\u003e\u003eWhy Does New Knowledge Create Messy Ripple Effects in LLMs?\u003cbr\u003eBreaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale\u003cbr\u003eInvestigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?\u003cbr\u003eSelf-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages\u003cbr\u003eCross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models\u003cbr\u003eBreaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment\u003cbr\u003e... and 21 more","\u003cb\u003ePre-training\u003c\u002fb\u003e\u003cbr\u003eTransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling\u003cbr\u003eCASE: Efficient Curricular Data Pre-training for Building Assistive Psychology Expert Models\u003cbr\u003eWhich Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?\u003cbr\u003eScaling Laws for Fact Memorization of Large Language Models\u003cbr\u003eNumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning\u003cbr\u003eA Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery\u003cbr\u003eWhen Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages\u003cbr\u003eAre ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity\u003cbr\u003eBanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla\u003cbr\u003eA Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models\u003cbr\u003e... and 20 more","\u003cb\u003eFaithfulness\u003c\u002fb\u003e\u003cbr\u003eMaking Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning\u003cbr\u003eEvaluating Readability and Faithfulness of Concept-based Explanations\u003cbr\u003eLearning to Rank Salient Content for Query-focused Summarization\u003cbr\u003eRevisiting Automated Evaluation for Long-form Table Question Answering\u003cbr\u003eThe Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems\u003cbr\u003eAPPLS: Evaluating Evaluation Metrics for Plain Language Summarization\u003cbr\u003eModel-based Preference Optimization in Abstractive Summarization without Human Feedback\u003cbr\u003eTowards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering\u003cbr\u003eActivation Scaling for Steering and Interpreting Language Models\u003cbr\u003eDial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning\u003cbr\u003e... and 20 more","\u003cb\u003eHuman Evaluation\u003c\u002fb\u003e\u003cbr\u003eMMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language\u003cbr\u003eWeak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems\u003cbr\u003eHow Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?\u003cbr\u003ePARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data\u003cbr\u003eEnhancing AI Assisted Writing with One-Shot Implicit Negative Feedback\u003cbr\u003eInvestigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?\u003cbr\u003eHow Does Quantization Affect Multilingual LLMs?\u003cbr\u003eRethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning\u003cbr\u003eReference-based Metrics Disprove Themselves in Question Generation\u003cbr\u003eIs Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering\u003cbr\u003e... and 19 more","\u003cb\u003eNatural Language Understanding\u003c\u002fb\u003e\u003cbr\u003eLarge Language Models Can Not Perform Well in Understanding and Manipulating Natural Language at Both Character and Word Levels?\u003cbr\u003eMediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations\u003cbr\u003ePerceptions of Linguistic Uncertainty by Language Models and Humans\u003cbr\u003eLeveraging Grammar Induction for Language Understanding and Generation\u003cbr\u003eBiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks\u003cbr\u003eAdvancing Event Causality Identification via Heuristic Semantic Dependency Inquiry Network\u003cbr\u003ePragmatic Norms Are All You Need \u2013 Why The Symbol Grounding Problem Does Not Apply to LLMs\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eHyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation\u003cbr\u003eEncouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\u003cbr\u003e... and 18 more","\u003cb\u003eReinforcement Learning From Human Feedback\u003c\u002fb\u003e\u003cbr\u003eRethinking the Role of Proxy Rewards in Language Model Alignment\u003cbr\u003eFlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003eTowards Pareto-Efficient RLHF: Paying Attention to a Few High-Reward Samples with Reward Dropout\u003cbr\u003eNavigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models\u003cbr\u003eModel-based Preference Optimization in Abstractive Summarization without Human Feedback\u003cbr\u003eReward Modeling Requires Automatic Adjustment Based on Data Quality\u003cbr\u003eOptimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning\u003cbr\u003eCalibrating Language Models with Adaptive Temperature Scaling\u003cbr\u003eGlobal Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents\u003cbr\u003e... and 18 more","\u003cb\u003eTransformers\u003c\u002fb\u003e\u003cbr\u003eInterpretability-based Tailored Knowledge Editing in Transformers\u003cbr\u003eCommonsense Knowledge Editing Based on Free-Text in LLMs\u003cbr\u003eBirdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives\u003cbr\u003eMOSEL: Inference Serving Using Dynamic Modality Selection\u003cbr\u003eRecent Trends in Linear Text Segmentation: A Survey\u003cbr\u003eMolTRES: Improving Chemical Language Representation Learning for Molecular Property Prediction\u003cbr\u003eCan Transformers Learn n-gram Language Models?\u003cbr\u003eChain and Causal Attention for Efficient Entity Tracking\u003cbr\u003eModeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding\u003cbr\u003eInvestigating Mysteries of CoT-Augmented Distillation\u003cbr\u003e... and 18 more","\u003cb\u003eInformation Extraction\u003c\u002fb\u003e\u003cbr\u003eEmbedded Named Entity Recognition using Probing Classifiers\u003cbr\u003eSciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement\u003cbr\u003eRefiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities\u003cbr\u003eUpdating Large Language Models' Memories with Time Constraints\u003cbr\u003eTKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs\u003cbr\u003eDe-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP\u003cbr\u003eText-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eSchema-Driven Information Extraction from Heterogeneous Tables\u003cbr\u003eGeneral Collaborative Framework between Large Language Model and Experts for Universal Information Extraction\u003cbr\u003e... and 18 more","\u003cb\u003eBias Mitigation\u003c\u002fb\u003e\u003cbr\u003eBiasDora: Exploring Hidden Biased Associations in Vision-Language Models\u003cbr\u003eCan AI Relate: Testing Large Language Model Response for Mental Health Support\u003cbr\u003eStudying and Mitigating Biases in Sign Language Understanding Models\u003cbr\u003e\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models\u003cbr\u003eSplit and Merge: Aligning Position Biases in LLM-based Evaluators\u003cbr\u003eGrasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction\u003cbr\u003eApplying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation\u003cbr\u003eRSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework\u003cbr\u003eCluster-Norm for Unsupervised Probing of Knowledge\u003cbr\u003eToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information\u003cbr\u003e... and 17 more","\u003cb\u003eBias Detection\u003c\u002fb\u003e\u003cbr\u003eTowards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey\u003cbr\u003eCan AI Relate: Testing Large Language Model Response for Mental Health Support\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eDiscovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation\u003cbr\u003eLocating Information Gaps and Narrative Inconsistencies Across Languages: A Case Study of LGBT People Portrayals on Wikipedia\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003eJobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models\u003cbr\u003eLLM Tropes: Revealing Fine-Grained Values and Opinions in Large Language Models\u003cbr\u003eQuantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles\u003cbr\u003eThe Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models\u003cbr\u003e... and 17 more","\u003cb\u003eMultimodal Llms\u003c\u002fb\u003e\u003cbr\u003eLayout-aware GUI Screen Reading with Tree-of-Lens Grounding\u003cbr\u003eMAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering\u003cbr\u003eQuantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective\u003cbr\u003eEFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models\u003cbr\u003eCross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models\u003cbr\u003eTag-grounded Visual Instruction Tuning with Retrieval Augmentation\u003cbr\u003eLOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference\u003cbr\u003eMultimodal Misinformation Detection by Learning from Synthetic Data with Multimodal LLMs\u003cbr\u003eEfficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge\u003cbr\u003eTinyChart: Efficient Chart Understanding with Program-of-Thoughts Learning and Visual Token Merging\u003cbr\u003e... and 17 more","\u003cb\u003eLLM Evaluation\u003c\u002fb\u003e\u003cbr\u003eTowards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey\u003cbr\u003eCan AI Relate: Testing Large Language Model Response for Mental Health Support\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eSedarEval: Automated Evaluation using Self-Adaptive Rubrics\u003cbr\u003eIs this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs\u003cbr\u003ePARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data\u003cbr\u003eA Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations\u003cbr\u003eSplit and Merge: Aligning Position Biases in LLM-based Evaluators\u003cbr\u003ePlausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning\u003cbr\u003eA Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios\u003cbr\u003e... and 17 more","\u003cb\u003eHallucination Detection\u003c\u002fb\u003e\u003cbr\u003eReference-free Hallucination Detection for Large Vision-Language Models\u003cbr\u003eEmbedding and Gradient Say Wrong: A White-Box Method for Hallucination Detection\u003cbr\u003eWhen Context Leads but Parametric Memory Follows in Large Language Models\u003cbr\u003eCliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios\u003cbr\u003eEnhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation\u003cbr\u003ePelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification\u003cbr\u003eCan LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators\u003cbr\u003eAXCEL: Automated eXplainable Consistency Evaluation using LLMS\u003cbr\u003eBLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation\u003cbr\u003eSuccessfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections\u003cbr\u003e... and 16 more","\u003cb\u003eDomain Adaptation\u003c\u002fb\u003e\u003cbr\u003eBeyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs.\u003cbr\u003eDiverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking\u003cbr\u003eRobust AI-Generated Text Detection by Restricted Embeddings\u003cbr\u003eGeneration with Dynamic Vocabulary\u003cbr\u003eTogether We Can: Multilingual Automatic Post-Editing for Low-Resource Languages\u003cbr\u003eMELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science\u003cbr\u003eImproving Referring Ability for Biomedical Language Models\u003cbr\u003eCross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective\u003cbr\u003eUnsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts\u003cbr\u003eTask Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition\u003cbr\u003e... and 16 more","\u003cb\u003eDiversity\u003c\u002fb\u003e\u003cbr\u003eAn Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eAutomatic Instruction Evolving for Large Language Models\u003cbr\u003ePython is Not Always the Best Choice: Embracing Multilingual Program of Thoughts\u003cbr\u003eImproving Diversity of Commonsense Generation by Large Language Models via In-Context Learning\u003cbr\u003eSYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation\u003cbr\u003eEffective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process\u003cbr\u003eMIRRORSTORIES: Reflecting Diversity through Personalized Narrative Generation with Large Language Models\u003cbr\u003eAnnotator-Centric Active Learning for Subjective NLP Tasks\u003cbr\u003eExplaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM\u003cbr\u003e... and 16 more","\u003cb\u003eHallucination Mitigation\u003c\u002fb\u003e\u003cbr\u003ePelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification\u003cbr\u003eMitigating Open-Vocabulary Caption Hallucinations\u003cbr\u003eEFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models\u003cbr\u003eSH2: Self-Highlighted Hesitation Helps You Decode More Truthfully\u003cbr\u003eDial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning\u003cbr\u003eLONGAGENT: Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration\u003cbr\u003eAtomic Self-Consistency for Better Long Form Generations\u003cbr\u003eExplaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective for Molecular Property Prediction\u003cbr\u003eStructured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations\u003cbr\u003eMAVEN-FACT: A Large-scale Event Factuality Detection Dataset\u003cbr\u003e... and 16 more","\u003cb\u003eConsistency\u003c\u002fb\u003e\u003cbr\u003eAn Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification\u003cbr\u003eDEVIL'S ADVOCATE: Anticipatory Reflection for LLM Agents\u003cbr\u003eUnraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness\u003cbr\u003eDecompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison\u003cbr\u003eContext-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eSplit and Merge: Aligning Position Biases in LLM-based Evaluators\u003cbr\u003eConsistent Bidirectional Language Modelling: Expressive Power and Representational Conciseness\u003cbr\u003eSubword Segmentation in LLMs: Looking at Inflection and Consistency\u003cbr\u003eAXCEL: Automated eXplainable Consistency Evaluation using LLMS\u003cbr\u003e... and 16 more","\u003cb\u003eCalibration\u003c\u002fb\u003e\u003cbr\u003eCalibrating Long-form Generations from Large Language Models\u003cbr\u003eTeaching LLMs to Abstain across Languages via Multilingual Feedback\u003cbr\u003eDistance-aware Calibration for Pre-trained Language Models\u003cbr\u003eReconfidencing LLMs from the Grouping Loss Perspective\u003cbr\u003eThe Craft of Selective Prediction: Towards Reliable Case Outcome Classification - An Empirical Study on European Court of Human Rights Cases\u003cbr\u003eEnhancing Healthcare LLM Trust with Atypical Presentations Recalibration\u003cbr\u003eCalibrating Language Models with Adaptive Temperature Scaling\u003cbr\u003eConsolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing\u003cbr\u003eUncertainty Calibration for Tool-Using Language Agents\u003cbr\u003eEnhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding\u003cbr\u003e... and 15 more","\u003cb\u003eBenchmarkss\u003c\u002fb\u003e\u003cbr\u003eFAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition\u003cbr\u003eAcademics Can Contribute to Domain-Specialized Language Models\u003cbr\u003ePerceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models\u003cbr\u003eAUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models\u003cbr\u003eA Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations\u003cbr\u003eThe Effect of Sampling Temperature on Problem Solving in Large Language Models\u003cbr\u003eDistractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation\u003cbr\u003eUnveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models\u003cbr\u003eModeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding\u003cbr\u003eBuilding Resources for Emakhuwa: Machine Translation and News Classification Benchmarks\u003cbr\u003e... and 15 more","\u003cb\u003eSocial Media\u003c\u002fb\u003e\u003cbr\u003eDecoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach\u003cbr\u003eCURE: Context- and Uncertainty-Aware Mental Disorder Detection\u003cbr\u003eA Closer Look at Multidimensional Online Political Incivility\u003cbr\u003eLLMs Cannot (Yet) Match the Specificity and Simplicity of Online Finance Communities in Long Form Question Answering\u003cbr\u003eThe Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse\u003cbr\u003eSPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness\u003cbr\u003eLLM generated responses to mitigate the impact of hate speech\u003cbr\u003eStill Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis\u003cbr\u003eEmotion Granularity from Text: An Aggregate-Level Indicator of Mental Health\u003cbr\u003eMental Disorder Classification via Temporal Representation of Text\u003cbr\u003e... and 14 more","\u003cb\u003ePreference Optimization\u003c\u002fb\u003e\u003cbr\u003eRLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs\u003cbr\u003eWeak-to-Strong Reasoning\u003cbr\u003eAdaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers\u003cbr\u003eModel-based Preference Optimization in Abstractive Summarization without Human Feedback\u003cbr\u003eRethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning\u003cbr\u003eLearning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain\u003cbr\u003eSelf-training Language Models for Arithmetic Reasoning\u003cbr\u003eWPO: Enhancing RLHF with Weighted Preference Optimization\u003cbr\u003eAligning Large Language Models with Diverse Political Viewpoints\u003cbr\u003eAdvancing Large Language Model Attribution through Self-Improving\u003cbr\u003e... and 14 more","\u003cb\u003eLora\u003c\u002fb\u003e\u003cbr\u003eMEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation\u003cbr\u003eTowards One-to-Many Visual Question Answering\u003cbr\u003eInstance-Level Dynamic LoRAs Composition for Cross-Task Generalization\u003cbr\u003eRoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization\u003cbr\u003eDA\u00b3: A Distribution-Aware Adversarial Attack against Language Models\u003cbr\u003eLLOCO: Learning Long Contexts Offline\u003cbr\u003eAdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models\u003cbr\u003ePSC: Extending Context Window of Large Language Models via Phase Shift Calibration\u003cbr\u003eFine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation\u003cbr\u003eMore Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs\u003cbr\u003e... and 14 more","\u003cb\u003eRelation Extraction\u003c\u002fb\u003e\u003cbr\u003eTopic-Oriented Open Relation Extraction with A Priori Seed Generation\u003cbr\u003eAliGATr: Graph-based layout generation for form understanding\u003cbr\u003eMedINST: Meta Dataset of Biomedical Instructions\u003cbr\u003eGrasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction\u003cbr\u003eFAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding\u003cbr\u003eConsistent Document-Level Relation Extraction via Counterfactuals\u003cbr\u003eModeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding\u003cbr\u003eBio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints\u003cbr\u003eVE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models\u003cbr\u003eATAP: Automatic Template-Augmented Commonsense Knowledge Graph Completion via Pre-Trained Language Models\u003cbr\u003e... and 14 more","\u003cb\u003eBert\u003c\u002fb\u003e\u003cbr\u003eNoise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature\u003cbr\u003eCASE: Efficient Curricular Data Pre-training for Building Assistive Psychology Expert Models\u003cbr\u003eRevisiting Supertagging for Faster HPSG Parsing\u003cbr\u003eThe effects of distance on NPI illusive effects in BERT\u003cbr\u003eKAR\u00b3L: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students\u003cbr\u003eAre ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity\u003cbr\u003eMIPD: Exploring Manipulation and Intention in A Novel Corpus of Polish Disinformation\u003cbr\u003eOEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary\u003cbr\u003eNeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries\u003cbr\u003eClass Name Guided Out-of-Scope Intent Classification\u003cbr\u003e... and 14 more","\u003cb\u003eModel Compression\u003c\u002fb\u003e\u003cbr\u003eTransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling\u003cbr\u003eAdaptive Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eOptimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs\u003cbr\u003eDLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model\u003cbr\u003eLaCo: Large Language Model Pruning via Layer Collapse\u003cbr\u003eHead-wise Shareable Attention for Large Language Models\u003cbr\u003eDual-Space Knowledge Distillation for Large Language Models\u003cbr\u003eNormalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003e... and 14 more","\u003cb\u003eSpeech Recognition\u003c\u002fb\u003e\u003cbr\u003eSpeechQE: Estimating the Quality of Direct Speech Translation\u003cbr\u003e950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages\u003cbr\u003eMOSEL: Inference Serving Using Dynamic Modality Selection\u003cbr\u003eSelf-Powered LLM Modality Expansion for Large Speech-Text Models\u003cbr\u003eModeling Gender and Dialect Bias in Automatic Speech Recognition\u003cbr\u003eAre Modern Neural ASR Architectures Robust for Polysynthetic Languages?\u003cbr\u003eScaling Properties of Speech Language Models\u003cbr\u003ePolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition\u003cbr\u003eCasablanca: Data and Models for Multidialectal Arabic Speech Recognition\u003cbr\u003eTask Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition\u003cbr\u003e... and 13 more","\u003cb\u003eGsm8k\u003c\u002fb\u003e\u003cbr\u003eMaking Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eMirror-Consistency: Harnessing Inconsistency in Majority Voting\u003cbr\u003eTask Oriented In-Domain Data Augmentation\u003cbr\u003eWeak-to-Strong Reasoning\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eExploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems\u003cbr\u003eLaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation\u003cbr\u003e... and 13 more","\u003cb\u003eLogical Reasoning\u003c\u002fb\u003e\u003cbr\u003eFOLIO: Natural Language Reasoning with First-Order Logic\u003cbr\u003eWhich Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?\u003cbr\u003eStep-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eLearning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing\u003cbr\u003eExploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems\u003cbr\u003eSymbolic Working Memory Enhances Language Models for Complex Rule Application\u003cbr\u003eRetrieved In-Context Principles from Previous Mistakes\u003cbr\u003eChain of Condition: Construct, Verify and Solve Conditions for Conditional Question Answering\u003cbr\u003eMulti-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models\u003cbr\u003e... and 12 more","\u003cb\u003eReading Comprehension\u003c\u002fb\u003e\u003cbr\u003eFAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition\u003cbr\u003eSCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models\u003cbr\u003eFunctionality learning through specification instructions\u003cbr\u003eLawBench: Benchmarking Legal Knowledge of Large Language Models\u003cbr\u003eMeasuring and Improving Attentiveness to Partial Inputs with Counterfactuals\u003cbr\u003eFine-Grained Prediction of Reading Comprehension from Eye Movements\u003cbr\u003eDistractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003eA Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios\u003cbr\u003e... and 12 more","\u003cb\u003eDeep Learning\u003c\u002fb\u003e\u003cbr\u003eMEANT: Multimodal Encoder for Antecedent Information\u003cbr\u003eSeeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients\u003cbr\u003eTending Towards Stability: Convergence Challenges in Small Language Models\u003cbr\u003eROBERT2VecTM: A Novel Approach for Topic Extraction in Islamic Studies\u003cbr\u003eBi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check\u003cbr\u003eUnifying Multimodal Retrieval via Document Screenshot Embedding\u003cbr\u003eMental Disorder Classification via Temporal Representation of Text\u003cbr\u003eImproving LLM Attributions with Randomized Path-Integration\u003cbr\u003eASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction\u003cbr\u003eThe Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples\u003cbr\u003e... and 12 more","\u003cb\u003eFactuality\u003c\u002fb\u003e\u003cbr\u003eZero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval\u003cbr\u003eModel Editing Harms General Abilities of Large Language Models: Regularization to the Rescue\u003cbr\u003eMulti-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models\u003cbr\u003eKnowTuning: Knowledge-aware Fine-tuning for Large Language Models\u003cbr\u003eEstimating Knowledge in Large Language Models Without Generating a Single Token\u003cbr\u003eExplaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM\u003cbr\u003eReformatted Alignment\u003cbr\u003eEnhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding\u003cbr\u003eFACTALIGN: Long-form Factuality Alignment of Large Language Models\u003cbr\u003eImproving Multi-Agent Debate with Sparse Communication Topology\u003cbr\u003e... and 12 more","\u003cb\u003eText Summarization\u003c\u002fb\u003e\u003cbr\u003eDivide and Conquer: Legal Concept-guided Criminal Court View Generation\u003cbr\u003eStyle-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles\u003cbr\u003eCan We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?\u003cbr\u003eEnhancing Reinforcement Learning with Dense Rewards from Language Model Critic\u003cbr\u003eLaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation\u003cbr\u003eAligning Large Language Models with Diverse Political Viewpoints\u003cbr\u003eFastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models\u003cbr\u003eDisordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts\u003cbr\u003eExploring the Relationship between In-Context Learning and Instruction Tuning\u003cbr\u003eFrom Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression\u003cbr\u003e... and 12 more","\u003cb\u003eKnowledge Graphs\u003c\u002fb\u003e\u003cbr\u003eLLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments\u003cbr\u003eLess is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA\u003cbr\u003eLearning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs\u003cbr\u003eDKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction\u003cbr\u003eWhat Would Happen Next? Predicting Consequences from An Event Causality Graph\u003cbr\u003eGenerate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering\u003cbr\u003eMP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs\u003cbr\u003eEvidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering\u003cbr\u003eXplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs\u003cbr\u003eBio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints\u003cbr\u003e... and 12 more","\u003cb\u003eKnowledge Base\u003c\u002fb\u003e\u003cbr\u003eDo You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation\u003cbr\u003eROCEL: Advancing Table Entity Linking through Distinctive Row and Column Contexts\u003cbr\u003eDynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG\u003cbr\u003eContext-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs\u003cbr\u003eMELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science\u003cbr\u003eKB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases\u003cbr\u003eTriad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering\u003cbr\u003eWhat are the Generator Preferences for End-to-end Task-Oriented Dialog System?\u003cbr\u003eEvOR: Evolving Retrieval for Code Generation\u003cbr\u003eHoneyComb: A Flexible LLM-Based Agent System for Materials Science\u003cbr\u003e... and 11 more","\u003cb\u003eGraph Neural Networks\u003c\u002fb\u003e\u003cbr\u003eOpenGraph: Towards Open Graph Foundation Models\u003cbr\u003eDGLF: A Dual Graph-based Learning Framework for Multi-modal Sarcasm Detection\u003cbr\u003eTowards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering\u003cbr\u003eEnhancing Emotion-Cause Pair Extraction in Conversations via Center Event Detection and Reasoning\u003cbr\u003eBridging Local Details and Global Context in Text-Attributed Graphs\u003cbr\u003eWhat Would Happen Next? Predicting Consequences from An Event Causality Graph\u003cbr\u003eCan Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction\u003cbr\u003eLet's Ask GNN: Empowering Large Language Model for Graph In-Context Learning\u003cbr\u003eExplaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective for Molecular Property Prediction\u003cbr\u003eEnhancing High-order Interaction Awareness in LLM-based Recommender Model\u003cbr\u003e... and 11 more","\u003cb\u003eModel Editing\u003c\u002fb\u003e\u003cbr\u003eTo Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models\u003cbr\u003eModel Editing Harms General Abilities of Large Language Models: Regularization to the Rescue\u003cbr\u003eCross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models\u003cbr\u003eLEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models\u003cbr\u003eLocal Contrastive Editing of Gender Stereotypes\u003cbr\u003eFAME: Towards Factual Multi-Task Model Editing\u003cbr\u003eOn the Robustness of Editing Large Language Models\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003eRebuilding ROME : Resolving Model Collapse during Sequential Model Editing\u003cbr\u003eInfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration\u003cbr\u003e... and 11 more","\u003cb\u003eHate Speech Detection\u003c\u002fb\u003e\u003cbr\u003eDelving into Qualitative Implications of Synthetic Data for Hate Speech Detection\u003cbr\u003eFunctionality learning through specification instructions\u003cbr\u003eUnveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism\u003cbr\u003eComputational Meme Understanding: A Survey\u003cbr\u003eHateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models\u003cbr\u003eCost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation\u003cbr\u003eLLM generated responses to mitigate the impact of hate speech\u003cbr\u003ePREDICT: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection\u003cbr\u003eAnnotator-Centric Active Learning for Subjective NLP Tasks\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003e... and 10 more","\u003cb\u003eSelf-supervised Learning\u003c\u002fb\u003e\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eLearning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training\u003cbr\u003eUnleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training\u003cbr\u003eGenerative Deduplication For Socia Media Data Selection\u003cbr\u003eMolTRES: Improving Chemical Language Representation Learning for Molecular Property Prediction\u003cbr\u003eEmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models\u003cbr\u003eMitigating Matthew Effect: Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation\u003cbr\u003eExploring Question Guidance and Answer Calibration for Visually Grounded Video Question Answering\u003cbr\u003eImproving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach\u003cbr\u003eSelection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability\u003cbr\u003e... and 10 more","\u003cb\u003eArithmetic Reasoning\u003c\u002fb\u003e\u003cbr\u003eLarge Language Models Can Self-Correct with Key Condition Verification\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eEvaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eMirror-Consistency: Harnessing Inconsistency in Majority Voting\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003eEncouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\u003cbr\u003eLayer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models\u003cbr\u003eSelf-training Language Models for Arithmetic Reasoning\u003cbr\u003e... and 10 more","\u003cb\u003eMultimodal\u003c\u002fb\u003e\u003cbr\u003eMMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos\u003cbr\u003eIn-Context Compositional Generalization for Large Vision-Language Models\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eRethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning\u003cbr\u003eSciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading\u003cbr\u003eMemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification\u003cbr\u003eM3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought\u003cbr\u003eYesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models\u003cbr\u003eTV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning\u003cbr\u003eRULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models\u003cbr\u003e... and 10 more","\u003cb\u003eCatastrophic Forgetting\u003c\u002fb\u003e\u003cbr\u003eUnlocking Continual Learning Abilities in Language Models\u003cbr\u003eBreaking the Curse of Multilinguality with Cross-lingual Expert Language Models\u003cbr\u003eMitigating Catastrophic Forgetting in Language Transfer via Model Merging\u003cbr\u003eBreaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale\u003cbr\u003eRevisiting Catastrophic Forgetting in Large Language Model Tuning\u003cbr\u003eLifelong Event Detection via Optimal Transport\u003cbr\u003eLEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models\u003cbr\u003eMore Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs\u003cbr\u003eEmpowering Large Language Model for Continual Video Question Answering with Collaborative Prompting\u003cbr\u003eICL: Iterative Continual Learning for Multi-domain Neural Machine Translation\u003cbr\u003e... and 10 more","\u003cb\u003eVisual Reasoning\u003c\u002fb\u003e\u003cbr\u003eMMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems\u003cbr\u003ePlot Twist: Multimodal Models Don't Comprehend Simple Chart Details\u003cbr\u003eBeyond Embeddings: The Promise of Visual Table in Visual Reasoning\u003cbr\u003eOmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer\u003cbr\u003eFineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension\u003cbr\u003ePelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification\u003cbr\u003eKnowledge-Aware Reasoning over Multimodal Semi-structured Tables\u003cbr\u003eThe Instinctive Bias: Spurious Images lead to Illusion in MLLMs\u003cbr\u003eVDebugger: Harnessing Execution Feedback for Debugging Visual Programs\u003cbr\u003eLarge Language Models Are Challenged by Habitat-Centered Reasoning\u003cbr\u003e... and 10 more","\u003cb\u003eSafety\u003c\u002fb\u003e\u003cbr\u003eMerge to Learn: Efficiently Adding Skills to Language Models with Model Merging\u003cbr\u003eKidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions\u003cbr\u003eMulti-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models\u003cbr\u003eHouseholder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective\u003cbr\u003eCMD: a framework for Context-aware Model self-Detoxification\u003cbr\u003ePURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness\u003cbr\u003eSafety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations\u003cbr\u003eThe Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm\u003cbr\u003eVLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment\u003cbr\u003eCantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues\u003cbr\u003e... and 10 more","\u003cb\u003eSupervised Fine-tuning\u003c\u002fb\u003e\u003cbr\u003eSynthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models\u003cbr\u003eAre LLMs Good Annotators for Discourse-level Event Relation Extraction?\u003cbr\u003eARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback\u003cbr\u003eSaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales\u003cbr\u003eSelf-training Language Models for Arithmetic Reasoning\u003cbr\u003eLIONS: An Empirically Optimized Approach to Align Language Models\u003cbr\u003eA Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences\u003cbr\u003eHow to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective\u003cbr\u003eInstruction Fine-Tuning: Does Prompt Loss Matter?\u003cbr\u003eORPO: Monolithic Preference Optimization without Reference Model\u003cbr\u003e... and 10 more","\u003cb\u003ePreference Learning\u003c\u002fb\u003e\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eEnhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing\u003cbr\u003eTowards Tool Use Alignment of Large Language Models\u003cbr\u003eNot All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning\u003cbr\u003eStep-level Value Preference Optimization for Mathematical Reasoning\u003cbr\u003eWhat are the Generator Preferences for End-to-end Task-Oriented Dialog System?\u003cbr\u003eSELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards\u003cbr\u003eInterpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts\u003cbr\u003eUnsupervised Human Preference Learning\u003cbr\u003eVLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment\u003cbr\u003e... and 9 more","\u003cb\u003eRag\u003c\u002fb\u003e\u003cbr\u003eDo You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation\u003cbr\u003eUnified Active Retrieval for Retrieval Augmented Generation\u003cbr\u003eSATYRN: A Platform for Analytics Augmented Generation\u003cbr\u003eInvestigating LLMs as Voting Assistants via Contextual Augmentation: A Case Study on the European Parliament Elections 2024\u003cbr\u003eLess is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA\u003cbr\u003eGlue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models\u003cbr\u003eImprove Dense Passage Retrieval with Entailment Tuning\u003cbr\u003eEvOR: Evolving Retrieval for Code Generation\u003cbr\u003eFastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models\u003cbr\u003eQuBE: Question-based Belief Enhancement for Agentic LLM Reasoning\u003cbr\u003e... and 9 more","\u003cb\u003eQuantization\u003c\u002fb\u003e\u003cbr\u003eMobileQuant: Mobile-friendly Quantization for On-device Language Models\u003cbr\u003eRoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization\u003cbr\u003eKV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches\u003cbr\u003eOptimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs\u003cbr\u003eDLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model\u003cbr\u003ePrefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization\u003cbr\u003eHow Does Quantization Affect Multilingual LLMs?\u003cbr\u003eATQ: Activation Transformation for Weight-Activation Quantization of Large Language Models\u003cbr\u003eApiQ: Finetuning of 2-Bit Quantized Large Language Model\u003cbr\u003eQEFT: Quantization for Efficient Fine-Tuning of LLMs\u003cbr\u003e... and 9 more","\u003cb\u003eLLM Alignment\u003c\u002fb\u003e\u003cbr\u003eReward Difference Optimization For Sample Reweighting In Offline RLHF\u003cbr\u003eGlobal Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents\u003cbr\u003ePURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness\u003cbr\u003eInterpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts\u003cbr\u003eSELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards\u003cbr\u003eModel Merging and Safety Alignment: One Bad Model Spoils the Bunch\u003cbr\u003eContrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion\u003cbr\u003eOn the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization\u003cbr\u003eChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline\u003cbr\u003eAligners: Decoupling LLMs and Alignment\u003cbr\u003e... and 9 more","\u003cb\u003eUnsupervised Learning\u003c\u002fb\u003e\u003cbr\u003eLaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning\u003cbr\u003eCOMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities\u003cbr\u003eRe-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval\u003cbr\u003eCluster-Norm for Unsupervised Probing of Knowledge\u003cbr\u003eUnsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts\u003cbr\u003eUnsupervised Human Preference Learning\u003cbr\u003eClass Name Guided Out-of-Scope Intent Classification\u003cbr\u003eToeing the Party Line: Election Manifestos as a Key to Understand Political Discourse on Twitter\u003cbr\u003eUnsupervised Extraction of Dialogue Policies from Conversations\u003cbr\u003eA Simple Angle-based Approach for Contrastive Learning of Unsupervised Sentence Representation\u003cbr\u003e... and 9 more","\u003cb\u003eTokenization\u003c\u002fb\u003e\u003cbr\u003eBPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training\u003cbr\u003eUnsupervised Discrete Representations of American Sign Language\u003cbr\u003eApplying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation\u003cbr\u003eTokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR\u003cbr\u003eLexically Grounded Subword Segmentation\u003cbr\u003eFishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models\u003cbr\u003eTokenization Falling Short: On Subword Robustness in Large Language Models\u003cbr\u003eAn Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference\u003cbr\u003eScaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia\u003cbr\u003eExploring Design Choices for Building Language-Specific LLMs\u003cbr\u003e... and 8 more","\u003cb\u003eKnowledge Graph\u003c\u002fb\u003e\u003cbr\u003eUnsupervised Named Entity Disambiguation for Low Resource Domains\u003cbr\u003eCNEQ: Incorporating numbers into Knowledge Graph Reasoning\u003cbr\u003eMedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures\u003cbr\u003eStorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for Children's Story-Based Learning\u003cbr\u003eR\u00b3-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL\u003cbr\u003eA Usage-centric Take on Intent Understanding in E-Commerce\u003cbr\u003eTKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs\u003cbr\u003eTRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation\u003cbr\u003eMitigating Hallucination in Fictional Character Role-Play\u003cbr\u003eDALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature\u003cbr\u003e... and 8 more","\u003cb\u003eTranslation\u003c\u002fb\u003e\u003cbr\u003eA Thorough Examination of Decoding Methods in the Era of LLMs\u003cbr\u003eMitigating the Alignment Tax of RLHF\u003cbr\u003eAC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models\u003cbr\u003eCreative and Context-Aware Translation of East Asian Idioms with GPT-4\u003cbr\u003eA Recipe to Train Powerful Romanian LLMs with English Instructions\u003cbr\u003eCost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS\u003cbr\u003eLayer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models\u003cbr\u003eGeneralists vs. Specialists: Evaluating Large Language Models for Urdu\u003cbr\u003eHop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models\u003cbr\u003eQuantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval\u003cbr\u003e... and 8 more","\u003cb\u003eRlhf\u003c\u002fb\u003e\u003cbr\u003eRethinking the Role of Proxy Rewards in Language Model Alignment\u003cbr\u003eMitigating the Alignment Tax of RLHF\u003cbr\u003eRLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs\u003cbr\u003eFlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003eFiltered Direct Preference Optimization\u003cbr\u003eA Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors\u003cbr\u003eWPO: Enhancing RLHF with Weighted Preference Optimization\u003cbr\u003eGDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets\u003cbr\u003eNot All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning\u003cbr\u003e... and 8 more","\u003cb\u003eSynthetic Data Generation\u003c\u002fb\u003e\u003cbr\u003eSynthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models\u003cbr\u003eBetter Alignment with Instruction Back-and-Forth Translation\u003cbr\u003eSuccessfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections\u003cbr\u003eGOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory\u003cbr\u003ePedagogical Alignment of Large Language Models\u003cbr\u003eUnleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting\u003cbr\u003eAligners: Decoupling LLMs and Alignment\u003cbr\u003eLink, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval\u003cbr\u003eA Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners\u003cbr\u003eSYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists\u003cbr\u003e... and 8 more","\u003cb\u003eAnnotation\u003c\u002fb\u003e\u003cbr\u003eAre LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues\u003cbr\u003eNoise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature\u003cbr\u003eLet's discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment\u003cbr\u003eCEAMC: Corpus and Empirical Study of Argument Analysis in Education via LLMs\u003cbr\u003eCERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays\u003cbr\u003eDesigning Logic Pattern Templates for Counter-Argument Logical Structure Analysis\u003cbr\u003eWhat's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs\u003cbr\u003eFlee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling\u003cbr\u003eFactcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers\u003cbr\u003eThe Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse\u003cbr\u003e... and 8 more","\u003cb\u003eKnowledge Editing\u003c\u002fb\u003e\u003cbr\u003eInterpretability-based Tailored Knowledge Editing in Transformers\u003cbr\u003eLLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eEditing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models\u003cbr\u003eRebuilding ROME : Resolving Model Collapse during Sequential Model Editing\u003cbr\u003eCommonsense Knowledge Editing Based on Free-Text in LLMs\u003cbr\u003e\u003eWhy Does New Knowledge Create Messy Ripple Effects in LLMs?\u003cbr\u003eAKEW: Assessing Knowledge Editing in the Wild\u003cbr\u003eUpdating Large Language Models' Memories with Time Constraints\u003cbr\u003eDyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs\u003cbr\u003e... and 8 more","\u003cb\u003eLow-rank Adaptation\u003c\u002fb\u003e\u003cbr\u003eAdvancing Vision-Language Models with Adapter Ensemble Strategies\u003cbr\u003eMixture-of-Subspaces in Low-Rank Adaptation\u003cbr\u003eLoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eRepMatch: Quantifying Cross-Instance Similarities in Representation Space\u003cbr\u003eExploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation\u003cbr\u003eOn Mitigating Performance Disparities in Multilingual Speech Recognition\u003cbr\u003eIntroducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly\u003cbr\u003eFast Forwarding Low-Rank Training\u003cbr\u003eLoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation\u003cbr\u003e... and 8 more","\u003cb\u003eGeneration\u003c\u002fb\u003e\u003cbr\u003eAre LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues\u003cbr\u003eFrom Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items\u003cbr\u003eSafely Learning with Private Data: A Federated Learning Framework for Large Language Model\u003cbr\u003eCOGEN: Learning from Feedback with Coupled Comprehension and Generation\u003cbr\u003eHiFT: A Hierarchical Full Parameter Fine-Tuning Strategy\u003cbr\u003eGeneralists vs. Specialists: Evaluating Large Language Models for Urdu\u003cbr\u003eUnveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism\u003cbr\u003eFuseGen: PLM Fusion for Data-generation based Zero-shot Learning\u003cbr\u003eVGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation\u003cbr\u003eWho is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models\u003cbr\u003e... and 8 more","\u003cb\u003eVideo Understanding\u003c\u002fb\u003e\u003cbr\u003eEliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties\u003cbr\u003ePersonalized Video Comment Generation\u003cbr\u003eOmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer\u003cbr\u003eMatchTime: Towards Automatic Soccer Game Commentary Generation\u003cbr\u003eCan CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP\u003cbr\u003eA Simple LLM Framework for Long-Range Video Question-Answering\u003cbr\u003eVideo-LLaVA: Learning United Visual Representation by Alignment Before Projection\u003cbr\u003eVideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models\u003cbr\u003eGRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization\u003cbr\u003eEncoding and Controlling Global Semantics for Long-form Video Question Answering\u003cbr\u003e... and 8 more","\u003cb\u003ePrivacy\u003c\u002fb\u003e\u003cbr\u003eTowards Robust Evaluation of Unlearning in LLMs via Data Transformations\u003cbr\u003eSafely Learning with Private Data: A Federated Learning Framework for Large Language Model\u003cbr\u003eMedAdapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning\u003cbr\u003eCross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models\u003cbr\u003eSeeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients\u003cbr\u003eGranular Privacy Control for Geolocation with Vision Language Models\u003cbr\u003eGeneralizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4\u003cbr\u003eRECALL: Membership Inference via Relative Conditional Log-Likelihoods\u003cbr\u003eRevisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective\u003cbr\u003eTo Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models\u003cbr\u003e... and 8 more","\u003cb\u003eAttention Mechanism\u003c\u002fb\u003e\u003cbr\u003eAll You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification\u003cbr\u003eDKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction\u003cbr\u003eVarying Sentence Representations via Condition-Specified Routers\u003cbr\u003eExternal Knowledge-Driven Argument Mining: Leveraging Attention-Enhanced Multi-Network Models\u003cbr\u003eROCEL: Advancing Table Entity Linking through Distinctive Row and Column Contexts\u003cbr\u003eTemporal Fact Reasoning over Hyper-Relational Knowledge Graphs\u003cbr\u003eQuery-based Cross-Modal Projector Bolstering Mamba Multimodal LLM\u003cbr\u003eDAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination\u003cbr\u003eStanceformer: Target-Aware Transformer for Stance Detection\u003cbr\u003eContextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation\u003cbr\u003e... and 8 more","\u003cb\u003eDialogue Generation\u003c\u002fb\u003e\u003cbr\u003eHow to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective\u003cbr\u003eSMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support\u003cbr\u003eMixed-Session Conversation with Egocentric Memory\u003cbr\u003eLeveraging Large Language Models for NLG Evaluation: Advances and Challenges\u003cbr\u003eTowards Aligning Language Models with Textual Feedback\u003cbr\u003eSecuring Multi-turn Conversational Language Models From Distributed Backdoor Triggers\u003cbr\u003eOntologically Faithful Generation of Non-Player Character Dialogues\u003cbr\u003eHOLLMWOOD: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing\u003cbr\u003eGDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003e... and 8 more","\u003cb\u003eError Analysis\u003c\u002fb\u003e\u003cbr\u003eA linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks\u003cbr\u003eMM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification\u003cbr\u003eEvaluation of Question Answer Generation for Portuguese: Insights and Datasets\u003cbr\u003ePPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion\u003cbr\u003eTo Err Is Human, but Llamas Can Learn It Too\u003cbr\u003eNOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition\u003cbr\u003eLiar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models\u003cbr\u003eRevisiting Automated Evaluation for Long-form Table Question Answering\u003cbr\u003eError Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation\u003cbr\u003eDISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers\u003cbr\u003e... and 8 more","\u003cb\u003eDatasets\u003c\u002fb\u003e\u003cbr\u003eAll You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification\u003cbr\u003eMEANT: Multimodal Encoder for Antecedent Information\u003cbr\u003eLet's discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment\u003cbr\u003eEvaluation of Question Answer Generation for Portuguese: Insights and Datasets\u003cbr\u003eDatasets for Multilingual Answer Sentence Selection\u003cbr\u003eText2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback\u003cbr\u003ePuzzle Solving using Reasoning of Large Language Models: A Survey\u003cbr\u003eAutomated Essay Scoring: A Reflection on the State of the Art\u003cbr\u003eMAIR: A Massive Benchmark for Evaluating Instructed Retrieval\u003cbr\u003eBASES: Large-scale Web Search User Simulation with Large Language Model based Agents\u003cbr\u003e... and 8 more","\u003cb\u003eAdversarial Attacks\u003c\u002fb\u003e\u003cbr\u003eCROWD: Certified Robustness via Weight Distribution for Smoothed Classifiers against Backdoor Attack\u003cbr\u003eTowards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis\u003cbr\u003eAdvancing Adversarial Suffix Transfer Learning on Aligned Large Language Models\u003cbr\u003eAdversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation\u003cbr\u003eFrom LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking\u003cbr\u003eAuthorship Obfuscation in Multilingual Machine-Generated Text Detection\u003cbr\u003eThinking Outside of the Differential Privacy Box: A Case Study in Text Privatization with Language Model Prompting\u003cbr\u003ePrompts have evil twins\u003cbr\u003eDetecting Machine-Generated Long-Form Content with Latent-Space Variables\u003cbr\u003eIM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method\u003cbr\u003e... and 7 more","\u003cb\u003eLanguage Understanding\u003c\u002fb\u003e\u003cbr\u003eConnecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game\u003cbr\u003eMalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language\u003cbr\u003eAC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models\u003cbr\u003eChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline\u003cbr\u003eMoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition\u003cbr\u003eLeveraging Grammar Induction for Language Understanding and Generation\u003cbr\u003eThe Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance\u003cbr\u003eLarge Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark\u003cbr\u003eLLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks\u003cbr\u003eEnhancing Agent Learning through World Dynamics Modeling\u003cbr\u003e... and 7 more","\u003cb\u003eUncertainty\u003c\u002fb\u003e\u003cbr\u003eThe Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning\u003cbr\u003eDoes Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?\u003cbr\u003eOn the Rigour of Scientific Writing: Criteria, Analysis, and Insights\u003cbr\u003eQUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios\u003cbr\u003eUncertainty Calibration for Tool-Using Language Agents\u003cbr\u003eUncertainty in Language Models: Assessment through Rank-Calibration\u003cbr\u003eWhispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models\u003cbr\u003eCURE: Context- and Uncertainty-Aware Mental Disorder Detection\u003cbr\u003eCan Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?\u003cbr\u003eCalibrating the Confidence of Large Language Models by Eliciting Fidelity\u003cbr\u003e... and 7 more","\u003cb\u003eReward Model\u003c\u002fb\u003e\u003cbr\u003eReward Difference Optimization For Sample Reweighting In Offline RLHF\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003eFiltered Direct Preference Optimization\u003cbr\u003eWeak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems\u003cbr\u003eA Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors\u003cbr\u003eSemi-Supervised Reward Modeling via Iterative Self-Training\u003cbr\u003eTS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models\u003cbr\u003eImproving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning\u003cbr\u003eDon't Forget Your Reward Values: Language Model Alignment via Value-based Calibration\u003cbr\u003eThe Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models\u003cbr\u003e... and 7 more","\u003cb\u003eGender Bias\u003c\u002fb\u003e\u003cbr\u003eModeling Gender and Dialect Bias in Automatic Speech Recognition\u003cbr\u003eApplying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation\u003cbr\u003eFrom Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment\u003cbr\u003eEvaluating Gender Bias of LLMs in Making Morality Judgements\u003cbr\u003eOn the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models\u003cbr\u003eWhat the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study\u003cbr\u003eGender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts\u003cbr\u003eTWBias: A Benchmark for Assessing Social Bias in Traditional Chinese Large Language Models through a Taiwan Cultural Lens\u003cbr\u003eImages Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective\u003cbr\u003eLocal Contrastive Editing of Gender Stereotypes\u003cbr\u003e... and 7 more","\u003cb\u003ePlanning\u003c\u002fb\u003e\u003cbr\u003eMSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making\u003cbr\u003eAn Evaluation Mechanism of LLM-based Agents on Manipulating APIs\u003cbr\u003eMathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models\u003cbr\u003eCAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans\u003cbr\u003eUnlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models\u003cbr\u003eMAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration\u003cbr\u003eExplaining Mixtures of Sources in News Articles\u003cbr\u003eExperience as Source for Anticipation and Planning: Experiential Policy Learning for Target-driven Recommendation Dialogues\u003cbr\u003eDivide-or-Conquer? Which Part Should You Distill Your LLM?\u003cbr\u003eOn the Empirical Complexity of Reasoning and Planning in LLMs\u003cbr\u003e... and 7 more","\u003cb\u003eMath\u003c\u002fb\u003e\u003cbr\u003eExploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eNash CoT: Multi-Path Inference with Preference Equilibrium\u003cbr\u003ePython is Not Always the Best Choice: Embracing Multilingual Program of Thoughts\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003eSCIAGENT: Tool-augmented Language Models for Scientific Reasoning\u003cbr\u003eReasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\u003cbr\u003eTask Oriented In-Domain Data Augmentation\u003cbr\u003e... and 7 more","\u003cb\u003eRetrieval\u003c\u002fb\u003e\u003cbr\u003eKAR\u00b3L: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students\u003cbr\u003eR\u00b3-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL\u003cbr\u003eHow Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?\u003cbr\u003eChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering\u003cbr\u003eRetrieving Contextual Information for Long-Form Question Answering using Weak Supervision\u003cbr\u003eLost in Translation: Chemical Language Models and the Misunderstanding of Molecule Structures\u003cbr\u003eFrom RAG to RICHES: Retrieval Interlaced with Sequence Generation\u003cbr\u003eQuantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval\u003cbr\u003eIFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning\u003cbr\u003eAnalysis of Plan-based Retrieval for Grounded Text Generation\u003cbr\u003e... and 7 more","\u003cb\u003eAdversarial Attack\u003c\u002fb\u003e\u003cbr\u003eRanking Manipulation for Conversational Search Engines\u003cbr\u003eEnhancing Learning-Based Binary Code Similarity Detection Model through Adversarial Training with Multiple Function Variants\u003cbr\u003eBaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting\u003cbr\u003eRAFT: Realistic Attacks to Fool Text Detectors\u003cbr\u003eRAt: Injecting Implicit Bias for Text-To-Image Prompt Refinement Models\u003cbr\u003eAdaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks\u003cbr\u003eMuting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models\u003cbr\u003eAttacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions\u003cbr\u003eIs LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment\u003cbr\u003eDA\u00b3: A Distribution-Aware Adversarial Attack against Language Models\u003cbr\u003e... and 7 more","\u003cb\u003eRepresentation Learning\u003c\u002fb\u003e\u003cbr\u003eHyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs\u003cbr\u003eBridging Local Details and Global Context in Text-Attributed Graphs\u003cbr\u003eAre ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity\u003cbr\u003eGender Identity in Pretrained Language Models: An Inclusive Approach to Data Creation and Probing\u003cbr\u003eA Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives\u003cbr\u003eTrain Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003eAutomated Tone Transcription and Clustering with Tone2Vec\u003cbr\u003eIf CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions\u003cbr\u003ePredicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement\u003cbr\u003e... and 7 more","\u003cb\u003eError Correction\u003c\u002fb\u003e\u003cbr\u003eC-LLM: Learn to Check Chinese Spelling Errors Character by Character\u003cbr\u003eCorrect after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method\u003cbr\u003eRetrieved In-Context Principles from Previous Mistakes\u003cbr\u003eARM: An Alignment-and-Replacement Module for Chinese Spelling Check Based on LLMs\u003cbr\u003eRepairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models\u003cbr\u003eEHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records\u003cbr\u003eBi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check\u003cbr\u003eE2CL: Exploration-based Error Correction Learning for Embodied Agents\u003cbr\u003eStraGo: Harnessing Strategic Guidance for Prompt Optimization\u003cbr\u003eCOFFEE-GYM: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code\u003cbr\u003e... and 7 more","\u003cb\u003eContinual Learning\u003c\u002fb\u003e\u003cbr\u003eICL: Iterative Continual Learning for Multi-domain Neural Machine Translation\u003cbr\u003eMerge to Learn: Efficiently Adding Skills to Language Models with Model Merging\u003cbr\u003eUnlocking Continual Learning Abilities in Language Models\u003cbr\u003eLifelong Event Detection via Optimal Transport\u003cbr\u003eCOGEN: Learning from Feedback with Coupled Comprehension and Generation\u003cbr\u003eGradient Localization Improves Lifelong Pretraining of Language Models\u003cbr\u003eMitigating Catastrophic Forgetting in Language Transfer via Model Merging\u003cbr\u003eA Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models\u003cbr\u003eSEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models\u003cbr\u003eExplicit Memory Learning with Expectation Maximization\u003cbr\u003e... and 7 more","\u003cb\u003ePrompt Optimization\u003c\u002fb\u003e\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eMETAREFLECTION: Learning Instructions for Language Agents using Past Reflections\u003cbr\u003eOptimizing Instructions and Demonstrations for Multi-Stage Language Model Programs\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eStraGo: Harnessing Strategic Guidance for Prompt Optimization\u003cbr\u003eLLM as a metric critic for low resource relation identification\u003cbr\u003eFewer is More: Boosting Math Reasoning with Reinforced Context Pruning\u003cbr\u003eAMPO: Automatic Multi-Branched Prompt Optimization\u003cbr\u003eMedical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?\u003cbr\u003eDynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models\u003cbr\u003e... and 6 more","\u003cb\u003eRanking\u003c\u002fb\u003e\u003cbr\u003eR\u00b3-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL\u003cbr\u003eEnhancing Alignment using Curriculum Learning & Ranked Preferences\u003cbr\u003eMake Large Language Model a Better Ranker\u003cbr\u003eTROTR: A Framework for Evaluating the Recontextualization of Text\u003cbr\u003eVideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models\u003cbr\u003eNumbers Matter! Bringing Quantity-awareness to Retrieval Systems\u003cbr\u003eClustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation\u003cbr\u003eHow Does the Disclosure of AI Assistance Affect the Perceptions of Writing?\u003cbr\u003eAGRAME: Any-Granularity Ranking with Multi-Vector Embeddings\u003cbr\u003eExploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank\u003cbr\u003e... and 6 more","\u003cb\u003eClustering\u003c\u002fb\u003e\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eUnsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance\u003cbr\u003eStory Morals: Surfacing value-driven narrative schemas using large language models\u003cbr\u003eDynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG\u003cbr\u003eDEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing\u003cbr\u003eFASTTRACK: Reliable Fact Tracing via Clustering and LLM-Powered Evidence Validation\u003cbr\u003eLeave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA\u003cbr\u003eClustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation\u003cbr\u003eA Generic Method for Fine-grained Category Discovery in Natural Language Texts\u003cbr\u003ePseudo-Label Enhanced Prototypical Contrastive Learning for Uniformed Intent Discovery\u003cbr\u003e... and 6 more","\u003cb\u003eMixture Of Experts\u003c\u002fb\u003e\u003cbr\u003eBiMediX: Bilingual Medical Mixture of Experts LLM\u003cbr\u003eEfficient and Interpretable Grammatical Error Correction with Mixture of Experts\u003cbr\u003eMODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning\u003cbr\u003eLet the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models\u003cbr\u003eIntegrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification\u003cbr\u003eMoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition\u003cbr\u003eMedCoT: Medical Chain of Thought via Hierarchical Expert\u003cbr\u003eScaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models\u003cbr\u003eFEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models\u003cbr\u003eAdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models\u003cbr\u003e... and 6 more","\u003cb\u003eLong Context\u003c\u002fb\u003e\u003cbr\u003eLLOCO: Learning Long Contexts Offline\u003cbr\u003eCOMPACT: Compressing Retrieved Documents Actively for Question Answering\u003cbr\u003eInfiniPot: Infinite Context Processing on Memory-Constrained LLMs\u003cbr\u003eSummary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems\u003cbr\u003eA Simple and Effective L2 Norm-Based Strategy for KV Cache Compression\u003cbr\u003eEigen Attention: Attention in Low-Rank Space for KV Cache Compression\u003cbr\u003eKV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches\u003cbr\u003eMemorize Step by Step: Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk\u003cbr\u003eLongAlign: A Recipe for Long Context Alignment of Large Language Models\u003cbr\u003eLONGEMBED: Extending Embedding Models for Long Context Retrieval\u003cbr\u003e... and 6 more","\u003cb\u003eClip\u003c\u002fb\u003e\u003cbr\u003eEFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models\u003cbr\u003eInvestigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models\u003cbr\u003eCan CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP\u003cbr\u003eTROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning\u003cbr\u003eVLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models\u003cbr\u003eVision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification\u003cbr\u003eUpdating CLIP to Prefer Descriptions Over Captions\u003cbr\u003eVPL: Visual Proxy Learning Framework for Zero-Shot Medical Image Diagnosis\u003cbr\u003eAfrican or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification\u003cbr\u003eRWKV-CLIP: A Robust Vision-Language Representation Learner\u003cbr\u003e... and 6 more","\u003cb\u003eToxicity\u003c\u002fb\u003e\u003cbr\u003eAttribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification\u003cbr\u003eStyle-Shifting Behaviour of the Manosphere on Reddit\u003cbr\u003ePromoting Constructive Deliberation: Reframing for Receptiveness\u003cbr\u003ePreference Tuning For Toxicity Mitigation Generalizes Across Languages\u003cbr\u003eIntrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis\u003cbr\u003eLLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives\u003cbr\u003eEvaluating Psychological Safety of Large Language Models\u003cbr\u003eBeyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression\u003cbr\u003eData, Data Everywhere: A Guide for Pretraining Dataset Construction\u003cbr\u003eThe Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis\u003cbr\u003e... and 6 more","\u003cb\u003eAutomatic Speech Recognition\u003c\u002fb\u003e\u003cbr\u003eModeling Gender and Dialect Bias in Automatic Speech Recognition\u003cbr\u003eWavLLM: Towards Robust and Adaptive Speech Large Language Model\u003cbr\u003eMuting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models\u003cbr\u003eUnveiling the Role of Pretraining in Direct Speech Translation\u003cbr\u003eOn Mitigating Performance Disparities in Multilingual Speech Recognition\u003cbr\u003eOptimized Speculative Sampling for GPU Hardware Accelerators\u003cbr\u003eInterventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding\u003cbr\u003eExploring the Potential of Multimodal LLM with Knowledge-Intensive Multimodal ASR\u003cbr\u003eAre Modern Neural ASR Architectures Robust for Polysynthetic Languages?\u003cbr\u003eFast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper\u003cbr\u003e... and 6 more","\u003cb\u003eSecurity\u003c\u002fb\u003e\u003cbr\u003eBaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting\u003cbr\u003eSecureSQL: Evaluating Data Leakage of Large Language Models as Natural Language Interfaces to Databases\u003cbr\u003eJailbreaking LLMs with Arabic Transliteration and Arabizi\u003cbr\u003eSafely Learning with Private Data: A Federated Learning Framework for Large Language Model\u003cbr\u003eGPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation\u003cbr\u003eWhere Am I From? Identifying Origin of LLM-generated Content\u003cbr\u003eSecuring Multi-turn Conversational Language Models From Distributed Backdoor Triggers\u003cbr\u003eFishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models\u003cbr\u003eCross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models\u003cbr\u003eFrom LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking\u003cbr\u003e... and 6 more","\u003cb\u003eBenchmarks Dataset\u003c\u002fb\u003e\u003cbr\u003eStill Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis\u003cbr\u003eRevealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues\u003cbr\u003eDECOR: Improving Coherence in L2 English Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting\u003cbr\u003eBOOLQUESTIONS: Does Dense Retrieval Understand Boolean Logic in Language?\u003cbr\u003eUnlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering\u003cbr\u003eText-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction\u003cbr\u003eHolistic Evaluation for Interleaved Text-and-Image Generation\u003cbr\u003eTowards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights\u003cbr\u003eEditing Conceptual Knowledge for Large Language Models\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003e... and 6 more","\u003cb\u003eActive Learning\u003c\u002fb\u003e\u003cbr\u003eEffective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process\u003cbr\u003eALVIN: Active Learning Via INterpolation\u003cbr\u003eOn the Fragility of Active Learners for Text Classification\u003cbr\u003eAnnotator-Centric Active Learning for Subjective NLP Tasks\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003eCOMPACT: Compressing Retrieved Documents Actively for Question Answering\u003cbr\u003eMulti-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation\u003cbr\u003eSelf-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models\u003cbr\u003eEfficient Active Learning with Adapters\u003cbr\u003eDISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers\u003cbr\u003e... and 5 more","\u003cb\u003eFactual Knowledge\u003c\u002fb\u003e\u003cbr\u003eOn the Robustness of Editing Large Language Models\u003cbr\u003eA Thorough Examination of Decoding Methods in the Era of LLMs\u003cbr\u003eFAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition\u003cbr\u003eCross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003eDoes Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?\u003cbr\u003e\u003eWhy Does New Knowledge Create Messy Ripple Effects in LLMs?\u003cbr\u003eSH2: Self-Highlighted Hesitation Helps You Decode More Truthfully\u003cbr\u003eLLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement\u003cbr\u003eDyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs\u003cbr\u003e... and 5 more","\u003cb\u003eTopic Modeling\u003c\u002fb\u003e\u003cbr\u003eROBERT2VecTM: A Novel Approach for Topic Extraction in Islamic Studies\u003cbr\u003ePhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study\u003cbr\u003eTopic Modeling: Contextual Token Embeddings Are All You Need\u003cbr\u003eNoise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature\u003cbr\u003eReap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora\u003cbr\u003eUnsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance\u003cbr\u003eEnhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs\u003cbr\u003eKnowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature\u003cbr\u003eCharacterizing Text Datasets with Psycholinguistic Features\u003cbr\u003eNeuroMax: Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization\u003cbr\u003e... and 5 more","\u003cb\u003eEmotion Recognition\u003c\u002fb\u003e\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eWavLLM: Towards Robust and Adaptive Speech Large Language Model\u003cbr\u003eEmotion Granularity from Text: An Aggregate-Level Indicator of Mental Health\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003eVisual Prompting in LLMs for Enhancing Emotion Recognition\u003cbr\u003eEmosical: An Emotion-Annotated Musical Theatre Dataset\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eDetectiveNN: Imitating Human Emotional Reasoning with a Recall-Detect-Predict Framework for Emotion Recognition in Conversations\u003cbr\u003ePALM: Few-Shot Prompt Learning for Audio Language Models\u003cbr\u003eWorry Words: Norms of Anxiety Association for over 44k English Words\u003cbr\u003e... and 5 more","\u003cb\u003eExplainable Ai\u003c\u002fb\u003e\u003cbr\u003eRationalizing Transformer Predictions via End-To-End Differentiable Self-Training\u003cbr\u003eFool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting\u003cbr\u003eTV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning\u003cbr\u003eEvaluating Readability and Faithfulness of Concept-based Explanations\u003cbr\u003eCoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems\u003cbr\u003eFaithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach\u003cbr\u003eThe Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems\u003cbr\u003eAdaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse\u003cbr\u003eImproving LLM Attributions with Randomized Path-Integration\u003cbr\u003eMultilingual Fine-Grained News Headline Hallucination Detection\u003cbr\u003e... and 5 more","\u003cb\u003eMulti-modal Learning\u003c\u002fb\u003e\u003cbr\u003eCreative Problem Solving in Large Language and Vision Models \u2013 What Would it Take?\u003cbr\u003eInfrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models\u003cbr\u003eMatchTime: Towards Automatic Soccer Game Commentary Generation\u003cbr\u003eTowards One-to-Many Visual Question Answering\u003cbr\u003eFEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models\u003cbr\u003eGeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation\u003cbr\u003eMOSEL: Inference Serving Using Dynamic Modality Selection\u003cbr\u003eMulti-Level Information Retrieval Augmented Generation for Knowledge-based Visual Question Answering\u003cbr\u003eSaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering\u003cbr\u003eBeyond Embeddings: The Promise of Visual Table in Visual Reasoning\u003cbr\u003e... and 5 more","\u003cb\u003eML\u003c\u002fb\u003e\u003cbr\u003eTowards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs\u003cbr\u003eC3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits\u003cbr\u003eWhen Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?\u003cbr\u003eVerba volant, scripta volant? Don't worry! There are computational solutions for protoword reconstruction\u003cbr\u003eDA-Code: Agent Data Science Code Generation Benchmark for Large Language Models\u003cbr\u003eOn the Rigour of Scientific Writing: Criteria, Analysis, and Insights\u003cbr\u003eLarge Language Models for Data Annotation and Synthesis: A Survey\u003cbr\u003eUnderstanding \u201cDemocratization\u201d in NLP and ML Research\u003cbr\u003eForecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003e... and 5 more","\u003cb\u003eOpen-domain QA\u003c\u002fb\u003e\u003cbr\u003eLarge Language Models Can Self-Correct with Key Condition Verification\u003cbr\u003eRaFe: Ranking Feedback Improves Query Rewriting for RAG\u003cbr\u003eChain-of-Rewrite: Aligning Question and Documents for Open-Domain Question Answering\u003cbr\u003eBlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering\u003cbr\u003eREAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering\u003cbr\u003eFrom RAG to RICHES: Retrieval Interlaced with Sequence Generation\u003cbr\u003eQPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs\u003cbr\u003eModel Editing Harms General Abilities of Large Language Models: Regularization to the Rescue\u003cbr\u003eMechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations\u003cbr\u003eImproving Zero-shot LLM Re-Ranker with Risk Minimization\u003cbr\u003e... and 5 more","\u003cb\u003eDialogue Systems\u003c\u002fb\u003e\u003cbr\u003eEvaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding\u003cbr\u003eAre LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues\u003cbr\u003eRA2FD: Distilling Faithfulness into Efficient Dialogue Systems\u003cbr\u003ePcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity\u003cbr\u003eIntegrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation\u003cbr\u003eA Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents\u003cbr\u003eCantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues\u003cbr\u003eEfficient Sequential Decision Making with Large Language Models\u003cbr\u003eFrom Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues\u003cbr\u003eLanguage Models in Dialogue: Conversational Maxims for Human-AI Interactions\u003cbr\u003e... and 5 more","\u003cb\u003eSelf-consistency\u003c\u002fb\u003e\u003cbr\u003eA Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eCalibrating Long-form Generations from Large Language Models\u003cbr\u003eTree of Problems: Improving structured problem solving with compositionality\u003cbr\u003eEmpowering Multi-step Reasoning across Languages via Program-Aided Language Models\u003cbr\u003ePython is Not Always the Best Choice: Embracing Multilingual Program of Thoughts\u003cbr\u003eRegression-aware Inference with LLMs\u003cbr\u003eDecompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison\u003cbr\u003eMirror-Consistency: Harnessing Inconsistency in Majority Voting\u003cbr\u003eReasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\u003cbr\u003e... and 5 more","\u003cb\u003ePrompt Tuning\u003c\u002fb\u003e\u003cbr\u003eUnlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization\u003cbr\u003eLLM as a metric critic for low resource relation identification\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003eDeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators\u003cbr\u003eStablePT: Towards Stable Prompting for Few-shot Learning via Input Separation\u003cbr\u003eUnderstanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs\u003cbr\u003eMitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment\u003cbr\u003eBiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks\u003cbr\u003eDivide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning\u003cbr\u003eRetrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation\u003cbr\u003e... and 5 more","\u003cb\u003eVqa\u003c\u002fb\u003e\u003cbr\u003eVGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning\u003cbr\u003eSURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information\u003cbr\u003eVisual Question Decomposition on Multimodal Large Language Models\u003cbr\u003eMMedAgent: Learning to Use Medical Tools with Multi-modal Agent\u003cbr\u003eTowards One-to-Many Visual Question Answering\u003cbr\u003eMobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding\u003cbr\u003eA Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect\u003cbr\u003eCommVQA: Situating Visual Question Answering in Communicative Contexts\u003cbr\u003eFrom the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis\u003cbr\u003eERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments\u003cbr\u003e... and 5 more","\u003cb\u003eSafety Alignment\u003c\u002fb\u003e\u003cbr\u003eDATA ADVISOR: Dynamic Data Curation for Safety Alignment of Large Language Models\u003cbr\u003eLoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models\u003cbr\u003eAdvancing Adversarial Suffix Transfer Learning on Aligned Large Language Models\u003cbr\u003eCantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues\u003cbr\u003eHow Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States\u003cbr\u003eFrom LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking\u003cbr\u003eCan Textual Unlearning Solve Cross-Modality Safety Alignment?\u003cbr\u003eLarge Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks\u003cbr\u003eDrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers\u003cbr\u003eHolistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction\u003cbr\u003e... and 5 more","\u003cb\u003eLanguage Model Alignment\u003c\u002fb\u003e\u003cbr\u003eRethinking the Role of Proxy Rewards in Language Model Alignment\u003cbr\u003eEvolutionary Contrastive Distillation for Language Model Alignment\u003cbr\u003eReverse-Engineering the Reader\u003cbr\u003eOptimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003eFiltered Direct Preference Optimization\u003cbr\u003eGDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets\u003cbr\u003eInference-Time Language Model Alignment via Integrated Value Guidance\u003cbr\u003eImproving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning\u003cbr\u003eLIONS: An Empirically Optimized Approach to Align Language Models\u003cbr\u003e... and 5 more","\u003cb\u003eTransferability\u003c\u002fb\u003e\u003cbr\u003eRethinking the Evaluation of In-Context Learning for LLMs\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eTransferability of Syntax-Aware Graph Neural Networks in Zero-Shot Cross-Lingual Semantic Role Labeling\u003cbr\u003eDA\u00b3: A Distribution-Aware Adversarial Attack against Language Models\u003cbr\u003ePrompts have evil twins\u003cbr\u003eDemystifying Verbatim Memorization in Large Language Models\u003cbr\u003eInstruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks\u003cbr\u003eASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings\u003cbr\u003eMIXTURE-OF-SKILLS: Learning to Optimize Data Usage for Fine-Tuning Large Language Models\u003cbr\u003eSelection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability\u003cbr\u003e... and 4 more","\u003cb\u003eDebiasing\u003c\u002fb\u003e\u003cbr\u003eMultimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference\u003cbr\u003eRecent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models\u003cbr\u003eUnlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization\u003cbr\u003eOvercome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue\u003cbr\u003eA Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect\u003cbr\u003eInterventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding\u003cbr\u003eWalking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias\u003cbr\u003eOffsetBias: Leveraging Debiased Data for Tuning Evaluators\u003cbr\u003eEfficient Overshadowed Entity Disambiguation by Mitigating Shortcut Learning\u003cbr\u003eDecoding Matters: Addressing Amplification Bias and Homogeneity Issue for LLM-based Recommendation\u003cbr\u003e... and 4 more","\u003cb\u003eAsr\u003c\u002fb\u003e\u003cbr\u003eModeling Gender and Dialect Bias in Automatic Speech Recognition\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eTokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR\u003cbr\u003eUnveiling the Role of Pretraining in Direct Speech Translation\u003cbr\u003eInterventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding\u003cbr\u003eAre Modern Neural ASR Architectures Robust for Polysynthetic Languages?\u003cbr\u003eLLMs Are Zero-Shot Context-Aware Simultaneous Translators\u003cbr\u003eContinual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech\u003cbr\u003ePolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition\u003cbr\u003eCasablanca: Data and Models for Multidialectal Arabic Speech Recognition\u003cbr\u003e... and 4 more","\u003cb\u003ePruning\u003c\u002fb\u003e\u003cbr\u003eFFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping\u003cbr\u003eExploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation\u003cbr\u003ePruning Multilingual Large Language Models for Multilingual Inference\u003cbr\u003eMoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition\u003cbr\u003eRethinking Token Reduction for State Space Models\u003cbr\u003ePruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging\u003cbr\u003eThreshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval\u003cbr\u003eLlama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection\u003cbr\u003ePruning Foundation Models for High Accuracy without Retraining\u003cbr\u003eChange Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy\u003cbr\u003e... and 4 more","\u003cb\u003eMulti-hop Reasoning\u003c\u002fb\u003e\u003cbr\u003eCNEQ: Incorporating numbers into Knowledge Graph Reasoning\u003cbr\u003eLLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments\u003cbr\u003eCOMPACT: Compressing Retrieved Documents Actively for Question Answering\u003cbr\u003eAn Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models\u003cbr\u003eTRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation\u003cbr\u003eReasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\u003cbr\u003eAugmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering\u003cbr\u003eGraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models\u003cbr\u003eFirst Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning\u003cbr\u003eAdaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations\u003cbr\u003e... and 4 more","\u003cb\u003eMechanistic Interpretability\u003c\u002fb\u003e\u003cbr\u003eOn the Similarity of Circuits across Languages: A Case Study on the Subject-verb Agreement Task\u003cbr\u003eInformation Flow Routes: Automatically Interpreting Language Models at Scale\u003cbr\u003eInterpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis\u003cbr\u003eUnlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models\u003cbr\u003eBeyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning\u003cbr\u003eActivation Scaling for Steering and Interpreting Language Models\u003cbr\u003ePreference Tuning For Toxicity Mitigation Generalizes Across Languages\u003cbr\u003eDeeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning\u003cbr\u003eEncourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective\u003cbr\u003eThe Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis\u003cbr\u003e... and 4 more","\u003cb\u003eKnowledge Retrieval\u003c\u002fb\u003e\u003cbr\u003eLarge Language Models are Limited in Out-of-Context Knowledge Reasoning\u003cbr\u003eVisual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant\u003cbr\u003eQPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs\u003cbr\u003eH-LegalKI: A Hierarchical Legal Knowledge Integration Framework for Legal Community Question Answering\u003cbr\u003eDALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature\u003cbr\u003eAtomic Self-Consistency for Better Long Form Generations\u003cbr\u003eThe Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance\u003cbr\u003eAn LLM-Enabled Knowledge Elicitation and Retrieval Framework for Zero-Shot Cross-Lingual Stance Identification\u003cbr\u003eEfficientRAG: Efficient Retriever for Multi-Hop Question Answering\u003cbr\u003eCItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling\u003cbr\u003e... and 4 more","\u003cb\u003eKnowledge Transfer\u003c\u002fb\u003e\u003cbr\u003eICL: Iterative Continual Learning for Multi-domain Neural Machine Translation\u003cbr\u003eInfrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models\u003cbr\u003eTransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling\u003cbr\u003eMoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition\u003cbr\u003eNeuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation\u003cbr\u003eAn LLM-Enabled Knowledge Elicitation and Retrieval Framework for Zero-Shot Cross-Lingual Stance Identification\u003cbr\u003ePREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment\u003cbr\u003eI Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses\u003cbr\u003eDLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model\u003cbr\u003eMcCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering\u003cbr\u003e... and 4 more","\u003cb\u003eTopic Classification\u003c\u002fb\u003e\u003cbr\u003eTransfer Learning for Text Classification via Model Risk Analysis\u003cbr\u003eCorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs\u003cbr\u003eIn-Context Learning with Iterative Demonstration Selection\u003cbr\u003eMultilingual Topic Classification in X: Dataset and Analysis\u003cbr\u003eLexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003eGeneralists vs. Specialists: Evaluating Large Language Models for Urdu\u003cbr\u003eSYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation\u003cbr\u003eInference and Verbalization Functions During In-Context Learning\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003e... and 4 more","\u003cb\u003eGenerative Models\u003c\u002fb\u003e\u003cbr\u003eAltogether: Image Captioning via Re-aligning Alt-text\u003cbr\u003eCompare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY\u003cbr\u003eEnhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs\u003cbr\u003eExplicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments\u003cbr\u003eGREEN: Generative Radiology Report Evaluation and Error Notation\u003cbr\u003eGRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization\u003cbr\u003eWeak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems\u003cbr\u003eMINERS: Multilingual Language Models as Semantic Retrievers\u003cbr\u003eAn image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance\u003cbr\u003eGenerate then Refine: Data Augmentation for Zero-shot Intent Detection\u003cbr\u003e... and 4 more","\u003cb\u003eEvaluation Metric\u003c\u002fb\u003e\u003cbr\u003eBLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation\u003cbr\u003eFAITHSCORE: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models\u003cbr\u003eQGEval: Benchmarking Multi-dimensional Evaluation for Question Generation\u003cbr\u003eLONG2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall\u003cbr\u003eLearning to Extract Structured Entities Using Language Models\u003cbr\u003eVLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models\u003cbr\u003eUnveiling the Invisible: Captioning Videos with Metaphors\u003cbr\u003eA Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios\u003cbr\u003eHolistic Evaluation for Interleaved Text-and-Image Generation\u003cbr\u003ePolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition\u003cbr\u003e... and 4 more","\u003cb\u003eMedical QA\u003c\u002fb\u003e\u003cbr\u003eMEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation\u003cbr\u003eMedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures\u003cbr\u003eBeyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs.\u003cbr\u003eAMPO: Automatic Multi-Branched Prompt Optimization\u003cbr\u003eMedical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?\u003cbr\u003eCasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures\u003cbr\u003eRoQLlama: A Lightweight Romanian Adapted Language Model\u003cbr\u003eLarge Language Models are In-context Teachers for Knowledge Reasoning\u003cbr\u003eLanguage Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks\u003cbr\u003eEnhancing Healthcare LLM Trust with Atypical Presentations Recalibration\u003cbr\u003e... and 4 more","\u003cb\u003eData Generation\u003c\u002fb\u003e\u003cbr\u003eLexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons\u003cbr\u003eCreative and Context-Aware Translation of East Asian Idioms with GPT-4\u003cbr\u003eEfficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts\u003cbr\u003eFuseGen: PLM Fusion for Data-generation based Zero-shot Learning\u003cbr\u003eExploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors\u003cbr\u003eTable Question Answering for Low-resourced Indic Languages\u003cbr\u003eMP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs\u003cbr\u003eDiverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking\u003cbr\u003eLLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification\u003cbr\u003eLeveraging pre-trained language models for linguistic analysis: A case of argument structure constructions\u003cbr\u003e... and 3 more","\u003cb\u003eText-to-image Generation\u003c\u002fb\u003e\u003cbr\u003eAltogether: Image Captioning via Re-aligning Alt-text\u003cbr\u003ePre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation\u003cbr\u003eGOME: Grounding-based Metaphor Binding With Conceptual Elaboration For Figurative Language Illustration\u003cbr\u003eEmpowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training\u003cbr\u003eLearning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training\u003cbr\u003eAdversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003eWords Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation\u003cbr\u003eThe Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention\u003cbr\u003eRe-ReST: Reflection-Reinforced Self-Training for Language Agents\u003cbr\u003e... and 3 more","\u003cb\u003eRelevance\u003c\u002fb\u003e\u003cbr\u003eTopic-Oriented Open Relation Extraction with A Priori Seed Generation\u003cbr\u003eMitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics\u003cbr\u003eQGEval: Benchmarking Multi-dimensional Evaluation for Question Generation\u003cbr\u003eLearning to Rank Salient Content for Query-focused Summarization\u003cbr\u003eImprove Dense Passage Retrieval with Entailment Tuning\u003cbr\u003eEvaluating D-MERIT of Partial-annotation on Information Retrieval\u003cbr\u003eThreshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval\u003cbr\u003eRetrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation\u003cbr\u003eThe Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models\u003cbr\u003eModel-based Preference Optimization in Abstractive Summarization without Human Feedback\u003cbr\u003e... and 3 more","\u003cb\u003ePerplexity\u003c\u002fb\u003e\u003cbr\u003eScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws\u003cbr\u003eEvaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding\u003cbr\u003eMoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition\u003cbr\u003eAdversarial Text Generation using Large Language Models for Dementia Detection\u003cbr\u003eWho is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eSocial Bias Probing: Fairness Benchmarking for Language Models\u003cbr\u003eI Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses\u003cbr\u003eAdaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?\u003cbr\u003eMonotonic Paraphrasing Improves Generalization of Language Model Prompting\u003cbr\u003e... and 3 more","\u003cb\u003eDataset Creation\u003c\u002fb\u003e\u003cbr\u003eDetecting Temporal Ambiguity in Questions\u003cbr\u003eClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures\u003cbr\u003eEmosical: An Emotion-Annotated Musical Theatre Dataset\u003cbr\u003eCOCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds\u003cbr\u003eFrom Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues\u003cbr\u003eSusu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.\u003cbr\u003eAsk the experts: Sourcing a high-quality nutrition counseling dataset through Human-AI collaboration\u003cbr\u003eAdversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation\u003cbr\u003eCasablanca: Data and Models for Multidialectal Arabic Speech Recognition\u003cbr\u003eForecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling\u003cbr\u003e... and 3 more","\u003cb\u003eLarge Multimodal Models\u003c\u002fb\u003e\u003cbr\u003eTowards Low-Resource Harmful Meme Detection with LMM Agents\u003cbr\u003eM5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks\u003cbr\u003eMM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification\u003cbr\u003eRecent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models\u003cbr\u003eAn Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models\u003cbr\u003eDocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding\u003cbr\u003eVisual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant\u003cbr\u003eMitigating Language Bias of LMMs in Social Intelligence Understanding with Virtual Counterfactual Calibration\u003cbr\u003eTowards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights\u003cbr\u003eTraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering\u003cbr\u003e... and 3 more","\u003cb\u003eTrustworthiness\u003c\u002fb\u003e\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eGenerating Media Background Checks for Automated Source Critical Reasoning\u003cbr\u003eThink Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection\u003cbr\u003eCan Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?\u003cbr\u003eKnowledge Conflicts for LLMs: A Survey\u003cbr\u003eSynchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation\u003cbr\u003eModel Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation\u003cbr\u003eEnhancing Healthcare LLM Trust with Atypical Presentations Recalibration\u003cbr\u003eMultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate\u003cbr\u003eKnowledge Mechanisms in Large Language Models: A Survey and Perspective\u003cbr\u003e... and 3 more","\u003cb\u003eChinese\u003c\u002fb\u003e\u003cbr\u003eCHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models\u003cbr\u003eCreative and Context-Aware Translation of East Asian Idioms with GPT-4\u003cbr\u003eCEAMC: Corpus and Empirical Study of Argument Analysis in Education via LLMs\u003cbr\u003eCERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays\u003cbr\u003eEmploying Glyphic Information for Chinese Event Extraction with Vision-Language Model\u003cbr\u003eOEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary\u003cbr\u003eToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations\u003cbr\u003eCLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models\u003cbr\u003eRe-Evaluating Evaluation for Multilingual Summarization\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003e... and 3 more","\u003cb\u003eScalability\u003c\u002fb\u003e\u003cbr\u003eWaterfall: Scalable Framework for Robust Text Watermarking and Provenance for LLMs\u003cbr\u003eBridging Local Details and Global Context in Text-Attributed Graphs\u003cbr\u003eEfficient Pointwise-Pairwise Learning-to-Rank for News Recommendation\u003cbr\u003eTrain Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation\u003cbr\u003eOptimizing Code Retrieval: High-Quality and Scalable Dataset Annotation through Large Language Models\u003cbr\u003eHyQE: Ranking Contexts with Hypothetical Query Embeddings\u003cbr\u003eReasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\u003cbr\u003eComparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval\u003cbr\u003eTS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models\u003cbr\u003eMisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"\u003cbr\u003e... and 3 more","\u003cb\u003eGeneralizability\u003c\u002fb\u003e\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003eVLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models\u003cbr\u003eWhat is \u201cTypological Diversity\u201d in NLP?\u003cbr\u003eInterventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding\u003cbr\u003eText Fluoroscopy: Detecting LLM-Generated Text through Intrinsic Features\u003cbr\u003eEvaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets\u003cbr\u003eJellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing\u003cbr\u003eMath-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models\u003cbr\u003eNavigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models\u003cbr\u003eSecCoder: Towards Generalizable and Robust Secure Code Generation\u003cbr\u003e... and 3 more","\u003cb\u003eDownstream Tasks\u003c\u002fb\u003e\u003cbr\u003eTransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling\u003cbr\u003eAxis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings\u003cbr\u003eA Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives\u003cbr\u003eMoral Foundations of Large Language Models\u003cbr\u003eTextual Dataset Distillation via Language Model Embedding\u003cbr\u003eMemory-Efficient Fine-Tuning of Transformers via Token Selection\u003cbr\u003eModel Editing Harms General Abilities of Large Language Models: Regularization to the Rescue\u003cbr\u003eDownstream Trade-offs of a Family of Text Watermarks\u003cbr\u003eCollaborative Performance Prediction for Large Language Models\u003cbr\u003eTEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models\u003cbr\u003e... and 3 more","\u003cb\u003eKnowledge Representation\u003c\u002fb\u003e\u003cbr\u003eExplicit Inductive Inference using Large Language Models\u003cbr\u003eTemporal Fact Reasoning over Hyper-Relational Knowledge Graphs\u003cbr\u003eCoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering\u003cbr\u003eEnhancing Incremental Summarization with Structured Representations\u003cbr\u003eSATYRN: A Platform for Analytics Augmented Generation\u003cbr\u003eDense Passage Retrieval: Is it Retrieving?\u003cbr\u003eCan Large Language Models Understand DL-Lite Ontologies? An Empirical Study\u003cbr\u003eBeyond Embeddings: The Promise of Visual Table in Visual Reasoning\u003cbr\u003eDiscovering Knowledge-Critical Subnetworks in Pretrained Language Models\u003cbr\u003eUnraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications\u003cbr\u003e... and 3 more","\u003cb\u003eMultilingual Llms\u003c\u002fb\u003e\u003cbr\u003eRepresentational Isomorphism and Alignment of Multilingual Large Language Models\u003cbr\u003eTeaching LLMs to Abstain across Languages via Multilingual Feedback\u003cbr\u003ePreference Tuning For Toxicity Mitigation Generalizes Across Languages\u003cbr\u003eExploring Multilingual Concepts of Human Values in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?\u003cbr\u003eRLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs\u003cbr\u003ePruning Multilingual Large Language Models for Multilingual Inference\u003cbr\u003eAn Analysis of Multilingual FActScore\u003cbr\u003eIs It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?\u003cbr\u003eUnderstanding and Mitigating Language Confusion in LLMs\u003cbr\u003eConcept Space Alignment in Multilingual LLMs\u003cbr\u003e... and 3 more","\u003cb\u003ePretraining\u003c\u002fb\u003e\u003cbr\u003eTending Towards Stability: Convergence Challenges in Small Language Models\u003cbr\u003eGradient Localization Improves Lifelong Pretraining of Language Models\u003cbr\u003eUnveiling the Role of Pretraining in Direct Speech Translation\u003cbr\u003eIs Child-Directed Speech Effective Training Data for Language Models?\u003cbr\u003eMitigating Catastrophic Forgetting in Language Transfer via Model Merging\u003cbr\u003eVIMI: Grounding Video Generation through Multi-modal Instruction\u003cbr\u003eGetting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection\u003cbr\u003eBLSP-Emo: Towards Empathetic Large Speech-Language Models\u003cbr\u003eHateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models\u003cbr\u003eAdaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?\u003cbr\u003e... and 3 more","\u003cb\u003eMisinformation\u003c\u002fb\u003e\u003cbr\u003eOn the Relationship between Truth and Political Bias in Language Models\u003cbr\u003eMisinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation\u003cbr\u003eIntegrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003eCan Language Models Recognize Convincing Arguments?\u003cbr\u003eDecoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach\u003cbr\u003eF2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation\u003cbr\u003eCan Large Language Models Identify Authorship?\u003cbr\u003eClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs\u003cbr\u003eSAFETY-J: Evaluating Safety with Critique\u003cbr\u003e... and 3 more","\u003cb\u003eContinual Pre-training\u003c\u002fb\u003e\u003cbr\u003eSynthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models\u003cbr\u003eUnlocking Continual Learning Abilities in Language Models\u003cbr\u003eImproving Referring Ability for Biomedical Language Models\u003cbr\u003eLLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training\u003cbr\u003eA Recipe to Train Powerful Romanian LLMs with English Instructions\u003cbr\u003eA Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models\u003cbr\u003eLLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages\u003cbr\u003eCMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models\u003cbr\u003eTask Oriented In-Domain Data Augmentation\u003cbr\u003eBreaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale\u003cbr\u003e... and 3 more","\u003cb\u003eFact Verification\u003c\u002fb\u003e\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003ePROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context\u003cbr\u003eClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs\u003cbr\u003eHow Entangled is Factuality and Deception in German?\u003cbr\u003eMolecular Facts: Desiderata for Decontextualization in LLM Fact Verification\u003cbr\u003eMitigating Hallucination in Fictional Character Role-Play\u003cbr\u003eDyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs\u003cbr\u003eRIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning\u003cbr\u003eZero-Shot Fact Verification via Natural Logic and Large Language Models\u003cbr\u003eNormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization\u003cbr\u003e... and 3 more"],"textposition":"none","x":["QA","In-context Learning","Fine-tuning","Zero-shot Learning","RAG","Reasoning","Instruction Tuning","Generalization","Benchmarks","Evaluation","Few-shot Learning","Data Augmentation","Text Classification","Robustness","Summarization","Machine Translation","Contrastive Learning","Sentiment Analysis","Language Modeling","Text Generation","Interpretability","Natural Language Inference","Commonsense Reasoning","Dataset","Visual QA","Vision-language Models","Reinforcement Learning","Instruction Following","Benchmarksing","Alignment","Chain-of-thought","Low-resource Languages","Transfer Learning","Hallucination","Code Generation","Information Retrieval","Parameter-efficient Fine-tuning","Bias","Synthetic Data","Mathematical Reasoning","Explainability","Evaluation Metrics","Knowledge Distillation","Direct Preference Optimization","Classification","Image Captioning","Multimodal Learning","Efficiency","Named Entity Recognition","Fairness","Transformer","Multi-task Learning","Multilingual","Cross-lingual Transfer","Pre-training","Faithfulness","Human Evaluation","Natural Language Understanding","Reinforcement Learning From Human Feedback","Transformers","Information Extraction","Bias Mitigation","Bias Detection","Multimodal Llms","LLM Evaluation","Hallucination Detection","Domain Adaptation","Diversity","Hallucination Mitigation","Consistency","Calibration","Benchmarkss","Social Media","Preference Optimization","Lora","Relation Extraction","Bert","Model Compression","Speech Recognition","Gsm8k","Logical Reasoning","Reading Comprehension","Deep Learning","Factuality","Text Summarization","Knowledge Graphs","Knowledge Base","Graph Neural Networks","Model Editing","Hate Speech Detection","Self-supervised Learning","Arithmetic Reasoning","Multimodal","Catastrophic Forgetting","Visual Reasoning","Safety","Supervised Fine-tuning","Preference Learning","Rag","Quantization","LLM Alignment","Unsupervised Learning","Tokenization","Knowledge Graph","Translation","Rlhf","Synthetic Data Generation","Annotation","Knowledge Editing","Low-rank Adaptation","Generation","Video Understanding","Privacy","Attention Mechanism","Dialogue Generation","Error Analysis","Datasets","Adversarial Attacks","Language Understanding","Uncertainty","Reward Model","Gender Bias","Planning","Math","Retrieval","Adversarial Attack","Representation Learning","Error Correction","Continual Learning","Prompt Optimization","Ranking","Clustering","Mixture Of Experts","Long Context","Clip","Toxicity","Automatic Speech Recognition","Security","Benchmarks Dataset","Active Learning","Factual Knowledge","Topic Modeling","Emotion Recognition","Explainable Ai","Multi-modal Learning","ML","Open-domain QA","Dialogue Systems","Self-consistency","Prompt Tuning","Vqa","Safety Alignment","Language Model Alignment","Transferability","Debiasing","Asr","Pruning","Multi-hop Reasoning","Mechanistic Interpretability","Knowledge Retrieval","Knowledge Transfer","Topic Classification","Generative Models","Evaluation Metric","Medical QA","Data Generation","Text-to-image Generation","Relevance","Perplexity","Dataset Creation","Large Multimodal Models","Trustworthiness","Chinese","Scalability","Generalizability","Downstream Tasks","Knowledge Representation","Multilingual Llms","Pretraining","Misinformation","Continual Pre-training","Fact Verification"],"y":[208,143,126,111,102,102,91,91,90,89,89,82,81,78,76,73,73,72,72,70,69,66,63,62,61,59,58,57,53,51,50,50,49,49,47,46,43,42,42,41,40,40,40,39,39,36,36,36,36,35,35,34,32,31,30,30,29,28,28,28,28,27,27,27,27,26,26,26,26,26,25,25,24,24,24,24,24,24,23,23,22,22,22,22,22,22,21,21,21,20,20,20,20,20,20,20,20,19,19,19,19,19,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,17,17,17,17,17,17,17,17,17,17,17,17,16,16,16,16,16,16,16,16,16,16,15,15,15,15,15,15,15,15,15,15,15,15,15,15,14,14,14,14,14,14,14,14,14,14,14,14,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13],"type":"bar"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"tickfont":{"size":12},"tickmode":"linear","tickangle":-60},"margin":{"l":50,"r":50,"t":80,"b":300},"font":{"size":14},"title":{"text":"EMNLP2024 Keyword"},"yaxis":{"title":{"text":"Number of papers"}},"height":700,"width":3200,"bargap":0.25},                        {"responsive": true}                    )                };            </script>        </div>
    <script>
    const ngramMap = {"QA": ["LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "Datasets for Multilingual Answer Sentence Selection", "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations", "Paraphrase Types Elicit Prompt Engineering Capabilities", "When Context Leads but Parametric Memory Follows in Large Language Models", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "Characterizing LLM Abstention Behavior in Science QA with Context Perturbations", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "Where is the signal in tokenization space?", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "An Empirical Study of Multilingual Reasoning Distillation for Question Answering", "LLOCO: Learning Long Contexts Offline", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "Large Language Model-based Human-Agent Collaboration for Complex Task Solving", "LONGAGENT: Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration", "Zero-Shot Fact Verification via Natural Logic and Large Language Models", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations", "Dual-Phase Accelerated Prompt Optimization", "LoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters", "Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models", "CSLM: A Framework for Question Answering Dataset Generation through Collaborative Small Language Models", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Don't Just Say \"I don't know\"! Self-aligning Large Language Models for Responding to Unknown Questions with Explanations", "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "INTENTIONQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce", "API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "Aligning Language Models to Explicitly Handle Ambiguity", "Extrinsic Evaluation of Cultural Competence in Large Language Models", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in LLMs", "More Bang for your Context: Virtual Documents for Question Answering over Long Documents", "TroL: Traversal of Layers for Large Language and Vision Models", "PhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study", "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning", "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models", "RoQLlama: A Lightweight Romanian Adapted Language Model", "A Survey on Natural Language Counterfactual Generation", "Can LLMs Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "Are LLMs Aware that Some Questions are not Open-ended?", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation", "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering", "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "\u201cImage, Tell me your story!\" Predicting the original meta-context of visual misinformation", "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension", "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "Calibrating Long-form Generations from Large Language Models", "Can't Remember Details in Long Documents? You Need Some R&R", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs", "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "Measuring the Robustness of NLP Models to Domain Shifts", "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity", "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization", "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering", "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation", "AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses", "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "LumberChunker: Long-Form Narrative Document Segmentation", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "A Survey of AMR Applications", "A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic", "M3SCIQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "Attribute or Abstain: Large Language Models as Long Document Assistants", "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "MixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity", "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection", "In-Context Learning with Iterative Demonstration Selection", "RepMatch: Quantifying Cross-Instance Similarities in Representation Space", "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "Teaching Small Language Models Reasoning through Counterfactual Distillation", "Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "Synthetic Multimodal Question Generation", "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs", "R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation", "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture", "BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering", "Searching for Best Practices in Retrieval-Augmented Generation", "Self-Contradictory Reasoning Evaluation and Detection", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language", "Does Large Language Model Contain Task-Specific Neurons?", "Estimating Knowledge in Large Language Models Without Generating a Single Token", "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs", "MDCR: A Dataset for Multi-Document Conditional Reasoning", "Investigating Mysteries of CoT-Augmented Distillation", "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation", "HoneyComb: A Flexible LLM-Based Agent System for Materials Science", "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism", "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models", "Exploring Hint Generation Approaches in Open-Domain Question Answering", "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation", "QuBE: Question-based Belief Enhancement for Agentic LLM Reasoning", "Cross-Lingual Multi-Hop Knowledge Editing", "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "Boosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?", "Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature", "RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment", "A Notion of Complexity for Theory of Mind via Discrete World Models", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "CHIRON: Rich Character Representations in Long-Form Narratives", "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice", "Language Models Still Struggle to Zero-shot Reason about Time Series", "Abstraction-of-Thought Makes Language Models Better Reasoners", "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries", "Language-to-Code Translation with a Single Labeled Example", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering", "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "Rethinking the Evaluation of In-Context Learning for LLMs", "Query Routing for Homogeneous Tools: An Instantiation in the RAG Scenario", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "M2QA: Multi-domain Multilingual Question Answering", "Encoding and Controlling Global Semantics for Long-form Video Question Answering", "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering", "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA", "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Finance Communities in Long Form Question Answering", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Generation with Dynamic Vocabulary", "Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition", "MedINST: Meta Dataset of Biomedical Instructions", "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "Learning Semantic Structure through First-Order-Logic Translation", "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases", "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "I Could've Asked That: Reformulating Unanswerable Questions", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "BOOLQUESTIONS: Does Dense Retrieval Understand Boolean Logic in Language?", "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "XC-CACHE: Cross-Attending to Cached Context for Efficient LLM Inference", "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models", "A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Learning to Correct for QA Reasoning with Black-box LLMs", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "Detecting Temporal Ambiguity in Questions", "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMS", "EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation", "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning", "Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism", "Encoding Spreadsheets for Large Language Models", "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation"], "In-context Learning": ["Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs.", "On the In-context Generation of Language Models", "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias", "In-Context Compositional Generalization for Large Vision-Language Models", "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment", "Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles", "Exploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions", "Revealing the Parallel Multilingual Learning within Large Language Models", "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models", "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL", "TUTOR-ICL: Guiding Large Language Models for Improved In-Context Learning Performance", "Scaling Sentence Embeddings with Large Language Models", "Self-AMPLIFY : Improving Small Language Models with Self Post Hoc Explanations", "Inference and Verbalization Functions During In-Context Learning", "Thoughts to Target: Enhance Planning for Target-driven Conversation", "ONE2SET + Large Language Model: Best Partners for Keyphrase Generation", "Are Large Language Models (LLMs) Good Social Predictors?", "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues", "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently", "In-context Contrastive Learning for Event Causality Identification", "STANDARDIZE: Aligning Language Models with Expert-Defined Standards for Content Generation", "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "Private prediction for large-scale synthetic text generation", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in LLMs", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Enhancing Advanced Visual Reasoning Ability of Large Language Models", "Data-Centric AI in the Age of Large Language Models", "Aligning Translation-Specific Understanding to General Understanding in Large Language Models", "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models", "Large Language Models are Limited in Out-of-Context Knowledge Reasoning", "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models", "NARRATIVE-OF-THOUGHT: Improving Temporal Reasoning of Large Language Models via Recounted Narratives", "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming", "Large Language Models are In-context Teachers for Knowledge Reasoning", "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?", "Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "Measuring the Robustness of NLP Models to Domain Shifts", "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent", "Tools Fail: Detecting Silent Errors in Faulty Tools", "Demonstration Selection Strategies for Numerical Time Series Data-to-Text", "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "Personalized Video Comment Generation", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "Generating Demonstrations for In-Context Compositional Generalization in Grounded Language Learning", "Categorial Grammar Supertagging via Large Language Models", "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning", "Focused Large Language Models are Stable Many-Shot Learners", "Diversity, Rationalize, and Combine: Ensembling Multiple QA Strategies for Zero-shot Knowledge-based VQA", "Can we teach language models to gloss endangered languages?", "Large Language Models Know What To Say But Not When to Speak", "In-Context Learning with Iterative Demonstration Selection", "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning", "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning", "SPECIALEX: A Benchmark for In-Context Specialized Lexicon Learning", "On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices", "Skills-in-Context: Unlocking Compositionality in Large Language Models", "Position Engineering: Boosting Large Language Models through Positional Information Manipulation", "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach", "LAMBDA: Large Language Model-Based Data Augmentation for Multi-Modal Machine Translation", "Interpretability-based Tailored Knowledge Editing in Transformers", "FLIRT: Feedback Loop In-context Red Teaming", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Ontologically Faithful Generation of Non-Player Character Dialogues", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Efficient Sequential Decision Making with Large Language Models", "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents", "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "Does Large Language Model Contain Task-Specific Neurons?", "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation", "Improving Referring Ability for Biomedical Language Models", "Retrieved In-Context Principles from Previous Mistakes", "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning", "Defending Jailbreak Prompts via In-Context Adversarial Game", "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences", "MAVEN-FACT: A Large-scale Event Factuality Detection Dataset", "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models", "Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree", "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "Revisiting the Impact of Pursuing Modularity for Code Generation", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "Tree of Problems: Improving structured problem solving with compositionality", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", " FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking ", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "On the Empirical Complexity of Reasoning and Planning in LLMs", "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers", "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks", "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting", "What Are the Odds? Language Models Are Capable of Probabilistic Reasoning", "The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning", "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning", "A Survey on In-context Learning", "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "What's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs", "Abstraction-of-Thought Makes Language Models Better Reasoners", "SPINACH: SPARQL-Based Information Navigation for Challenging Real-World Questions", "Language-to-Code Translation with a Single Labeled Example", "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation", "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "C-ICL: Contrastive In-context Learning for Information Extraction", "Rethinking the Evaluation of In-Context Learning for LLMs", "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing", "\"A good pun is its own reword\": Can Large Language Models Understand Puns?", "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement", "Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition", "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement", "Analyzing Context Contributions in LLM-based Machine Translation", "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning", "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs", "Representational Analysis of Binding in Language Models", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability", "Exploring the Learning Capabilities of Language Models using LEVERWORLDS", "Few shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering", "When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "Semformer: Transformer Language Models with Semantic Planning", "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMS", "SecCoder: Towards Generalizable and Robust Secure Code Generation", "What Matters in Memorizing and Recalling Facts? Multifaceted Benchmarks for Knowledge Probing in Language Models", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "Learning to Retrieve Iteratively for In-Context Learning", "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law", "Augmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering", "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning"], "Fine-tuning": ["Evaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding", "Breaking ReLU Barrier: Generalized MoEfication for Dense Pretrained Models", "ALANAVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding", "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Fast Forwarding Low-Rank Training", "To Err Is Human, but Llamas Can Learn It Too", "AdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation", "Scaling Sentence Embeddings with Large Language Models", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP", "Private prediction for large-scale synthetic text generation", "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "Data-Centric AI in the Age of Large Language Models", "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning", "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models", "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions", "Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension", "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging", "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs", "GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients", "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "I love pineapple on pizza != I hate pineapple on pizza: Stance-Aware Sentence Transformers for Opinion Mining", "Measuring the Robustness of NLP Models to Domain Shifts", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent", "TRANSLLAMA: LLM-based Simultaneous Translation System", "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach", "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models", "Dissecting Fine-Tuning Unlearning in Large Language Models", "DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models", "Remember This Event That Year? Assessing Temporal Information and Understanding in Large Language Models", "QEFT: Quantization for Efficient Fine-Tuning of LLMs", "Large Language Models Can Be Contextual Privacy Protection Learners", "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation", "LexMatcher: Dictionary-centric Data Curation for LLM-based Machine Translation", "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains", "Categorial Grammar Supertagging via Large Language Models", "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "An Analysis and Mitigation of the Reversal Curse", "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together", "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "Puzzle Solving using Reasoning of Large Language Models: A Survey", "CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning", "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems", "On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices", "Do Large Language Models Know How Much They Know?", "Skills-in-Context: Unlocking Compositionality in Large Language Models", "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups", "Can LLMs Reason in the Wild with Programs?", "Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions", "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation", "Estimating Knowledge in Large Language Models Without Generating a Single Token", "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation", "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering", "MAVEN-FACT: A Large-scale Event Factuality Detection Dataset", "Revisiting the Impact of Pursuing Modularity for Code Generation", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "Event-Keyed Summarization", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "On the Empirical Complexity of Reasoning and Planning in LLMs", "RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment", "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language Story Understanding", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "LaRA: Large Rank Adaptation for Speech and Text Cross-Modal Learning in Large Language Models", "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "Domain adapted machine translation: What does catastrophic forgetting forget and why?", "Stochastic Fine-Tuning of Language Models Using Masked Gradients", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "Enhancing Polyglot Voices by Leveraging Cross-Lingual Fine-Tuning in Any-to-One Voice Conversion", "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach", "MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Finance Communities in Long Form Question Answering", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction", "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models", "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets", "Step-level Value Preference Optimization for Mathematical Reasoning", "Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge", "Model Balancing Helps Low-data Training and Fine-tuning", "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "Visual Question Decomposition on Multimodal Large Language Models", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "SODA-EVAL: Open-Domain Dialogue Evaluation in the age of LLMs", "Systematic Biases in LLM Simulations of Debates", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "User Inference Attacks on Large Language Models", "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "\u201cIn Dialogues We Learn\u201d: Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning", "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases", "Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering", "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?", "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "MIXTURE-OF-SKILLS: Learning to Optimize Data Usage for Fine-Tuning Large Language Models", "Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation", "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities"], "Zero-shot Learning": ["Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "Functionality learning through specification instructions", "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement", "PALM: Few-Shot Prompt Learning for Audio Language Models", "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "Multi-trait User Simulation with Adaptive Decoding for Conversational Task Assistants", "Show and Guide: Instructional-Plan Grounded Vision and Language Model", "Zero-Shot Fact Verification via Natural Logic and Large Language Models", "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control", "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to Addressee Recognition and Response Selection in Conversations", "Text2Model: Text-based Model Induction for Zero-shot Image Classification", "PsFuture: A Pseudo-Future-based Zero-Shot Adaptive Policy for Simultaneous Machine Translation", "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Conditional and Modal Reasoning in Large Language Models", "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization", "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?", "Learning to Plan by Updating Natural Language", "Table Question Answering for Low-resourced Indic Languages", "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "Topic-Oriented Open Relation Extraction with A Priori Seed Generation", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "Multilingual Topic Classification in X: Dataset and Analysis", "A Simple LLM Framework for Long-Range Video Question-Answering", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "MixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity", "Diversity, Rationalize, and Combine: Ensembling Multiple QA Strategies for Zero-shot Knowledge-based VQA", "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "Can Large Language Models Identify Authorship?", "Exploring the Best Practices of Query Expansion with Large Language Models", "Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets", "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups", "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "SAFARI: Cross-lingual Bias and Factuality Detection in News Media and News Articles", "Unleashing Large Language Models' Proficiency in Zero-shot Essay Scoring", "Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents", "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval", "Visual Prompting in LLMs for Enhancing Emotion Recognition", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "LLMs Are Zero-Shot Context-Aware Simultaneous Translators", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction", "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval", "Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval", "DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding", "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs", "An LLM-Enabled Knowledge Elicitation and Retrieval Framework for Zero-Shot Cross-Lingual Stance Identification", "Deciphering Cognitive Distortions in Patient-Doctor Mental Health Conversations: A Multimodal LLM-Based Detection and Reasoning Framework", "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing", "Zero-shot Commonsense Reasoning over Machine Imagination", "Exploring Hint Generation Approaches in Open-Domain Question Answering", "Altogether: Image Captioning via Re-aligning Alt-text", "Pruning Multilingual Large Language Models for Multilingual Inference", "Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition", "Evaluating Large Language Models via Linguistic Profiling", "Lost in Translation: Chemical Language Models and the Misunderstanding of Molecule Structures", "What Are the Odds? Language Models Are Capable of Probabilistic Reasoning", "Large Language Models are Students at Various Levels: Zero-shot Question Difficulty Estimation", "Hate Personified: Investigating the role of LLMs in content moderation", "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "MalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "Language Models Still Struggle to Zero-shot Reason about Time Series", "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "Transferability of Syntax-Aware Graph Neural Networks in Zero-Shot Cross-Lingual Semantic Role Labeling", "Retrieval-enriched zero-shot image classification in low-resource domains", "ADELIE: Aligning Large Language Models on Information Extraction", "Incubating Text Classifiers Following User Instruction with Nothing but LLM", "UNIGEN: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "OpenGraph: Towards Open Graph Foundation Models", "Instance-Level Dynamic LoRAs Composition for Cross-Task Generalization", "GRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "Are LLMs Good Zero-Shot Fallacy Classifiers?", "Navigating Hallucinations for Reasoning of Unintentional Activities", "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment", "Style-Specific Neurons for Steering LLMs in Text Style Transfer", "Open-world Multi-label Text Classification with Extremely Weak Supervision", "Large Language Models for Propaganda Span Annotation", "SODA-EVAL: Open-Domain Dialogue Evaluation in the age of LLMs", "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "Zero-shot Cross-domain Dialogue State Tracking via Context-aware Auto-prompting and Instruction-following Contrastive Decoding", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages", "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection", "Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering", "Identifying Factual Inconsistencies in Summaries: Grounding LLM Inference via Task Taxonomy", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "VPL: Visual Proxy Learning Framework for Zero-Shot Medical Image Diagnosis", "LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law", "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text", "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "LongForm: Effective Instruction Tuning with Reverse Instructions"], "RAG": ["README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP", "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering", "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations", "Free your mouse! Command Large Language Models to Generate Code to Format Word Documents", "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction", "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "LLOCO: Learning Long Contexts Offline", "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "Improve Dense Passage Retrieval with Entailment Tuning", "EvOR: Evolving Retrieval for Code Generation", "Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models", "Knowledge Verification to Nip Hallucination in the Bud", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft", "Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework", "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation", "TimeR4: Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering", "Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision", "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering", "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs", "An Analysis of Multilingual FActScore", "\u201cImage, Tell me your story!\" Predicting the original meta-context of visual misinformation", "TRIAGEAGENT: Towards Better Multi-Agents Collaborations for Large Language Model-Based Clinical Triage", "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "Can't Remember Details in Long Documents? You Need Some R&R", "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR", "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent", "OPEN-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models", "From RAG to RICHES: Retrieval Interlaced with Sequence Generation", "Can Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction", "LumberChunker: Long-Form Narrative Document Segmentation", "DVD: Dynamic Contrastive Decoding for Knowledge Amplification in Multi-Document Question Answering", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "Dense Passage Retrieval: Is it Retrieving?", "Attribute or Abstain: Large Language Models as Long Document Assistants", "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "Analysis of Plan-based Retrieval for Grounded Text Generation", "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "MixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity", "ConTReGen: Context-driven Tree-structured Retrieval for Open-domain Long-form Text Generation", "BOOKWORM: A Dataset for Character Description and Analysis", "AGRAME: Any-Granularity Ranking with Multi-Vector Embeddings", "Position Engineering: Boosting Large Language Models through Positional Information Manipulation", "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs", "R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation", "Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation", "Unified Active Retrieval for Retrieval Augmented Generation", "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models", "Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA", "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering", "Searching for Best Practices in Retrieval-Augmented Generation", "Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA", "LLMs as Collaborator: Demands-Guided Collaborative Retrieval-Augmented Generation for Commonsense Knowledge-Grounded Open-Domain Dialogue Systems", "Improving Zero-shot LLM Re-Ranker with Risk Minimization", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "LLM generated responses to mitigate the impact of hate speech", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "LONG2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall", "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation", "Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs", "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation", "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering", "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation", "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models", "QuBE: Question-based Belief Enhancement for Agentic LLM Reasoning", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature", "RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment", "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation", "Multi-Level Information Retrieval Augmented Generation for Knowledge-based Visual Question Answering", "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "Query Routing for Homogeneous Tools: An Instantiation in the RAG Scenario", "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "SATYRN: A Platform for Analytics Augmented Generation", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering", "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts", "\"Knowing When You Don't Know\u201d: A Multilingual Relevance Assessment Dataset for Robust Retrieval-Augmented Generation", "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation", "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs", "Towards Low-Resource Harmful Meme Detection with LMM Agents", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation", "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA", "RAR: Retrieval-augmented retrieval for code generation in low-resource languages", "Mitigating Hallucination in Fictional Character Role-Play", "Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs", "Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models", "Defending Against Social Engineering Attacks in the Age of LLMs", "Can We Instruct LLMs to Compensate for Position Bias?"], "Reasoning": ["Merely Judging Metaphor is Not Enough: Research on Reasonable Metaphor Detection", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts", "SOCIALGAZE: Improving the Integration of Human Social Norms in Large Language Models", "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification", "Revealing the Parallel Multilingual Learning within Large Language Models", "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models", "Self-AMPLIFY : Improving Small Language Models with Self Post Hoc Explanations", "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs", "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning", "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner for Insightful Table Summarization", "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA", "Re-Reading Improves Reasoning in Large Language Models", "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently", "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity", "Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping", "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "PhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "Learning to Plan by Updating Natural Language", "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models", "Towards Benchmarking Situational Awareness of Large Language Models: Comprehensive Benchmark, Evaluation and Analysis", "FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "Head-wise Shareable Attention for Large Language Models", "When Reasoning Meets Information Aggregation: A Case Study with Sports Narratives", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering", "OPEN-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "Improving Multi-Agent Debate with Sparse Communication Topology", "Explicit Inductive Inference using Large Language Models", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning", "ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering", "M3SCIQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons", "An Analysis and Mitigation of the Reversal Curse", "One Thousand and One Pairs: A \u201cnovel\u201d challenge for long-context language models", "In-Context Learning with Iterative Demonstration Selection", "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning", "Puzzle Solving using Reasoning of Large Language Models: A Survey", "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework", "Skills-in-Context: Unlocking Compositionality in Large Language Models", "Can LLMs Reason in the Wild with Programs?", "Teaching LLMs to Abstain across Languages via Multilingual Feedback", "Self-Contradictory Reasoning Evaluation and Detection", "Belief Revision: The Adaptability of Large Language Models Reasoning", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate", "Enhancing Emotion-Cause Pair Extraction in Conversations via Center Event Detection and Reasoning", "Retrieved In-Context Principles from Previous Mistakes", "CAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans", "Will LLMs Sink or Swim? Exploring Decision-Making Under Pressure", "Large Language Models Are Challenged by Habitat-Centered Reasoning", "Boosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?", "Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization", "Subjective Topic meets LLMs: Unleashing Comprehensive, Reflective and Creative Thinking through the Negation of Negation", "On the Empirical Complexity of Reasoning and Planning in LLMs", "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "Language Models Still Struggle to Zero-shot Reason about Time Series", "DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting", "Abstraction-of-Thought Makes Language Models Better Reasoners", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning?", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "Large Language Models Can Self-Correct with Key Condition Verification", "Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness", "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "Weak-to-Strong Reasoning", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Navigating Hallucinations for Reasoning of Unintentional Activities", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study", "ANALOBENCH: Benchmarking the Identification of Abstract and Long-context Analogies", "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation", "Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction", "Learning to Correct for QA Reasoning with Black-box LLMs", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models", "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "Multi-Granularity History and Entity Similarity Learning for Temporal Knowledge Graph Reasoning", "Natural Evolution-based Dual-Level Aggregation for Temporal Knowledge Graph Reasoning", "Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction", "Working Memory Identifies Reasoning Limits in Language Models", "Fine-tuning Smaller Language Models for Question Answering over Financial Documents"], "Instruction Tuning": ["TextLap: Customizing Language Models for Text-to-Layout Planning", "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG", "Fast Forwarding Low-Rank Training", "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "XRec: Large Language Models for Explainable Recommendation", "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Knowledge Verification to Nip Hallucination in the Bud", "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs", "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently", "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search", "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with Really Good Data", "Data Diversity Matters for Robust Instruction Tuning", "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models", "Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation", "Can LLMs Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner", "DC-Instruct: An Effective Framework for Generative Multi-intent Spoken Language Understanding", "Achieving Stronger Generation via Simple Contrastive Tuning", "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning", "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs", "Automatic Instruction Evolving for Large Language Models", "Adversarial Text Generation using Large Language Models for Dementia Detection", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities", "Improving Argument Effectiveness Across Ideologies using Instruction-tuned Large Language Models", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "Which questions should I answer? Salience Prediction of Inquisitive Questions", "VIMI: Grounding Video Generation through Multi-modal Instruction", "From RAG to RICHES: Retrieval Interlaced with Sequence Generation", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Large Language Models Can Be Contextual Privacy Protection Learners", "QUDSELECT: Selective Decoding for Questions Under Discussion Parsing", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis", "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations", "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "M\u00b2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "LLMs Are Zero-Shot Context-Aware Simultaneous Translators", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "Learning Personalized Alignment in Evaluating Open-ended Text Generation", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model", "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?", "Better Alignment with Instruction Back-and-Forth Translation", "GOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory", "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization", "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters", "ADELIE: Aligning Large Language Models on Information Extraction", "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach", "Incubating Text Classifiers Following User Instruction with Nothing but LLM", "An Instruction Tuning-Based Contrastive Learning Framework for Aspect Sentiment Quad Prediction with Implicit Aspects and Opinions", "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging", "The Overlooked Repetitive Lengthening Form in Sentiment Analysis", "Control Large Language Models via Divide and Conquer", "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback", "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?", "Towards Tool Use Alignment of Large Language Models", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context", "BiMediX: Bilingual Medical Mixture of Experts LLM", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "Zero-shot Cross-domain Dialogue State Tracking via Context-aware Auto-prompting and Instruction-following Contrastive Decoding", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Temporally Consistent Factuality Probing for Large Language Models", "Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing", "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "How Susceptible are Large Language Models to Ideological Manipulation?", "Curriculum Consistency Learning for Conditional Sentence Generation", "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "LongForm: Effective Instruction Tuning with Reverse Instructions"], "Generalization": ["Merely Judging Metaphor is Not Enough: Research on Reasonable Metaphor Detection", "On the Generalization of Training-based ChatGPT Detection Methods", "Functionality learning through specification instructions", "Breaking ReLU Barrier: Generalized MoEfication for Dense Pretrained Models", "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "Rethinking Evaluation Methods for Machine Unlearning", "Morpheus: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space", "Cross-Domain Audio Deepfake Detection: Dataset and Analysis", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "Preserving Generalization of Language Models in Few-shot Continual Relation Extraction", "On the Universal Truthfulness Hyperplane Inside LLMS", "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "Achieving Stronger Generation via Simple Contrastive Tuning", "Personas as a Way to Model Truthfulness in Language Models", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners", "Scaling Laws for Fact Memorization of Large Language Models", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing", "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains", "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "Immunization against harmful fine-tuning attacks", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "A Training Data Recipe to Accelerate A* Search with Large Language Models\\", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions", "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "Learning from Natural Language Explanations for Generalizable Entity Matching", "Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization", "PREDICT: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "Monotonic Paraphrasing Improves Generalization of Language Model Prompting", "Zero-shot Commonsense Reasoning over Machine Imagination", "Cross-Lingual Multi-Hop Knowledge Editing", "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization", "Predicting generalization performance with correctness discriminators", "TOOLVERIFIER: Generalization to New Tools via Self-Verification", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?", "Stochastic Fine-Tuning of Language Models Using Masked Gradients", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "OpenGraph: Towards Open Graph Foundation Models", "Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS", ">Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "Towards One-to-Many Visual Question Answering", "Can Large Language Models Learn Independent Causal Mechanisms?", "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models", "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion", "Subword Segmentation in LLMs: Looking at Inflection and Consistency", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "Learning Semantic Structure through First-Order-Logic Translation", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation", "Towards Robust Evaluation of Unlearning in LLMs via Data Transformations", "MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic", "Can Language Models Induce Grammatical Knowledge from Indirect Evidence?", "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "Data Contamination Can Cross Language Barriers", "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "Learning to Retrieve Iteratively for In-Context Learning", "Knowledge Graph Enhanced Large Language Model Editing", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology", "Demystifying Verbatim Memorization in Large Language Models", "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion"], "Benchmarks": ["Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models", "EU DisinfoTest: a Benchmark for Evaluating Language Models' Ability to Detect Disinformation Narratives", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer", "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction", "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance", "INTENTIONQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce", "AKEW: Assessing Knowledge Editing in the Wild", "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "LEGOBENCH: Scientific Leaderboard Generation Benchmark", "RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion", "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "Towards Benchmarking Situational Awareness of Large Language Models: Comprehensive Benchmark, Evaluation and Analysis", "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark", "SedarEval: Automated Evaluation using Self-Adaptive Rubrics", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "Measuring the Robustness of NLP Models to Domain Shifts", "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models", "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "One-to-many testing for code generation from (just) natural language", "M3SCIQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation", "Attribute or Abstain: Large Language Models as Long Document Assistants", "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation", "SPECIALEX: A Benchmark for In-Context Specialized Lexicon Learning", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "Can LLMs Reason in the Wild with Programs?", "Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?", "LawBench: Benchmarking Legal Knowledge of Large Language Models", "CUTE: Measuring LLMs' Understanding of Their Tokens", "CLEAR: Can Language Models Really Understand Causal Graphs?", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "MVP-Bench: Can Large Vision\u2013Language Models Conduct Multi-level Visual Perception Like Humans?", "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading", "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models", "Mitigating Open-Vocabulary Caption Hallucinations", "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles", "CAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans", "LONG2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall", "Knowledge-Centric Hallucination Detection", "Enhancing Tool Retrieval with Iterative Feedback from Large Language Models", "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs", "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "TROTR: A Framework for Evaluating the Recontextualization of Text", "Subjective Topic meets LLMs: Unleashing Comprehensive, Reflective and Creative Thinking through the Negation of Negation", "VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values", "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories", "SecureSQL: Evaluating Data Leakage of Large Language Models as Natural Language Interfaces to Databases", "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models", "MalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language", "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models", "TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners", "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "M2QA: Multi-domain Multilingual Question Answering", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "LONGGENBENCH: Long-context Generation Benchmark", "GuardBench: A Large-Scale Benchmark for Guardrail Models", "BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla", "I Could've Asked That: Reformulating Unanswerable Questions", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction", "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning", "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning", "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA", "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification", "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning", "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays", "Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments", "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages"], "Evaluation": ["Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "MiTTenS: A Dataset for Evaluating Gender Mistranslation", "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations", "EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models", "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios", "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer", "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs", "TOWER: Tree Organized Weighting for Evaluating Complex Instructions", "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation", "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs", "Knowledge-Centric Templatic Views of Documents", "Rethinking Evaluation Methods for Machine Unlearning", "AKEW: Assessing Knowledge Editing in the Wild", "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "ALIGN-SIM: A Task-Free Test Bed for Evaluating and Interpreting Sentence Embeddings through Semantic Similarity Alignment", "Assessing and Verifying Task Utility in LLM-Powered Applications", "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion", "Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics", "Towards Benchmarking Situational Awareness of Large Language Models: Comprehensive Benchmark, Evaluation and Analysis", "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition", "An Open-Source Data Contamination Report for Large Language Models", "One-to-many testing for code generation from (just) natural language", "CELLO: Causal Evaluation of Large Vision-Language Models", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents", "SPECIALEX: A Benchmark for In-Context Specialized Lexicon Learning", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "Privacy Evaluation Benchmarks for NLP Models", "Evaluating Diversity in Automatic Poetry Generation", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "Factuality of Large Language Models: A Survey", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "Academics Can Contribute to Domain-Specialized Language Models", "Self-Contradictory Reasoning Evaluation and Detection", "LawBench: Benchmarking Legal Knowledge of Large Language Models", "CUTE: Measuring LLMs' Understanding of Their Tokens", "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP", "A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation", "MVP-Bench: Can Large Vision\u2013Language Models Conduct Multi-level Visual Perception Like Humans?", "POSIX: A Prompt Sensitivity Index For Large Language Models", "ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models", "MIRRORSTORIES: Reflecting Diversity through Personalized Narrative Generation with Large Language Models", "BLADE: Benchmarking Language Model Agents for Data-Driven Science", "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "Evaluating Gender Bias of LLMs in Making Morality Judgements", "PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models", "Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis", "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models", "Towards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey", "Multi-dimensional Evaluation of Empathetic Dialogue Responses", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion", "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Finance Communities in Long Form Question Answering", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "COPYBENCH: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "In2Core: Leveraging Influence Functions for Coreset Selection in Instruction Finetuning of Large Language Models", "ANALOBENCH: Benchmarking the Identification of Abstract and Long-context Analogies", "CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.", "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Do LLMs Plan Like Human Writers? Comparing Journalist Coverage of Press Releases with LLMs", "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification", "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning", "Can LLM be a Personalized Judge?"], "Few-shot Learning": ["Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "INDUCT-LEARN: Short Phrase Prompting with Instruction Induction", "PALM: Few-Shot Prompt Learning for Audio Language Models", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning", "Dual-Phase Accelerated Prompt Optimization", "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models", "Sanitizing Large Language Models in Bug Detection with Data-Flow", "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs", "Cross-Domain Audio Deepfake Detection: Dataset and Analysis", "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "Improve Meta-learning for Few-Shot Text Classification with All You Can Acquire from the Tasks", "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft", "Preserving Generalization of Language Models in Few-shot Continual Relation Extraction", "AnyTrans: Translate AnyText in the Image with Large Scale Models", "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Learning to Plan by Updating Natural Language", "Large Language Models are In-context Teachers for Knowledge Reasoning", "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL", "Emoknob: Enhance Voice Cloning with Fine-Grained Emotion Control", "Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters", "Measuring the Robustness of NLP Models to Domain Shifts", "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "Personalized Video Comment Generation", "Multilingual Topic Classification in X: Dataset and Analysis", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "Generating Demonstrations for In-Context Compositional Generalization in Grounded Language Learning", "In-Context Learning with Iterative Demonstration Selection", "Representational Isomorphism and Alignment of Multilingual Large Language Models", "Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models", "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups", "TINYSTYLER: Efficient Few-Shot Text Style Transfer with Authorship Embeddings", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction", "Scalable and Domain-General Abstractive Proposition Segmentation", "OpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation", "POSIX: A Prompt Sensitivity Index For Large Language Models", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language", "DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction", "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", "Exploring Hint Generation Approaches in Open-Domain Question Answering", "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "TOOLVERIFIER: Generalization to New Tools via Self-Verification", "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs", "Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition", "Evaluating Large Language Models via Linguistic Profiling", "What Are the Odds? Language Models Are Capable of Probabilistic Reasoning", "A Survey on In-context Learning", "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "Multilingual Fine-Grained News Headline Hallucination Detection", "ADELIE: Aligning Large Language Models on Information Extraction", "Language-to-Code Translation with a Single Labeled Example", "C-ICL: Contrastive In-context Learning for Information Extraction", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting", "A Coarse-to-Fine Prototype Learning Approach for Multi-Label Few-Shot Intent Detection", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "Optimizing Chinese Lexical Simplification Across Word Types: A Hybrid Approach", "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning", "Analyzing Context Contributions in LLM-based Machine Translation", "Towards Low-Resource Harmful Meme Detection with LMM Agents", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs", "Major Entity Identification: A Generalizable Alternative to Coreference Resolution", "Learning to Retrieve Iteratively for In-Context Learning", "Augmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation"], "Data Augmentation": ["README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP", "HUMVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid", "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study", "RWKV-CLIP: A Robust Vision-Language Representation Learner", "ADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL", "Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4", "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search", "Does Context Help Mitigate Gender Bias in Neural Machine Translation?", "A Study of Implicit Ranking Unfairness in Large Language Models", "SARCAT: Generative Span-Act Guided Response Generation Using Copy-Enhanced Target Augmentation", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "Cross-lingual Contextualized Phrase Retrieval", "Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing", "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing", "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification", "GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "VerifyMatch: A Semi-Supervised Learning Paradigm for Natural Language Inference with Confidence-Aware MixUp", "CareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation", "An Audit on the Perspectives and Challenges of Hallucinations in NLP", "When and Where Did It Happen? An Encoder-Decoder Model to Identify Scenario Context", "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free-Word-Ordered and Morphologically-Rich Low-Resource Languages", "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support", "Teaching Small Language Models Reasoning through Counterfactual Distillation", "Controlled Transformation of Text-Attributed Graphs", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper", "On Creating an English-Thai Code-switched Machine Translation in Medical Domain", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "LAMBDA: Large Language Model-Based Data Augmentation for Multi-Modal Machine Translation", "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models", "Task Oriented In-Domain Data Augmentation", "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP", "GE2PE: Persian End-to-End Grapheme-to-Phoneme Conversion", "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models", "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "ICON: Improving Inter-Report Consistency in Radiology Report Generation via Lesion-aware Mixup Augmentation", "\u201cWhat is the value of {templates}?\u201d Rethinking Document Information Extraction Datasets for LLMs", "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model", "Better Alignment with Instruction Back-and-Forth Translation", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "Retrieval-enriched zero-shot image classification in low-resource domains", "DICTDIS: Dictionary Constrained Disambiguation for Improved NMT", "CodeFort: Robust Training for Code Generation Models", "Enhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs", "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing", "Deciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "OpenGraph: Towards Open Graph Foundation Models", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference", "Story Embeddings \u2013 Narrative-Focused Representations of Fictional Stories", "Self-training Language Models for Arithmetic Reasoning", "Authorship Obfuscation in Multilingual Machine-Generated Text Detection", "STARK: Social Long-Term Multi-Modal Conversation with Persona Commonsense Knowledge", "ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "Evolutionary Contrastive Distillation for Language Model Alignment", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?", "LongForm: Effective Instruction Tuning with Reverse Instructions"], "Text Classification": ["On the Generalization of Training-based ChatGPT Detection Methods", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models", "Textual Dataset Distillation via Language Model Embedding", "ADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers", "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Mental Disorder Classification via Temporal Representation of Text", "Efficient Active Learning with Adapters", "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "Rethinking Evaluation Methods for Machine Unlearning", "Improve Meta-learning for Few-Shot Text Classification with All You Can Acquire from the Tasks", "Linear Layer Extrapolation for Fine-Grained Emotion Classification", "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference", "PERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts", "A Generic Method for Fine-grained Category Discovery in Natural Language Texts", "CED: Comparing Embedding Differences for Detecting Out-of-Distribution and Hallucinated Text", "Towards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs", "Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification", "A Survey on Natural Language Counterfactual Generation", "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions", "Quantum Recurrent Architectures for Text Classification", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "Distance-aware Calibration for Pre-trained Language Models", "Adversarial Text Generation using Large Language Models for Dementia Detection", "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction", "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples", "Detecting Online Community Practices with Large Language Models: A Case Study of Pro-Ukrainian Publics on Twitter", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Transfer Learning for Text Classification via Model Risk Analysis", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Characterizing Text Datasets with Psycholinguistic Features", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices", "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI", "Random Label Forests: An Ensemble Method with Label Subsampling For Extreme Multi-Label Problems", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "CERT-ED: Certifiably Robust Text Classification for Edit Distance", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", " 'Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated Peer Reviews ", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "GottBERT: a pure German Language Model", "Unlocking the Potential of Model Merging for Low-Resource Languages", "Bias Wipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability", "Multi-Loss Fusion: Angular and Contrastive Integration for Machine-Generated Text Detection", "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Robust Text Classification: Analyzing Prototype-Based Networks", "Enhancing Byzantine-Resistant Aggregations with Client Embedding", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "Incubating Text Classifiers Following User Instruction with Nothing but LLM", "Rethinking the Evaluation of In-Context Learning for LLMs", "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Logits Reranking via Semantic Labels for Hard Samples in Text Classification", "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "MedINST: Meta Dataset of Biomedical Instructions", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment", "On the Fragility of Active Learners for Text Classification", "AMPO: Automatic Multi-Branched Prompt Optimization", "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging", "PE: A Poincare Explanation Method for Fast Text Hierarchy Generation", "A Morphology-Based Investigation of Positional Encodings", "A Bayesian Approach to Harnessing the Power of LLMs in Authorship Attribution", "LoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion"], "Robustness": ["Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations", "ADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers", "Paraphrase Types Elicit Prompt Engineering Capabilities", "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations", "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs", "Rethinking Evaluation Methods for Machine Unlearning", "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models", "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently", "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics", "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions", "Assessing and Verifying Task Utility in LLM-Powered Applications", "Data Diversity Matters for Robust Instruction Tuning", "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "A Survey on Natural Language Counterfactual Generation", "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models", "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models", "An LLM Feature-based Framework for Dialogue Constructiveness Assessment", "Enhancing Learning-Based Binary Code Similarity Detection Model through Adversarial Training with Multiple Function Variants", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "Revisiting the Robustness of Watermarking to Paraphrasing Attacks", "Distance-aware Calibration for Pre-trained Language Models", "Prompts have evil twins", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "Robust AI-Generated Text Detection by Restricted Embeddings", "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR", "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples", "Advancing Large Language Model Attribution through Self-Improving", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making", "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "Exploring the Practicality of Generative Retrieval on Dynamic Corpora", "Revisiting Query Variation Robustness of Transformer Models", "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning", "Towards Robust Speech Representation Learning for Thousands of Languages", "CLEAR: Can Language Models Really Understand Causal Graphs?", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "POSIX: A Prompt Sensitivity Index For Large Language Models", "Waterfall: Scalable Framework for Robust Text Watermarking and Provenance for LLMs", "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation", "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Knowledge Conflicts for LLMs: A Survey", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks", "RAFT: Realistic Attacks to Fool Text Detectors", "A Thorough Examination of Decoding Methods in the Era of LLMs", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "Nash CoT: Multi-Path Inference with Preference Equilibrium", "ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations", "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "Robust Text Classification: Analyzing Prototype-Based Networks", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "CodeFort: Robust Training for Code Generation Models", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking", "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction", "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts", "Authorship Obfuscation in Multilingual Machine-Generated Text Detection", "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets", "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation", "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning", "FOOL ME IF YOU CAN! An Adversarial Dataset to Investigate the Robustness of LMs in Word Sense Disambiguation", "SecCoder: Towards Generalizable and Robust Secure Code Generation", "IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding", "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval", "Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation"], "Summarization": ["SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization", "Calibrating Long-form Generations from Large Language Models", "Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA", "Towards Aligning Language Models with Textual Feedback", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "Optimized Speculative Sampling for GPU Hardware Accelerators", "Searching for Best Practices in Retrieval-Augmented Generation", "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "MedINST: Meta Dataset of Biomedical Instructions", "Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters", "Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic", "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation", "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity", "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS", "Story Embeddings \u2013 Narrative-Focused Representations of Fictional Stories", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "Which questions should I answer? Salience Prediction of Inquisitive Questions", "Reformatted Alignment", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "TUTOR-ICL: Guiding Large Language Models for Improved In-Context Learning Performance", "MULTI-NEWS+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation", "Inference-Time Language Model Alignment via Integrated Value Guidance", "Aligning Large Language Models with Diverse Political Viewpoints", "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism", "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs", "Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization", "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion", "A Survey of AMR Applications", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "A Thorough Examination of Decoding Methods in the Era of LLMs", "Semformer: Transformer Language Models with Semantic Planning", "Event-Keyed Summarization", "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs", "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Attribute or Abstain: Large Language Models as Long Document Assistants", "Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models", "Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works", "ConTReGen: Context-driven Tree-structured Retrieval for Open-domain Long-form Text Generation", "Identifying Factual Inconsistencies in Summaries: Grounding LLM Inference via Task Taxonomy", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "LoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts", "Re-Evaluating Evaluation for Multilingual Summarization", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "LONGEMBED: Extending Embedding Models for Long Context Retrieval", "Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History", "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!", "A Critical Look at Meta-evaluating Summarisation Evaluation Metrics", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries"], "Machine Translation": ["Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level", "Back to School: Translation Using Grammar Books", "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing", "Datasets for Multilingual Answer Sentence Selection", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language", "Leveraging Grammar Induction for Language Understanding and Generation", "LLMs Are Zero-Shot Context-Aware Simultaneous Translators", "MiTTenS: A Dataset for Evaluating Gender Mistranslation", "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling", "Can Automatic Metrics Assess High-Quality Translations?", "Reconsidering Sentence-Level Sign Language Translation", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Distributional Properties of Subword Regularization", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "Revealing the Parallel Multilingual Learning within Large Language Models", "XTOWER: A Multilingual LLM for Explaining and Correcting Translation Errors", "Dual-teacher Knowledge Distillation for Low-frequency Word Translation", "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation", "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation", "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS", "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators", "Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs", "Analyzing Context Contributions in LLM-based Machine Translation", "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "Building Resources for Emakhuwa: Machine Translation and News Classification Benchmarks", "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models", "Pretraining Language Models Using Translationese", "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?", "An Audit on the Perspectives and Challenges of Hallucinations in NLP", "A Survey of AMR Applications", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Evaluating Automatic Metrics with Incremental Machine Translation Systems", "What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study", "Benchmarking Machine Translation with Cultural Awareness", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "A Deep Analysis of the Impact of Multiword Expressions and Named Entities on Chinese-English Machine Translations", "LexMatcher: Dictionary-centric Data Curation for LLM-based Machine Translation", "Word Alignment as Preference for Machine Translation", "Multi-Target Cross-Lingual Summarization: a novel task and a language-neutral approach", "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Lexically Grounded Subword Segmentation", "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation", "Aligning Translation-Specific Understanding to General Understanding in Large Language Models", "BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training", "Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing", "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation", "Improving Minimum Bayes Risk Decoding with Multi-Prompt", "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "Visual Pivoting Unsupervised Multimodal Machine Translation in Low-Resource Distant Language Pairs", "Representational Isomorphism and Alignment of Multilingual Large Language Models", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "Curriculum Consistency Learning for Conditional Sentence Generation", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics", "Cross-lingual Contextualized Phrase Retrieval", "On Creating an English-Thai Code-switched Machine Translation in Medical Domain", "Exploring Design Choices for Building Language-Specific LLMs", "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation", "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation", "Low-Resource Machine Translation through the Lens of Personalized Federated Learning", "Beyond Reference: Evaluating High Quality Translations Better than Human References", "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects"], "Contrastive Learning": ["An Instruction Tuning-Based Contrastive Learning Framework for Aspect Sentiment Quad Prediction with Implicit Aspects and Opinions", "Deciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Video-Text Prompting for Weakly Supervised Spatio-Temporal Video Grounding", "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs", "SignCLIP: Connecting Text and Sign Language by Contrastive Learning", "Vanessa : Visual Connotation and Aesthetic Attributes Understanding Network for Multimodal Aspect-based Sentiment Analysis", "Interpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding", "Pseudo-Label Enhanced Prototypical Contrastive Learning for Uniformed Intent Discovery", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "Unsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance", "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese", "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles", "Story Embeddings \u2013 Narrative-Focused Representations of Fictional Stories", "Cluster-Norm for Unsupervised Probing of Knowledge", "Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss", "BOOLQUESTIONS: Does Dense Retrieval Understand Boolean Logic in Language?", "Multi-label Sequential Sentence Classification via Large Language Model", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "Local Contrastive Editing of Gender Stereotypes", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP", "Improving Knowledge Graph Completion with Structure-Aware Supervised Contrastive Learning", "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation", "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "Applying Contrastive Learning to Code Vulnerability Type Classification", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "Cross-Lingual Multi-Hop Knowledge Editing", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "TOOLVERIFIER: Generalization to New Tools via Self-Verification", "CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "Contrastive Entity Coreference and Disambiguation for Historical Texts", "NCPrompt: NSP-Based Prompt Learning and Contrastive Learning for Implicit Discourse Relation Recognition", "A Simple Angle-based Approach for Contrastive Learning of Unsupervised Sentence Representation", "In-context Contrastive Learning for Event Causality Identification", "Learning to Correct for QA Reasoning with Black-box LLMs", "Document Hashing with Multi-Grained Prototype-Induced Hierarchical Generative Model", "Divide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning", "PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction", "A Generic Method for Fine-grained Category Discovery in Natural Language Texts", "Improving Factual Consistency of News Summarization by Contrastive Preference Optimization", "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality", "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free-Word-Ordered and Morphologically-Rich Low-Resource Languages", "Decoding the Echoes of Vision from fMRI: Memory Disentangling for Past Semantic Information", "Evolutionary Contrastive Distillation for Language Model Alignment", "Multi-Loss Fusion: Angular and Contrastive Integration for Machine-Generated Text Detection", "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs", "Representational Isomorphism and Alignment of Multilingual Large Language Models", "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval", "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation", "VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models", "Updating CLIP to Prefer Descriptions Over Captions", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement", "QUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware", "Cross-lingual Contextualized Phrase Retrieval", "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!", "Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System", "Nearest Neighbor Normalization Improves Multimodal Retrieval", "DC-Instruct: An Effective Framework for Generative Multi-intent Spoken Language Understanding", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "CodeFort: Robust Training for Code Generation Models", "MiniConGTS: A Near Ultimate Minimalist Contrastive Grid Tagging Scheme for Aspect Sentiment Triplet Extraction", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "C-ICL: Contrastive In-context Learning for Information Extraction"], "Sentiment Analysis": ["Rethinking the Evaluation of In-Context Learning for LLMs", "An Instruction Tuning-Based Contrastive Learning Framework for Aspect Sentiment Quad Prediction with Implicit Aspects and Opinions", "The Overlooked Repetitive Lengthening Form in Sentiment Analysis", "Quantum Recurrent Architectures for Text Classification", "Crisis counselor language and perceived genuine concern in crisis conversations", "Functionality learning through specification instructions", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training", "DA\u00b3: A Distribution-Aware Adversarial Attack against Language Models", "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study", "Locating Information Gaps and Narrative Inconsistencies Across Languages: A Case Study of LGBT People Portrayals on Wikipedia", "MOSEL: Inference Serving Using Dynamic Modality Selection", "CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "DADEE: Unsupervised Domain Adaptation in Early Exit PLMS", "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates", "Measuring the Robustness of NLP Models to Domain Shifts", "Does Large Language Model Contain Task-Specific Neurons?", "Semantics and Sentiment: Cross-lingual Variations in Emoji Use", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Revisiting Supervised Contrastive Learning for Microblog Classification", "BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla", "Regression-aware Inference with LLMs", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "ASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction", "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists", "Modeling News Interactions and Influence for Financial Market Prediction", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Inference and Verbalization Functions During In-Context Learning", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "\u201cWe Demand Justice!\u201d: Towards Social Context Grounding of Political Texts", "LoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Transfer Learning for Text Classification via Model Risk Analysis", "Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis", "CROWD: Certified Robustness via Weight Distribution for Smoothed Classifiers against Backdoor Attack", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "RepMatch: Quantifying Cross-Instance Similarities in Representation Space", "ALVIN: Active Learning Via INterpolation", "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification", "Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research", "A Survey on Natural Language Counterfactual Generation", "Worry Words: Norms of Anxiety Association for over 44k English Words", "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters", "Exploring Design Choices for Building Language-Specific LLMs", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI", "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "Latent Concept-based Explanation of NLP Models"], "Language Modeling": ["Evaluating n-Gram Novelty of Language Models Using RUSTY-DAWG", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Teaching LLMs to Abstain across Languages via Multilingual Feedback", "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models", "Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS", "Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives", "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models", "KidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions", "Leading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Probabilities", "Can Large Language Models Learn Independent Causal Mechanisms?", "Scaling Laws for Linear Complexity Language Models", "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling", "GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients", "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling", "Generation with Dynamic Vocabulary", "Can Transformers Learn n-gram Language Models?", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "Chain and Causal Attention for Efficient Entity Tracking", "LONGHEADS: Multi-Head Attention is Secretly a Long Context Processor", "Calibrating Language Models with Adaptive Temperature Scaling", "Tending Towards Stability: Convergence Challenges in Small Language Models", "Stable Language Model Pre-training by Reducing Embedding Variability", "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages", "Lifelong Event Detection via Optimal Transport", "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Securing Multi-turn Conversational Language Models From Distributed Backdoor Triggers", "Rethinking Token Reduction for State Space Models", "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models", "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "Do LLMs learn a true syntactic universal?", "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "Semformer: Transformer Language Models with Semantic Planning", "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training", "Language models and brains align due to more than next-word prediction and word-level information", "Is Child-Directed Speech Effective Training Data for Language Models?", "On the token distance modeling ability of higher RoPE attention dimension", "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation", "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models", "PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "Turn Waste into Worth: Rectifying Top-k Router of MoE", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "Extending Context Window of Large Language Models from a Distributional Perspective", "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models", "Transformers are Multi-State RNNS", "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules", "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "Generalized Measures of Anticipation and Responsivity in Online Language Processing", "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning", "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection", "Can Textual Unlearning Solve Cross-Modality Safety Alignment?", "Demystifying Verbatim Memorization in Large Language Models", "Target-Aware Language Modeling via Granular Data Sampling", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "Empirical Prior for Text Autoencoders", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper", "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models", "Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens", "How to Compute the Probability of a Word", "Instruction Pre-Training: Language Models are Supervised Multitask Learners"], "Text Generation": ["Precise Model Benchmarking with Only a Few Observations", "Evaluating n-Gram Novelty of Language Models Using RUSTY-DAWG", "Reverse-Engineering the Reader", "Enable Fast Sampling for Seq2Seq Text Diffusion", "Control Large Language Models via Divide and Conquer", "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "Adversarial Text Generation using Large Language Models for Dementia Detection", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "Prompts have evil twins", "KidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions", "Scaling Laws for Linear Complexity Language Models", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements", "Preference-Guided Reflective Sampling for Aligning Language Models", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Empirical Prior for Text Autoencoders", "An Empirical Analysis of the Writing Styles of Persona-Assigned LLMs", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer", "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains", "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic", "Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM", "Atomic Self-Consistency for Better Long Form Generations", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "Style-Specific Neurons for Steering LLMs in Text Style Transfer", "Enhancing High-order Interaction Awareness in LLM-based Recommender Model", "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards", "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models", "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation", "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?", "DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing", "TARA: Token-level Attribute Relation Adaptation for Multi-Attribute Controllable Text Generation", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "Evaluating Large Language Models via Linguistic Profiling", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation", "PERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts", "ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings", "Analysis of Plan-based Retrieval for Grounded Text Generation", "Contextualized Graph Representations for Generating Counter-Narratives against Hate Speech", "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "Extrinsic Evaluation of Cultural Competence in Large Language Models", "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models", "Transformers are Multi-State RNNS", "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules", "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions", "Breaking the Boundaries: A Unified Framework for Chinese Named Entity Recognition Across Text and Speech", "Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments", "Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood", "Improving Minimum Bayes Risk Decoding with Multi-Prompt", "Filtered Direct Preference Optimization", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "SPECIALEX: A Benchmark for In-Context Specialized Lexicon Learning", "Are LLMs Aware that Some Questions are not Open-ended?", "Local and Global Decoding in Text Generation", "Downstream Trade-offs of a Family of Text Watermarks", "Achieving Stronger Generation via Simple Contrastive Tuning", "TAB2TEXT - A framework for deep learning with tabular data", "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation", "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs", "LongForm: Effective Instruction Tuning with Reverse Instructions"], "Interpretability": ["Interpretability-based Tailored Knowledge Editing in Transformers", "Rethinking the Role of Proxy Rewards in Language Model Alignment", "Efficient and Interpretable Grammatical Error Correction with Mixture of Experts", "Information Flow Routes: Automatically Interpreting Language Models at Scale", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "Adversarial Text Generation using Large Language Models for Dementia Detection", "StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements", "Backward Lens: Projecting Language Model Gradients into the Vocabulary Space", "Interpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding", "NALA: an Effective and Interpretable Entity Alignment Method", "Metrics for What, Metrics for Whom: Assessing Actionability of Bias Evaluation Metrics in NLP\\", "XTOWER: A Multilingual LLM for Explaining and Correcting Translation Errors", "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "Does Large Language Model Contain Task-Specific Neurons?", "Dual Process Masking for Dialogue Act Recognition", "Activation Scaling for Steering and Interpreting Language Models", "Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning", "GREEN: Generative Radiology Report Evaluation and Error Notation", "Variational Language Concepts for Interpreting Foundation Language Models", "DIVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions", "PANDA: Persona Attributes Navigation for Detecting and Alleviating Overuse Problem in Large Language Models", "Analyzing Context Contributions in LLM-based Machine Translation", "Atomic Inference for NLI with Generated Facts as Atoms", "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists", "Improving LLM Attributions with Randomized Path-Integration", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Knowledge Mechanisms in Large Language Models: A Survey and Perspective", "Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals", "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP", "IntCoOp: Interpretability-Aware Vision-Language Prompt Tuning", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions", "Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation", "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models", "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning", "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings", "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model", "MGCL: Multi-Granularity Clue Learning for Emotion-Cause Pair Extraction via Cross-Grained Knowledge Distillation", "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models", "Learning Personalized Alignment in Evaluating Open-ended Text Generation", "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning", "Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability", "Towards Explainable Computerized Adaptive Testing with Large Language Model", "An LLM Feature-based Framework for Dialogue Constructiveness Assessment", "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation", "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions", "Bias Wipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability", "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence", "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation", "Unveiling the Role of Pretraining in Direct Speech Translation", "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP", "MedCoT: Medical Chain of Thought via Hierarchical Expert", "Updating CLIP to Prefer Descriptions Over Captions", "Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes", "Low-Resource Machine Translation through the Lens of Personalized Federated Learning", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "Learning to Generate Rules for Realistic Few-Shot Relation Classification: An Encoder-Decoder Approach", "Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics", "Robust Text Classification: Analyzing Prototype-Based Networks", "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective", "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries", "Taxonomy-guided Semantic Indexing for Academic Paper Search", "Toward Compositional Behavior in Neural Models: A Survey of Current Views", "Exploring Intra and Inter-language Consistency in Embeddings with ICA", "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models", "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models", "FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document", "Latent Concept-based Explanation of NLP Models"], "Natural Language Inference": ["Self-Contradictory Reasoning Evaluation and Detection", "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study", "How Hard is this Test Set?\nNLI Characterization by Exploiting Training Dynamics", "Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "DADEE: Unsupervised Domain Adaptation in Early Exit PLMS", "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals", "\u201cSeeing the Big through the Small\u201d: Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "ECON: On the Detection and Resolution of Evidence Conflicts", "Scalable and Domain-General Abstractive Proposition Segmentation", "Revealing the Parallel Multilingual Learning within Large Language Models", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "Inference Helps PLMs' Conceptual Understanding: Improving the Abstract Inference Ability with Hierarchical Conceptual Entailment Graphs", "Measuring the Robustness of NLP Models to Domain Shifts", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "How Do Humans Write Code? Large Models Do It the Same Way Too", "Improve Dense Passage Retrieval with Entailment Tuning", "Atomic Inference for NLI with Generated Facts as Atoms", "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets", "VerifyMatch: A Semi-Supervised Learning Paradigm for Natural Language Inference with Confidence-Aware MixUp", "Inference and Verbalization Functions During In-Context Learning", "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models", "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars", "CONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules", "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Event-Keyed Summarization", "QUDSELECT: Selective Decoding for Questions Under Discussion Parsing", "CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models", "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic", "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference", "How Entangled is Factuality and Deception in German?", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "Attribute or Abstain: Large Language Models as Long Document Assistants", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "GottBERT: a pure German Language Model", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving", "In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search", "A Morphology-Based Investigation of Positional Encodings", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference", "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models", "ALVIN: Active Learning Via INterpolation", "Identifying Factual Inconsistencies in Summaries: Grounding LLM Inference via Task Taxonomy", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "A Survey on Natural Language Counterfactual Generation", "Multilingual Fine-Grained News Headline Hallucination Detection", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "Latent Concept-based Explanation of NLP Models"], "Commonsense Reasoning": ["Guided Knowledge Generation with Language Models for Commonsense Reasoning", "Large Language Models Can Self-Correct with Key Condition Verification", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "Commonsense Knowledge Editing Based on Free-Text in LLMs", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "Scaling Laws for Linear Complexity Language Models", "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "Retrieved In-Context Principles from Previous Mistakes", "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning", "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "Investigating Mysteries of CoT-Augmented Distillation", "Learning to Paraphrase for Alignment with LLM Preference", "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs", "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "Zero-shot Commonsense Reasoning over Machine Imagination", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Mixture-of-Subspaces in Low-Rank Adaptation", "A Thorough Examination of Decoding Methods in the Era of LLMs", "ABSEval: An Agent-based Framework for Script Evaluation", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "Re-Reading Improves Reasoning in Large Language Models", "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training", "Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "Nash CoT: Multi-Path Inference with Preference Equilibrium", "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning", "Focused Large Language Models are Stable Many-Shot Learners", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning", "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via LLM-based Theory Resolution", "Teaching Small Language Models Reasoning through Counterfactual Distillation", "PizzaCommonSense: Learning to Model Commonsense Reasoning about Intermediate Steps in Cooking Recipes", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Exploring Design Choices for Building Language-Specific LLMs", "Large Language Models are In-context Teachers for Knowledge Reasoning", "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering", "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning", "MUSCLE: A Model Update Strategy for Compatible LLM Evolution", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation"], "Dataset": ["Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets", "MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "FOLIO: Natural Language Reasoning with First-Order Logic", "COCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds", "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures", "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization", "TextLap: Customizing Language Models for Text-to-Layout Planning", "MiTTenS: A Dataset for Evaluating Gender Mistranslation", "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models", "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion", "\"I Never Said That\": A dataset, taxonomy and baselines on response clarity classification", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "ALANAVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding", "C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits", "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles", "Unveiling the Invisible: Captioning Videos with Metaphors", "CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems", "BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla", "OEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary", "Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS", "Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations", "MDCR: A Dataset for Multi-Document Conditional Reasoning", "DocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing", "Leveraging Conflicts in Social Media Posts: Unintended Offense Dataset", "ArMeme: Propagandistic Content in Arabic Memes", "CoCoHD: Congress Committee Hearing Dataset", "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models", "Multilingual Topic Classification in X: Dataset and Analysis", "CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research", "Towards Explainable Chinese Native Learner Essay Fluency Assessment: Dataset, Tasks, and Method", "Evaluating Gender Bias of LLMs in Making Morality Judgements", "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains", "Ukrainian Resilience: A Dataset for Detection of Help-Seeking Signals Amidst the Chaos of War", "ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "CELLO: Causal Evaluation of Large Vision-Language Models", "Exploring the Capability of Multimodal LLMs with Yonkoma Manga: The YManga dataset and Its Challenging Tasks", "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language Story Understanding", "ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos", "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "STORYSUMM: Evaluating Faithfulness in Story Summarization", "Towards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs", "Android in the Zoo: Chain-of-Action-Thought for GUI Agents", "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence", "Shoes-ACOSI: A Dataset for Aspect-Based Sentiment Analysis with Implicit Opinion Extraction", "BOOKWORM: A Dataset for Character Description and Analysis", "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays", "Designing Logic Pattern Templates for Counter-Argument Logical Structure Analysis", "Emosical: An Emotion-Annotated Musical Theatre Dataset", "TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners", "Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments", "ESG-Kor: A Korean Dataset for ESG-related Information Extraction and Practical Use Cases", "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions", "MEDREADME: A Systematic Study for Fine-grained Sentence Readability in Medical Domain"], "Visual QA": ["M5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models", "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models", "Towards One-to-Many Visual Question Answering", "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering", "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems", "CommVQA: Situating Visual Question Answering in Communicative Contexts", "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP", "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "In-Context Compositional Generalization for Large Vision-Language Models", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts", "Tag-grounded Visual Instruction Tuning with Retrieval Augmentation", "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "Zero-shot Commonsense Reasoning over Machine Imagination", "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models", "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "Query-based Cross-Modal Projector Bolstering Mamba Multimodal LLM", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering", "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "Benchmarking Vision Language Models for Cultural Understanding", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "\u201cWhat is the value of {templates}?\u201d Rethinking Document Information Extraction Datasets for LLMs", "Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models", "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM", "Diversity, Rationalize, and Combine: Ensembling Multiple QA Strategies for Zero-shot Knowledge-based VQA", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "Why do LLaVA Vision-Language Models Reply to Images in English?", "Game on Tree: Visual Hallucination Mitigation via Coarse-to-Fine View Tree and Game Theory", "VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "Learning to Ask Denotative and Connotative Questions for Knowledge-based VQA", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "A Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect", "Attribute Diversity Determines the Systematicity Gap in VQA", "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis", "Multi-Level Information Retrieval Augmented Generation for Knowledge-based Visual Question Answering", "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments", "PropTest: Automatic Property Testing for Improved Visual Programming", "Nearest Neighbor Normalization Improves Multimodal Retrieval", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs"], "Vision-language Models": ["ModSCAN: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities", "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture", "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?", "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP", "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models", "Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement", "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison", "CommVQA: Situating Visual Question Answering in Communicative Contexts", "GRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "Interpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification", "Improving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts", "Unveiling the Invisible: Captioning Videos with Metaphors", "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models", "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning", "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables", "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models", "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP", "IntCoOp: Interpretability-Aware Vision-Language Prompt Tuning", "MMOE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts", "Towards Low-Resource Harmful Meme Detection with LMM Agents", "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering", "ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective", "Benchmarking Vision Language Models for Cultural Understanding", "CELLO: Causal Evaluation of Large Vision-Language Models", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "Enhancing Advanced Visual Reasoning Ability of Large Language Models", "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality", "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding", "Why do LLaVA Vision-Language Models Reply to Images in English?", "VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning", "FAITHSCORE: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models", "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners", "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis", "Is GPT-4V (ision) All You Need for Automating Academic Data Visualization? Exploring Vision-Language Models' Capability in Reproducing Academic Charts", "Can Textual Unlearning Solve Cross-Modality Safety Alignment?", "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models", "Retrieval-enriched zero-shot image classification in low-resource domains", "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models", "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models"], "Reinforcement Learning": ["RaFe: Ranking Feedback Improves Query Rewriting for RAG", "Reward Difference Optimization For Sample Reweighting In Offline RLHF", "Towards Aligning Language Models with Textual Feedback", "E2CL: Exploration-based Error Correction Learning for Embodied Agents", "Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives", "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback", "Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models", "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction", "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "Mitigating Open-Vocabulary Caption Hallucinations", "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents", "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic", "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain", "Outcome-Constrained Large Language Models for Countering Hate Speech", "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback", "How Do Humans Write Code? Large Models Do It the Same Way Too", "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback", "Large Language Model-based Human-Agent Collaboration for Complex Task Solving", "Enhancing Agent Learning through World Dynamics Modeling", "Step-level Value Preference Optimization for Mathematical Reasoning", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "VIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation", "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards", "TRIP NEGOTIATOR: A Travel Persona-aware Reinforced Dialogue Generation Model for Personalized Integrative Negotiation in Tourism", "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion", "METAREFLECTION: Learning Instructions for Language Agents using Past Reflections", "ORPO: Monolithic Preference Optimization without Reference Model", "A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies", "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment", "Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue", "Temporally Consistent Factuality Probing for Large Language Models", "Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation", "COFFEE-GYM: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code", "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use", "Mitigating the Alignment Tax of RLHF", "Exploiting Careful Design of SVM Solution for Aspect-term Sentiment Analysis", "Improving Multi-party Dialogue Generation via Topic and Rhetorical Coherence", "Learning to Retrieve Iteratively for In-Context Learning", "F2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation", "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "Learning Autonomous Driving Tasks via Human Feedbacks with Large Language Models", "Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation", "MIXTURE-OF-SKILLS: Learning to Optimize Data Usage for Fine-Tuning Large Language Models", "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models without Preference Data", "Grounding Language in Multi-Perspective Referential Communication", "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension", "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "Direct Multi-Turn Preference Optimization for Language Agents", "Bootstrapped Policy Learning for Task-oriented Dialogue through Goal Shaping", "Be Helpful but Don't Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support", "ABLE: Personalized Disability Support with Politeness and Empathy Integration"], "Instruction Following": ["FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition", "Automatic Instruction Evolving for Large Language Models", "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment", "Preference-Guided Reflective Sampling for Aligning Language Models", "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation", "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?", "Dual-Space Knowledge Distillation for Large Language Models", "Nebula: A Discourse-Aware Minecraft Builder", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models", "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding", "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG", "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation", "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets", "WPO: Enhancing RLHF with Weighted Preference Optimization", "BLSP-Emo: Towards Empathetic Large Speech-Language Models", "Inference-Time Language Model Alignment via Integrated Value Guidance", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "A Thorough Examination of Decoding Methods in the Era of LLMs", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation\\", "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course", "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "RULER: A Model-Agnostic Method to Control Generated Length for Large Language Models", "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection", "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks", "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs", "Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors", "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models", "Resilience of Large Language Models for Noisy Instructions", "How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection", "Achieving Stronger Generation via Simple Contrastive Tuning", "Direct Multi-Turn Preference Optimization for Language Agents", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "LongForm: Effective Instruction Tuning with Reverse Instructions"], "Benchmarksing": ["Precise Model Benchmarking with Only a Few Observations", "Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets", "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "FOLIO: Natural Language Reasoning with First-Order Logic", "Revisiting Automated Evaluation for Long-form Table Question Answering", "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities", "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification", "METAKP: On-Demand Keyphrase Generation", "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation", "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks", "Authorship Obfuscation in Multilingual Machine-Generated Text Detection", "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models", "ASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?", "DocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing", "ANALOBENCH: Benchmarking the Identification of Abstract and Long-context Analogies", "On the Fragility of Active Learners for Text Classification", "BLADE: Benchmarking Language Model Agents for Data-Driven Science", "On Leakage of Code Generation Evaluation Datasets", "Social Bias Probing: Fairness Benchmarking for Language Models", "CONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules", "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs", "Towards Robust Evaluation of Unlearning in LLMs via Data Transformations", "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "Large Language Models Can Be Contextual Privacy Protection Learners", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "MINERS: Multilingual Language Models as Semantic Retrievers", "Benchmarking Vision Language Models for Cultural Understanding", "A Notion of Complexity for Theory of Mind via Discrete World Models", "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language Story Understanding", "FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents", "An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making", "What Are the Odds? Language Models Are Capable of Probabilistic Reasoning", "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model", "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection", "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "SCIAGENT: Tool-augmented Language Models for Scientific Reasoning", "NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning", "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems", "AppBench: Planning of Multiple APIs from Various APPs for Complex User Instruction", "Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets", "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models", "FROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation"], "Alignment": ["Let Me Teach You: Pedagogical Foundations of Feedback for Language Models", "ARM: An Alignment-and-Replacement Module for Chinese Spelling Check Based on LLMs", "Towards Aligning Language Models with Textual Feedback", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States", "LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification", "Enhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing", "Reward Modeling Requires Automatic Adjustment Based on Data Quality", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "On the Relationship between Truth and Political Bias in Language Models", "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment", "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness", "Towards Tool Use Alignment of Large Language Models", "Alignment-Enhanced Decoding: Defending Jailbreaks via Token-Level Adaptive Refining of Probability Distributions", "A SMART Mnemonic Sounds like \u201cGlue Tonic\": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick", "Reformatted Alignment", "WPO: Enhancing RLHF with Weighted Preference Optimization", "Scaling Sentence Embeddings with Large Language Models", "Do LLMs Know to Respect Copyright Notice?", "Learning to Paraphrase for Alignment with LLM Preference", "FACTALIGN: Long-form Factuality Alignment of Large Language Models", "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence", "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations", "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline", "ORPO: Monolithic Preference Optimization without Reference Model", "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance", "A Simple Angle-based Approach for Contrastive Learning of Unsupervised Sentence Representation", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "Aligners: Decoupling LLMs and Alignment", "Learning Personalized Alignment in Evaluating Open-ended Text Generation", "Semi-Supervised Reward Modeling via Iterative Self-Training", "Aligning Language Models to Explicitly Handle Ambiguity", "Leveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training", "HealthAlignSumm: Utilizing Alignment for Multimodal Summarization of Code-Mixed Healthcare Dialogues", "MathFish: Evaluating Language Model Math Reasoning via Grounding in Educational Curricula", "Better Alignment with Instruction Back-and-Forth Translation", "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "Self-Evolution Fine-Tuning for Policy Optimization", "MDPO: Conditional Preference Optimization for Multimodal Large Language Models", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "Visual Pivoting Unsupervised Multimodal Machine Translation in Low-Resource Distant Language Pairs", "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "ADELIE: Aligning Large Language Models on Information Extraction", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness"], "Chain-of-thought": ["Guided Knowledge Generation with Language Models for Commonsense Reasoning", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering", "INDUCT-LEARN: Short Phrase Prompting with Instruction Induction", "Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text", "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "XTOWER: A Multilingual LLM for Explaining and Correcting Translation Errors", "Sequential API Function Calling Using GraphQL Schema", "Reference-based Metrics Disprove Themselves in Question Generation", "Mixed Distillation Helps Smaller Language Models Reason Better", "An Empirical Study of Multilingual Reasoning Distillation for Question Answering", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models", "How Do Humans Write Code? Large Models Do It the Same Way Too", "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback", "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models", "MULTI-NEWS+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "Reasoning Robustness of LLMs to Adversarial Typographical Errors", "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences", "Few shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity", "Tree of Problems: Improving structured problem solving with compositionality", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMS", "HalluMeasure: Fine-grained Hallucination Measurement Using Chain-of-Thought Reasoning", "Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning", "Teaching Small Language Models Reasoning through Counterfactual Distillation", "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "Abstraction-of-Thought Makes Language Models Better Reasoners", "Understanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs", "Is GPT-4V (ision) All You Need for Automating Academic Data Visualization? Exploring Vision-Language Models' Capability in Reproducing Academic Charts", "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach"], "Low-resource Languages": ["Back to School: Translation Using Grammar Books", "M5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks", "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing", "Targeted Multilingual Adaptation for Low-resource Language Families", "Teaching LLMs to Abstain across Languages via Multilingual Feedback", "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "Datasets for Multilingual Answer Sentence Selection", "Representation Alignment and Adversarial Networks for Cross-lingual Dependency Parsing", "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages", "How Does Quantization Affect Multilingual LLMs?", "The Zeno's Paradox of \u2018Low-Resource' Languages", "When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS", "To Err Is Human, but Llamas Can Learn It Too", "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "Are Modern Neural ASR Architectures Robust for Polysynthetic Languages?", "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models", "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "Pretraining Language Models Using Translationese", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "GlossLM: A Massively Multilingual Corpus and Pretrained Model for Interlinear Glossed Text", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "MINERS: Multilingual Language Models as Semantic Retrievers", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Unlocking the Potential of Model Merging for Low-Resource Languages", "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization", "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese", "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection", "Can we teach language models to gloss endangered languages?", "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free-Word-Ordered and Morphologically-Rich Low-Resource Languages", "RAR: Retrieval-augmented retrieval for code generation in low-resource languages", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "An Analysis of Multilingual FActScore", "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization", "Table Question Answering for Low-resourced Indic Languages", "DICTDIS: Dictionary Constrained Disambiguation for Improved NMT", "IndoCL: Benchmarking Indonesian Language Development Assessment", "Exploring Design Choices for Building Language-Specific LLMs", "Using Language Models to Disambiguate Lexical Choices in Translation", "Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "Low-Resource Machine Translation through the Lens of Personalized Federated Learning"], "Transfer Learning": ["Efficient and Interpretable Grammatical Error Correction with Mixture of Experts", "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging", "Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents", "Datasets for Multilingual Answer Sentence Selection", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging", "Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "M2QA: Multi-domain Multilingual Question Answering", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "Representation Alignment and Adversarial Networks for Cross-lingual Dependency Parsing", "Causal Discovery Inspired Unsupervised Domain Adaptation for Emotion-Cause Pair Extraction", "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "Dual-teacher Knowledge Distillation for Low-frequency Word Translation", "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases", "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning", "Scaling Sentence Embeddings with Large Language Models", "SEG2ACT: Global Context-aware Action Generation for Document Logical Structuring", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "Concept Space Alignment in Multilingual LLMs", "Audio-Based Linguistic Feature Extraction for Enhancing Multi-lingual and Low-Resource Text-to-Speech", "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "Boosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?", "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "Aligners: Decoupling LLMs and Alignment", "Data Contamination Can Cross Language Barriers", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning", "GottBERT: a pure German Language Model", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models", "LaRA: Large Rank Adaptation for Speech and Text Cross-Modal Learning in Large Language Models", "A Survey on In-context Learning", "Transfer Learning for Text Classification via Model Risk Analysis", "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation", "RepMatch: Quantifying Cross-Instance Similarities in Representation Space", "Reusing Transferable Weight Increments for Low-resource Style Generation", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Table Question Answering for Low-resourced Indic Languages", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data"], "Hallucination": ["Unified Active Retrieval for Retrieval Augmented Generation", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?", "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models", "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering", "Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration", "When Context Leads but Parametric Memory Follows in Large Language Models", "Factuality of Large Language Models: A Survey", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Navigating Hallucinations for Reasoning of Unintentional Activities", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "Learning Semantic Structure through First-Order-Logic Translation", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales", "Knowledge-Centric Hallucination Detection", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations", "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation", "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs", "An Audit on the Perspectives and Challenges of Hallucinations in NLP", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses", "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity", "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models", "Word Alignment as Preference for Machine Translation", "Attribute or Abstain: Large Language Models as Long Document Assistants", "Analysis of Plan-based Retrieval for Grounded Text Generation", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models", "On the Universal Truthfulness Hyperplane Inside LLMS", "FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping", "FAITHSCORE: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models", "Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via LLM-based Theory Resolution", "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?", "MDPO: Conditional Preference Optimization for Multimodal Large Language Models", "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation", "VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts", "Do Large Language Models Know How Much They Know?", "Are LLMs Aware that Some Questions are not Open-ended?", "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models", "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "Improving Factual Consistency of News Summarization by Contrastive Preference Optimization", "Self-training Large Language Models through Knowledge Detection"], "Code Generation": ["SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "Automatic Instruction Evolving for Large Language Models", "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing", "GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation", "Free your mouse! Command Large Language Models to Generate Code to Format Word Documents", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision", "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation", "Sequential API Function Calling Using GraphQL Schema", "CODEIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code", "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity", "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation", "Large Language Model-based Human-Agent Collaboration for Complex Task Solving", "EvOR: Evolving Retrieval for Code Generation", "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "On Leakage of Code Generation Evaluation Datasets", "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "Revisiting the Impact of Pursuing Modularity for Code Generation", "CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research", "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "One-to-many testing for code generation from (just) natural language", "DocCGen: Document-based Controlled Code Generation", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft", "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with Really Good Data", "CODEJUDGE: Evaluating Code Generation with Large Language Models", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "RAR: Retrieval-augmented retrieval for code generation in low-resource languages", "Improving Minimum Bayes Risk Decoding with Multi-Prompt", "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models", "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "Is GPT-4V (ision) All You Need for Automating Academic Data Visualization? Exploring Vision-Language Models' Capability in Reproducing Academic Charts", "Learning Autonomous Driving Tasks via Human Feedbacks with Large Language Models", "CodeFort: Robust Training for Code Generation Models", "PropTest: Automatic Property Testing for Improved Visual Programming", "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization"], "Information Retrieval": ["MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval", "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation", "Scaling Laws for Linear Complexity Language Models", "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "Dense X Retrieval: What Retrieval Granularity Should We Use?", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "Bridging Local Details and Global Context in Text-Attributed Graphs", "MedAdapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning", "Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell", "Unifying Multimodal Retrieval via Document Screenshot Embedding", "HyQE: Ranking Contexts with Hypothetical Query Embeddings", "Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism", "Cross-Lingual Multi-Hop Knowledge Editing", "Exploring the Practicality of Generative Retrieval on Dynamic Corpora", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "M3SCIQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents", "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "Document Hashing with Multi-Grained Prototype-Induced Hierarchical Generative Model", "GENRA: Enhancing Zero-shot Retrieval with Rank Aggregation", "DocCGen: Document-based Controlled Code Generation", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "One Thousand and One Pairs: A \u201cnovel\u201d challenge for long-context language models", "LitSearch: A Retrieval Benchmark for Scientific Literature Search", "Evaluating D-MERIT of Partial-annotation on Information Retrieval", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction", "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems", "Revisiting Query Variation Robustness of Transformer Models", "Do Large Language Models Know How Much They Know?", "Exploring the Best Practices of Query Expansion with Large Language Models", "STARD: A Chinese Statute Retrieval Dataset Derived from Real-life Queries by Non-professionals", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models", "Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval", "FIRST: Faster Improved Listwise Reranking with Single Token Decoding"], "Parameter-efficient Fine-tuning": ["ADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers", "Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "Fast Forwarding Low-Rank Training", "OPEN-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation", "Efficient Active Learning with Adapters", "TL-CL: Task And Language Incremental Continual Learning", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "Mixture-of-Subspaces in Low-Rank Adaptation", "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning", "PERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "LoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation", "Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune", "RoQLlama: A Lightweight Romanian Adapted Language Model", "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning", "Achieving Stronger Generation via Simple Contrastive Tuning", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach"], "Bias": ["Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection", "Towards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey", "\u201cThey are uncultured\u201d: Unveiling Covert Harms and Social Threats in LLM Generated Conversations", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "MiTTenS: A Dataset for Evaluating Gender Mistranslation", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "SOCIALGAZE: Improving the Integration of Human Social Norms in Large Language Models", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "AGENTREVIEW: Exploring Peer Review Dynamics with LLM Agents", "Gender Identity in Pretrained Language Models: An Inclusive Approach to Data Creation and Probing", "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge", "Look Who's Talking Now: Covert Channels From Biased LLMs", "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "Understanding Slang with LLMs: Modelling Cross-Cultural Nuances through Paraphrasing", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis", "Systematic Biases in LLM Simulations of Debates", "Moral Foundations of Large Language Models", "Please note that I'm just an AI: Analysis of Behavior Patterns of LLMs in (Non-)offensive Speech Identification", "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse", "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.", "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric", "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "SAFETY-J: Evaluating Safety with Critique", "Voices in a Crowd: Searching for Clusters of Unique Perspectives", "BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers", "Evaluating Biases in Context-Dependent Sexual and Reproductive Health Questions", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models", "Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models", "How Susceptible are Large Language Models to Ideological Manipulation?", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models", "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps"], "Synthetic Data": ["Effective Synthetic Data and Test-Time Adaptation for OCR Correction", "Personas as a Way to Model Truthfulness in Language Models", "LLMs Are Prone to Fallacies in Causal Inference", "Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "On the In-context Generation of Language Models", "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "CACTUS: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory", "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "Scalable and Domain-General Abstractive Proposition Segmentation", "Emoknob: Enhance Voice Cloning with Fine-Grained Emotion Control", "LONGGENBENCH: Long-context Generation Benchmark", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models", "To Err Is Human, but Llamas Can Learn It Too", "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts", "Scaling Properties of Speech Language Models", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "Advancing Large Language Model Attribution through Self-Improving", "CareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation", "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents", "Multimodal Misinformation Detection by Learning from Synthetic Data with Multimodal LLMs", "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?", "Pretraining Language Models Using Translationese", "Precision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model", "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch", "Altogether: Image Captioning via Re-aligning Alt-text", "TOOLVERIFIER: Generalization to New Tools via Self-Verification", "Is Child-Directed Speech Effective Training Data for Language Models?", "Private prediction for large-scale synthetic text generation", "Aligners: Decoupling LLMs and Alignment", "RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment", "Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs", "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?", "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring", "Better Alignment with Instruction Back-and-Forth Translation", "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models", "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "Enhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives"], "Mathematical Reasoning": ["Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "Automatic Instruction Evolving for Large Language Models", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "Weak-to-Strong Reasoning", "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information", "How Does Quantization Affect Multilingual LLMs?", "Exploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions", "Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "LM\u00b2: A Simple Society of Language Models Solves Complex Reasoning", "Revealing the Parallel Multilingual Learning within Large Language Models", "Mixed Distillation Helps Smaller Language Models Reason Better", "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "Retrieved In-Context Principles from Previous Mistakes", "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts", "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS", "How Do Humans Write Code? Large Models Do It the Same Way Too", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "Reformatted Alignment", "Self-Consistency Boosts Calibration for Math Reasoning", "Step-level Value Preference Optimization for Mathematical Reasoning", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "Enhancing Arguments Recognition for Financial Mathematical Reasoning over Hybrid Data", "AUTODETECT: Towards a Unified Framework for Automated Weakness Detection in Large Language Models", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "Learning to Plan by Updating Natural Language", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "Table Question Answering for Low-resourced Indic Languages", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "FROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation"], "Explainability": ["The Overlooked Repetitive Lengthening Form in Sentiment Analysis", "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP", "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States", "CURE: Context- and Uncertainty-Aware Mental Disorder Detection", "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study", "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization", "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "PANDA: Persona Attributes Navigation for Detecting and Alleviating Overuse Problem in Large Language Models", "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales", "Improving LLM Attributions with Randomized Path-Integration", "Zero-Shot Fact Verification via Natural Logic and Large Language Models", "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation", "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze", "Local Contrastive Editing of Gender Stereotypes", "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs", "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective for Molecular Property Prediction", "Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals", "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models", "LLM Explainability via Attributive Masking Learning", "Towards Explainable Chinese Native Learner Essay Fluency Assessment: Dataset, Tasks, and Method", "On Evaluating Explanation Utility for Human-AI Decision Making in NLP", "Denoising Rationalization for Multi-hop Fact Verification via Multi-granular Explainer", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records", "Towards Explainable Computerized Adaptive Testing with Large Language Model", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "PE: A Poincare Explanation Method for Fast Text Hierarchy Generation", "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring", "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "Can Large Language Models Identify Authorship?", "A Survey on Natural Language Counterfactual Generation", "PepRec: Progressive Enhancement of Prompting for Recommendation", "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors", "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "Latent Concept-based Explanation of NLP Models"], "Evaluation Metrics": ["Evaluating Readability and Faithfulness of Concept-based Explanations", "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language", "APPLS: Evaluating Evaluation Metrics for Plain Language Summarization", "Recent Trends in Linear Text Segmentation: A Survey", "Efficiently Computing Susceptibility to Context in Language Models", "Can Automatic Metrics Assess High-Quality Translations?", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation", "Scalable and Domain-General Abstractive Proposition Segmentation", "Emoknob: Enhance Voice Cloning with Fine-Grained Emotion Control", "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?", "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation", "Information Parity: Measuring and Predicting the Multilingual Capabilities of Language Models", "Regression-aware Inference with LLMs", "I Could've Asked That: Reformulating Unanswerable Questions", "What's under the hood: Investigating Automatic Metrics on Meeting Summarization", "Improving LLM Attributions with Randomized Path-Integration", "Do LLMs Know to Respect Copyright Notice?", "Beyond Reference: Evaluating High Quality Translations Better than Human References", "LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid QA Evaluation via LLMs", "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions", "ABSEval: An Agent-based Framework for Script Evaluation", "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics", "Benchmarking Machine Translation with Cultural Awareness", "A Deep Analysis of the Impact of Multiword Expressions and Named Entities on Chinese-English Machine Translations", "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models", "Automated Essay Scoring: A Reflection on the State of the Art", "Measuring Psychological Depth in Language Models", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "STORYSUMM: Evaluating Faithfulness in Story Summarization", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "BOOKWORM: A Dataset for Character Description and Analysis", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "A Survey on Natural Language Counterfactual Generation", "Understanding and Mitigating Language Confusion in LLMs", "Re-Evaluating Evaluation for Multilingual Summarization", "Editing Conceptual Knowledge for Large Language Models", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-context Models"], "Knowledge Distillation": ["TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation", "DADEE: Unsupervised Domain Adaptation in Early Exit PLMS", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "A Coarse-to-Fine Prototype Learning Approach for Multi-Label Few-Shot Intent Detection", "Dual-Space Knowledge Distillation for Large Language Models", "LM\u00b2: A Simple Society of Language Models Solves Complex Reasoning", "Dual-teacher Knowledge Distillation for Low-frequency Word Translation", "Mixed Distillation Helps Smaller Language Models Reason Better", "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models", "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "Optimizing Chinese Lexical Simplification Across Word Types: A Hybrid Approach", "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL", "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation", "LoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "PAIRDISTILL: Pairwise Relevance Distillation for Dense Retrieval", "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic", "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models", "MGCL: Multi-Granularity Clue Learning for Emotion-Cause Pair Extraction via Cross-Grained Knowledge Distillation", "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation", "AlignCap: Aligning Speech Emotion Captioning to Human Preferences", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "MTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval", "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding", "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "Privacy Evaluation Benchmarks for NLP Models", "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper", "Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision", "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server", "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning", "MUSCLE: A Model Update Strategy for Compatible LLM Evolution"], "Direct Preference Optimization": ["Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization", "Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers", "Enhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "Closing the Loop: Learning to Generate Writing Feedback via Language Model Simulated Student Revisions", "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain", "Pedagogical Alignment of Large Language Models", "Self-Training Large Language and Vision Assistant for Medical Question-Answering", "A SMART Mnemonic Sounds like \u201cGlue Tonic\": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick", "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning", "Step-level Value Preference Optimization for Mathematical Reasoning", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "LIONS: An Empirically Optimized Approach to Align Language Models", "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization", "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline", "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback", "Evaluating Psychological Safety of Large Language Models", "Word Alignment as Preference for Machine Translation", "\u201cIn Dialogues We Learn\u201d: Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "HealthAlignSumm: Utilizing Alignment for Multimodal Summarization of Code-Mixed Healthcare Dialogues", "MDPO: Conditional Preference Optimization for Multimodal Large Language Models", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment", "Filtered Direct Preference Optimization", "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization", "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models without Preference Data", "ADELIE: Aligning Large Language Models on Information Extraction", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "Direct Multi-Turn Preference Optimization for Language Agents", "Self-training Large Language Models through Knowledge Detection"], "Classification": ["Unified Active Retrieval for Retrieval Augmented Generation", "Adversarial Text Generation using Large Language Models for Dementia Detection", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Computational Meme Understanding: A Survey", "A Closer Look at Multidimensional Online Political Incivility", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "\"I Never Said That\": A dataset, taxonomy and baselines on response clarity classification", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains", "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "MIPD: Exploring Manipulation and Intention in A Novel Corpus of Polish Disinformation", "TUTOR-ICL: Guiding Large Language Models for Improved In-Context Learning Performance", "Reformatted Alignment", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability", "CareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation", "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models", "Applying Contrastive Learning to Code Vulnerability Type Classification", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "Rethinking Code Refinement: Learning to Judge Code Efficiency", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Categorial Grammar Supertagging via Large Language Models", "Attribute or Abstain: Large Language Models as Long Document Assistants", "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models", "Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification", "Major Entity Identification: A Generalizable Alternative to Coreference Resolution", "Re-examining Sexism and Misogyny Classification with Annotator Attitudes", "Comparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media", "Downstream Trade-offs of a Family of Text Watermarks", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "ESG-Kor: A Korean Dataset for ESG-related Information Extraction and Practical Use Cases", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions"], "Image Captioning": ["Precise Model Benchmarking with Only a Few Observations", "M5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Improving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "Mitigating Open-Vocabulary Caption Hallucinations", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes", "Tag-grounded Visual Instruction Tuning with Retrieval Augmentation", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "A Survey of AMR Applications", "Altogether: Image Captioning via Re-aligning Alt-text", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "Benchmarking Vision Language Models for Cultural Understanding", "HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding", "Divide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning", "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "Updating CLIP to Prefer Descriptions Over Captions", "Nearest Neighbor Normalization Improves Multimodal Retrieval", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages", "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "LAMBDA: Large Language Model-Based Data Augmentation for Multi-Modal Machine Translation"], "Multimodal Learning": ["MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "MEANT: Multimodal Encoder for Antecedent Information", "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking", "VIEWS: Entity-Aware News Video Captioning", "Hope 'The Paragraph Guy' explains the rest : Introducing MeSum, the Meme Summarizer", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents", "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation", "When LLMs Meet Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection", "MMUTF: Multimodal Multimedia Event Argument Extraction with Unified Template Filling", "VIMI: Grounding Video Generation through Multi-modal Instruction", "Holistic Evaluation for Interleaved Text-and-Image Generation", "Deciphering Cognitive Distortions in Patient-Doctor Mental Health Conversations: A Multimodal LLM-Based Detection and Reasoning Framework", "Learning Musical Representations for Music Performance Question Answering", "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze", "Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts", "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP", "Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting", "MMOE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts", "Geneverse: A Collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research", "Unsupervised Discrete Representations of American Sign Language", "Large Language Models Are Challenged by Habitat-Centered Reasoning", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "Autoregressive Pre-Training on Pixels and Texts", "MiRAGeNews: Multimodal Realistic AI-Generated News Detection", "TroL: Traversal of Layers for Large Language and Vision Models", "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding", "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks", "Breaking the Boundaries: A Unified Framework for Chinese Named Entity Recognition Across Text and Speech", "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions", "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs"], "Efficiency": ["Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning", "Chain and Causal Attention for Efficient Entity Tracking", "Multi-pass Decoding for Grammatical Error Correction", "Bridging Local Details and Global Context in Text-Attributed Graphs", "MedAdapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning", "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness", "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "Rethinking Token Reduction for State Space Models", "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning", "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model", "Improving Multi-Agent Debate with Sparse Communication Topology", "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "In-Context Former: Lightning-fast Compressing Context for Large Language Model", " FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking ", "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules", "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence", "Learning to Extract Structured Entities Using Language Models", "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning", "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging", "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "QUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware", "Exploring Design Choices for Building Language-Specific LLMs", "Exploring the Practicality of Generative Retrieval on Dynamic Corpora", "EAVE: Efficient Product Attribute Value Extraction via Lightweight Sparse-layer Interaction"], "Named Entity Recognition": ["Embedded Named Entity Recognition using Probing Classifiers", "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages", "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "MedINST: Meta Dataset of Biomedical Instructions", "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science", "Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "General Collaborative Framework between Large Language Model and Experts for Universal Information Extraction", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Efficient Active Learning with Adapters", "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries", "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "MTLS: Making Texts into Linguistic Symbols", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "GottBERT: a pure German Language Model", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases", "A Morphology-Based Investigation of Positional Encodings", "Breaking the Boundaries: A Unified Framework for Chinese Named Entity Recognition Across Text and Speech", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR", "NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "ADELIE: Aligning Large Language Models on Information Extraction", "Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?", "C-ICL: Contrastive In-context Learning for Information Extraction", "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents"], "Fairness": ["Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "Reconfidencing LLMs from the Grouping Loss Perspective", "TWBias: A Benchmark for Assessing Social Bias in Traditional Chinese Large Language Models through a Taiwan Cultural Lens", "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models", "SOCIALGAZE: Improving the Integration of Human Social Norms in Large Language Models", "\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "Modeling Gender and Dialect Bias in Automatic Speech Recognition", "The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models", "Annotator-Centric Active Learning for Subjective NLP Tasks", "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?", "Local Contrastive Editing of Gender Stereotypes", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "Social Bias Probing: Fairness Benchmarking for Language Models", "Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration", "A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies", "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse", "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment", "\"Global is Good, Local is Bad?\u201d: Understanding Brand Bias in LLMs", "A Study of Implicit Ranking Unfairness in Large Language Models", "\"You Gotta be a Doctor, Lin\": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations", "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning", "Bias Wipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability", "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization", "Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models", "How Susceptible are Large Language Models to Ideological Manipulation?", "A Survey on Natural Language Counterfactual Generation", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric", "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps"], "Transformer": ["Leveraging Grammar Induction for Language Understanding and Generation", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "Generation with Dynamic Vocabulary", "LaCo: Large Language Model Pruning via Layer Collapse", "Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients", "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction", "Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell", "Varying Sentence Representations via Condition-Specified Routers", "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "TRANSLLAMA: LLM-based Simultaneous Translation System", "Exploring Question Guidance and Answer Calibration for Visually Grounded Video Question Answering", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Stanceformer: Target-Aware Transformer for Stance Detection", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "ASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction", "RECANTFormer: Referring Expression Comprehension with Varying Numbers of Targets", "LLM Explainability via Attributive Masking Learning", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "DisGeM: Distractor Generation for Multiple Choice Questions with Span Masking", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "TempoFormer: A Transformer for Temporally-aware Representations in Change Detection", "Semformer: Transformer Language Models with Semantic Planning", "On the token distance modeling ability of higher RoPE attention dimension", "Financial Forecasting from Textual and Tabular Time Series", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "Automatic Reconstruction of Ancient Chinese Pronunciations", "Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification", "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference", "Unveiling the Role of Pretraining in Direct Speech Translation", "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification", "LOCR: Location-Guided Transformer for Optical Character Recognition", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-context Models"], "Multi-task Learning": ["MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "Deciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment", "Enhancing Emotion-Cause Pair Extraction in Conversations via Center Event Detection and Reasoning", "METAKP: On-Demand Keyphrase Generation", "Mixed Distillation Helps Smaller Language Models Reason Better", "Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning", "Efficient Pointwise-Pairwise Learning-to-Rank for News Recommendation", "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction", "Incomplete Utterance Rewriting with Editing Operation Guidance and Utterance Augmentation", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "STTATTS: Unified Speech-To-Text And Text-To-Speech Model", "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models", "FAME: Towards Factual Multi-Task Model Editing", "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards", "MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "Language Concept Erasure for Language-invariant Dense Retrieval", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "MODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning", "Argument Relation Classification through Discourse Markers and Adversarial Training", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection", "Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction", "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "Temporal Cognitive Tree: A Hierarchical Modeling Approach for Event Temporal Relation Extraction", "Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering"], "Multilingual": ["MMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos", "Towards Robust Speech Representation Learning for Thousands of Languages", "Datasets for Multilingual Answer Sentence Selection", "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts", "M2QA: Multi-domain Multilingual Question Answering", "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering", "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures", "PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "SignCLIP: Connecting Text and Sign Language by Contrastive Learning", "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading", "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation", "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment", "Information Parity: Measuring and Predicting the Multilingual Capabilities of Language Models", "Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS", "MASIVE: Open-Ended Affective State Identification in English and Spanish", "Authorship Obfuscation in Multilingual Machine-Generated Text Detection", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups", "MTLS: Making Texts into Linguistic Symbols", "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language Story Understanding", "Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "Hate Personified: Investigating the role of LLMs in content moderation", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "Multilingual Fine-Grained News Headline Hallucination Detection", "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations"], "Cross-lingual Transfer": ["MMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", ">Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models", "Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment", "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings", "TL-CL: Task And Language Incremental Continual Learning", "The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm", "Exploring Multilingual Concepts of Human Values in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?", "Pruning Multilingual Large Language Models for Multilingual Inference", "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages", "MINERS: Multilingual Language Models as Semantic Retrievers", "PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language Story Understanding", "Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages", "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "A Morphology-Based Investigation of Positional Encodings", "Why do LLaVA Vision-Language Models Reply to Images in English?", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models", "Representational Isomorphism and Alignment of Multilingual Large Language Models", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "Large Language Models are Limited in Out-of-Context Knowledge Reasoning", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing"], "Pre-training": ["TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "CASE: Efficient Curricular Data Pre-training for Building Assistive Psychology Expert Models", "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "Scaling Laws for Fact Memorization of Large Language Models", "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning", "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery", "When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla", "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models", "Cross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective", "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding", "ASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "Pretraining Language Models Using Translationese", "MTLS: Making Texts into Linguistic Symbols", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "Can Language Models Induce Grammatical Knowledge from Indirect Evidence?", "PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "GottBERT: a pure German Language Model", "Autoregressive Pre-Training on Pixels and Texts", "Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction", "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers", "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging", "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision", "Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data"], "Faithfulness": ["Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "Evaluating Readability and Faithfulness of Concept-based Explanations", "Learning to Rank Salient Content for Query-focused Summarization", "Revisiting Automated Evaluation for Long-form Table Question Answering", "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems", "APPLS: Evaluating Evaluation Metrics for Plain Language Summarization", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "Activation Scaling for Steering and Interpreting Language Models", "Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning", "Atomic Inference for NLI with Generated Facts as Atoms", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "Zero-Shot Fact Verification via Natural Logic and Large Language Models", "LLM Explainability via Attributive Masking Learning", "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs", "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner for Insightful Table Summarization", "BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", " FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking ", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "STORYSUMM: Evaluating Faithfulness in Story Summarization", "FAITHSCORE: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "F2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation", "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "A Critical Look at Meta-evaluating Summarisation Evaluation Metrics"], "Human Evaluation": ["MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?", "PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "Enhancing AI Assisted Writing with One-Shot Implicit Negative Feedback", "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?", "How Does Quantization Affect Multilingual LLMs?", "Rethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning", "Reference-based Metrics Disprove Themselves in Question Generation", "Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering", "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval-Augmented Question Answering", "Aligning Large Language Models with Diverse Political Viewpoints", "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition", "DATANARRATIVE: Automated Data-Driven Storytelling with Visualizations and Texts", "SODA-EVAL: Open-Domain Dialogue Evaluation in the age of LLMs", "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics", "Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts", "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance", "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "STORYSUMM: Evaluating Faithfulness in Story Summarization", "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation", "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "I'm sure you're a real scholar yourself: Exploring Ironic Content Generation by Large Language Models", "Re-Evaluating Evaluation for Multilingual Summarization", "On Creating an English-Thai Code-switched Machine Translation in Medical Domain"], "Natural Language Understanding": ["Large Language Models Can Not Perform Well in Understanding and Manipulating Natural Language at Both Character and Word Levels?", "MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations", "Perceptions of Linguistic Uncertainty by Language Models and Humans", "Leveraging Grammar Induction for Language Understanding and Generation", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "Advancing Event Causality Identification via Heuristic Semantic Dependency Inquiry Network", "Pragmatic Norms Are All You Need \u2013 Why The Symbol Grounding Problem Does Not Apply to LLMs", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "Head-wise Shareable Attention for Large Language Models", "Revealing the Parallel Multilingual Learning within Large Language Models", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models", "BOOLQUESTIONS: Does Dense Retrieval Understand Boolean Logic in Language?", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "Zero-shot Commonsense Reasoning over Machine Imagination", "Dual-Phase Accelerated Prompt Optimization", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents", "On Training Data Influence of GPT Models", "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays", "FROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models"], "Reinforcement Learning From Human Feedback": ["Rethinking the Role of Proxy Rewards in Language Model Alignment", "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "Towards Pareto-Efficient RLHF: Paying Attention to a Few High-Reward Samples with Reward Dropout", "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Reward Modeling Requires Automatic Adjustment Based on Data Quality", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "Calibrating Language Models with Adaptive Temperature Scaling", "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "WPO: Enhancing RLHF with Weighted Preference Optimization", "Step-level Value Preference Optimization for Mathematical Reasoning", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization", "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "Semi-Supervised Reward Modeling via Iterative Self-Training", "DEFT: Distribution-guided Efficient Fine-Tuning for Human Alignment", "Self-Evolution Fine-Tuning for Policy Optimization", "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment", "Filtered Direct Preference Optimization", "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization", "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models without Preference Data", "On Diversified Preferences of Large Language Model Alignment"], "Transformers": ["Interpretability-based Tailored Knowledge Editing in Transformers", "Commonsense Knowledge Editing Based on Free-Text in LLMs", "Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives", "MOSEL: Inference Serving Using Dynamic Modality Selection", "Recent Trends in Linear Text Segmentation: A Survey", "MolTRES: Improving Chemical Language Representation Learning for Molecular Property Prediction", "Can Transformers Learn n-gram Language Models?", "Chain and Causal Attention for Efficient Entity Tracking", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "Investigating Mysteries of CoT-Augmented Distillation", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "DocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing", "Model Balancing Helps Low-data Training and Fine-tuning", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "ArMeme: Propagandistic Content in Arabic Memes", "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "Autoregressive Pre-Training on Pixels and Texts", "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers", "ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos", "Transformers are Multi-State RNNS", "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules", "A Unified Framework for Model Editing", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection", "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation", "Semantic Training Signals Promote Hierarchical Syntactic Generalization in Transformers"], "Information Extraction": ["Embedded Named Entity Recognition using Probing Classifiers", "SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement", "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "Updating Large Language Models' Memories with Time Constraints", "TKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs", "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Schema-Driven Information Extraction from Heterogeneous Tables", "General Collaborative Framework between Large Language Model and Experts for Universal Information Extraction", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries", "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering", "A Survey of AMR Applications", "When and Where Did It Happen? An Encoder-Decoder Model to Identify Scenario Context", "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "\u201cWhat is the value of {templates}?\u201d Rethinking Document Information Extraction Datasets for LLMs", "LEGOBENCH: Scientific Leaderboard Generation Benchmark", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "Learning to Extract Structured Entities Using Language Models", "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text", "Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards", "ADELIE: Aligning Large Language Models on Information Extraction", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "ESG-Kor: A Korean Dataset for ESG-related Information Extraction and Practical Use Cases", "C-ICL: Contrastive In-context Learning for Information Extraction"], "Bias Mitigation": ["BiasDora: Exploring Hidden Biased Associations in Vision-Language Models", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "Studying and Mitigating Biases in Sign Language Understanding Models", "\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction", "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation", "RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework", "Cluster-Norm for Unsupervised Probing of Knowledge", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", "Cognitive Bias in Decision-Making with LLMs", "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "Local Contrastive Editing of Gender Stereotypes", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective", "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric", "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "Does Context Help Mitigate Gender Bias in Neural Machine Translation?", "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning", "Bias Wipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability", "Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "Nearest Neighbor Normalization Improves Multimodal Retrieval", "TAB2TEXT - A framework for deep learning with tabular data", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives"], "Bias Detection": ["Towards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation", "Locating Information Gaps and Narrative Inconsistencies Across Languages: A Case Study of LGBT People Portrayals on Wikipedia", "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models", "LLM Tropes: Revealing Fine-Grained Values and Opinions in Large Language Models", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models", "Gender Identity in Pretrained Language Models: An Inclusive Approach to Data Creation and Probing", "Cognitive Bias in Decision-Making with LLMs", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "PclGPT: A Large Language Model for Patronizing and Condescending Language Detection", "ADAPTIVE AXES: A Pipeline for In-domain Social Stereotype Analysis", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "Moral Foundations of Large Language Models", "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "\"Global is Good, Local is Bad?\u201d: Understanding Brand Bias in LLMs", "Voices in a Crowd: Searching for Clusters of Unique Perspectives", "Evaluating Biases in Context-Dependent Sexual and Reproductive Health Questions", "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models", "Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models", "Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models"], "Multimodal Llms": ["Layout-aware GUI Screen Reading with Tree-of-Lens Grounding", "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective", "EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models", "Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models", "Tag-grounded Visual Instruction Tuning with Retrieval Augmentation", "LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference", "Multimodal Misinformation Detection by Learning from Synthetic Data with Multimodal LLMs", "Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge", "TinyChart: Efficient Chart Understanding with Program-of-Thoughts Learning and Visual Token Merging", "Personalized Video Comment Generation", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "Visual Question Decomposition on Multimodal Large Language Models", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance", "ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering", "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model", "Exploring the Capability of Multimodal LLMs with Yonkoma Manga: The YManga dataset and Its Challenging Tasks", "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models", "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting", "PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes", "MDPO: Conditional Preference Optimization for Multimodal Large Language Models", "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance", "M\u00b2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs"], "LLM Evaluation": ["Towards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "SedarEval: Automated Evaluation using Self-Adaptive Rubrics", "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs", "PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Cognitive Bias in Decision-Making with LLMs", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "Are Large Language Models Consistent over Value-laden Questions?", "TOWER: Tree Organized Weighting for Evaluating Complex Instructions", "Knowledge-based Consistency Testing of Large Language Models", "An Open-Source Data Contamination Report for Large Language Models", "Self-Evaluation of Large Language Model based on Glass-box Features", "SODA-EVAL: Open-Domain Dialogue Evaluation in the age of LLMs", "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts", "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation", "Assessing and Verifying Task Utility in LLM-Powered Applications", "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models", "Compare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY", "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "Adversarial Math Word Problem Generation"], "Hallucination Detection": ["Reference-free Hallucination Detection for Large Vision-Language Models", "Embedding and Gradient Say Wrong: A White-Box Method for Hallucination Detection", "When Context Leads but Parametric Memory Follows in Large Language Models", "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios", "Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation", "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "Knowledge-Centric Hallucination Detection", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning", "HalluMeasure: Fine-grained Hallucination Measurement Using Chain-of-Thought Reasoning", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "CED: Comparing Embedding Differences for Detecting Out-of-Distribution and Hallucinated Text", "On the Universal Truthfulness Hyperplane Inside LLMS", "FASTTRACK: Reliable Fact Tracing via Clustering and LLM-Powered Evidence Validation", "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "Multilingual Fine-Grained News Headline Hallucination Detection", "Factuality of Large Language Models: A Survey", "FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document"], "Domain Adaptation": ["Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs.", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "Robust AI-Generated Text Detection by Restricted Embeddings", "Generation with Dynamic Vocabulary", "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages", "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science", "Improving Referring Ability for Biomedical Language Models", "Cross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective", "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "Financial Forecasting from Textual and Tabular Time Series", "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis", "Advancing Test-Time Adaptation in Wild Acoustic Test Settings", "Transfer Learning for Text Classification via Model Risk Analysis", "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning", "Domain adapted machine translation: What does catastrophic forgetting forget and why?", "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models", "A Survey on Natural Language Counterfactual Generation", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "Domain Adaptation via Prompt Learning for Alzheimer's Detection", "Double-Checker: Large Language Model as a Checker for Few-shot Named Entity Recognition", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"], "Diversity": ["An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Automatic Instruction Evolving for Large Language Models", "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "MIRRORSTORIES: Reflecting Diversity through Personalized Narrative Generation with Large Language Models", "Annotator-Centric Active Learning for Subjective NLP Tasks", "Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM", "Inductive-Deductive Strategy Reuse for Multi-Turn Instructional Dialogues", "Aligning Large Language Models with Diverse Political Viewpoints", "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "Nash CoT: Multi-Path Inference with Preference Equilibrium", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making", "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use", "In-Context Learning with Iterative Demonstration Selection", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "How Far Can We Extract Diverse Perspectives from Large Language Models?", "Evaluating Diversity in Automatic Poetry Generation"], "Hallucination Mitigation": ["Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "Mitigating Open-Vocabulary Caption Hallucinations", "EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning", "LONGAGENT: Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration", "Atomic Self-Consistency for Better Long Form Generations", "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective for Molecular Property Prediction", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations", "MAVEN-FACT: A Large-scale Event Factuality Detection Dataset", "Sanitizing Large Language Models in Bug Detection with Data-Flow", "Knowledge Verification to Nip Hallucination in the Bud", "Dynamic Planning for LLM-based Graphical User Interface Automation", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "Word Alignment as Preference for Machine Translation", "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "Analysis of Plan-based Retrieval for Grounded Text Generation", "HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism", "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models", "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models", "Mitigating Hallucination in Fictional Character Role-Play", "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models", "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension", "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?"], "Consistency": ["An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "DEVIL'S ADVOCATE: Anticipatory Reflection for LLM Agents", "Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness", "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison", "Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs", "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "Consistent Bidirectional Language Modelling: Expressive Power and Representational Conciseness", "Subword Segmentation in LLMs: Looking at Inflection and Consistency", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "LONGGENBENCH: Long-context Generation Benchmark", "CAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans", "Consistent Document-Level Relation Extraction via Counterfactuals", "On the Reliability of Psychological Scales on Large Language Models", "Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "Moral Foundations of Large Language Models", "Denoising Rationalization for Multi-hop Fact Verification via Multi-granular Explainer", "Better Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization", "Temporally Consistent Factuality Probing for Large Language Models", "What Matters in Memorizing and Recalling Facts? Multifaceted Benchmarks for Knowledge Probing in Language Models", "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation", "Compare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY", "Evaluating Language Model Character Traits", "Virtual Personas for Language Models via an Anthology of Backstories", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering"], "Calibration": ["Calibrating Long-form Generations from Large Language Models", "Teaching LLMs to Abstain across Languages via Multilingual Feedback", "Distance-aware Calibration for Pre-trained Language Models", "Reconfidencing LLMs from the Grouping Loss Perspective", "The Craft of Selective Prediction: Towards Reliable Case Outcome Classification - An Empirical Study on European Court of Human Rights Cases", "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration", "Calibrating Language Models with Adaptive Temperature Scaling", "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing", "Uncertainty Calibration for Tool-Using Language Agents", "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "Self-Consistency Boosts Calibration for Math Reasoning", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees", "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "Calibrating the Confidence of Large Language Models by Eliciting Fidelity", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models", "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality", "ExpertEase: A Multi-Agent Framework for Grade-Specific Document Simplification with Large Language Models", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "Code Membership Inference for Detecting Unauthorized Data Use in Code Pre-trained Language Models", "Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "Factuality of Large Language Models: A Survey", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives"], "Benchmarkss": ["FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition", "Academics Can Contribute to Domain-Specialized Language Models", "Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models", "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models", "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations", "The Effect of Sampling Temperature on Problem Solving in Large Language Models", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "Building Resources for Emakhuwa: Machine Translation and News Classification Benchmarks", "An Open-Source Data Contamination Report for Large Language Models", "Comparing Edge-based and Node-based Methods on a Citation Prediction Task", "Do Text-to-Vis Benchmarks Test Real Use of Visualisations?", "A Survey on Detection of LLMs-Generated Content", "Data Contamination Can Cross Language Barriers", "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models", "Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs", "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization", "Data-Centric AI in the Age of Large Language Models", "What Matters in Memorizing and Recalling Facts? Multifaceted Benchmarks for Knowledge Probing in Language Models", "Compare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY", "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning", "Puzzle Solving using Reasoning of Large Language Models: A Survey", "Privacy Evaluation Benchmarks for NLP Models", "LLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks"], "Social Media": ["Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach", "CURE: Context- and Uncertainty-Aware Mental Disorder Detection", "A Closer Look at Multidimensional Online Political Incivility", "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Finance Communities in Long Form Question Answering", "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse", "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness", "LLM generated responses to mitigate the impact of hate speech", "Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis", "Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health", "Mental Disorder Classification via Temporal Representation of Text", "Leveraging Conflicts in Social Media Posts: Unintended Offense Dataset", "\u201cWe Demand Justice!\u201d: Towards Social Context Grounding of Political Texts", "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models", "Understanding Slang with LLMs: Modelling Cross-Cultural Nuances through Paraphrasing", "Multilingual Topic Classification in X: Dataset and Analysis", "Please note that I'm just an AI: Analysis of Behavior Patterns of LLMs in (Non-)offensive Speech Identification", "Ukrainian Resilience: A Dataset for Detection of Help-Seeking Signals Amidst the Chaos of War", "The Empirical Variability of Narrative Perceptions of Social Media Texts", "Revealing COVID-19's Social Dynamics: Diachronic Semantic Analysis of Vaccine and Symptom Discourse on Twitter", "F2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation", "Comparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media", "MisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"", "Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning"], "Preference Optimization": ["RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "Weak-to-Strong Reasoning", "Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Rethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning", "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain", "Self-training Language Models for Arithmetic Reasoning", "WPO: Enhancing RLHF with Weighted Preference Optimization", "Aligning Large Language Models with Diverse Political Viewpoints", "Advancing Large Language Model Attribution through Self-Improving", "ORPO: Monolithic Preference Optimization without Reference Model", "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback", "Word Alignment as Preference for Machine Translation", "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment", "AlignCap: Aligning Speech Emotion Captioning to Human Preferences", "EPO: Hierarchical LLM Agents with Environment Preference Optimization", "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring", "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation", "MDPO: Conditional Preference Optimization for Multimodal Large Language Models", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment", "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "Improving Factual Consistency of News Summarization by Contrastive Preference Optimization"], "Lora": ["MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "Towards One-to-Many Visual Question Answering", "Instance-Level Dynamic LoRAs Composition for Cross-Task Generalization", "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization", "DA\u00b3: A Distribution-Aware Adversarial Attack against Language Models", "LLOCO: Learning Long Contexts Offline", "AdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration", "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation", "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs", "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "Categorial Grammar Supertagging via Large Language Models", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis", "Reusing Transferable Weight Increments for Low-resource Style Generation", "Updating CLIP to Prefer Descriptions Over Captions", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune", "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning"], "Relation Extraction": ["Topic-Oriented Open Relation Extraction with A Priori Seed Generation", "AliGATr: Graph-based layout generation for form understanding", "MedINST: Meta Dataset of Biomedical Instructions", "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "Consistent Document-Level Relation Extraction via Counterfactuals", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "ATAP: Automatic Template-Augmented Commonsense Knowledge Graph Completion via Pre-Trained Language Models", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "A Survey on Open Information Extraction from Rule-based Model to Large Language Model", "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases", "Preserving Generalization of Language Models in Few-shot Continual Relation Extraction", "SRF: Enhancing Document-Level Relation Extraction with a Novel Secondary Reasoning Framework", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Entity or Relation Embeddings? An Analysis of Encoding Strategies for Relation Extraction", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "A Survey on Natural Language Counterfactual Generation", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "ADELIE: Aligning Large Language Models on Information Extraction", "C-ICL: Contrastive In-context Learning for Information Extraction", "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents"], "Bert": ["Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature", "CASE: Efficient Curricular Data Pre-training for Building Assistive Psychology Expert Models", "Revisiting Supertagging for Faster HPSG Parsing", "The effects of distance on NPI illusive effects in BERT", "KAR\u00b3L: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "MIPD: Exploring Manipulation and Intention in A Novel Corpus of Polish Disinformation", "OEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary", "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries", "Class Name Guided Out-of-Scope Intent Classification", "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "TempoFormer: A Transformer for Temporally-aware Representations in Change Detection", "Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models", "Automatic sentence segmentation of clinical record narratives in real-world data", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "Dense Passage Retrieval: Is it Retrieving?", "GottBERT: a pure German Language Model", "When Generative Adversarial Networks Meet Sequence Labeling Challenges", "Leveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training", "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs", "Bias Wipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability", "Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision", "Comparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media"], "Model Compression": ["TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "Adaptive Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "LaCo: Large Language Model Pruning via Layer Collapse", "Head-wise Shareable Attention for Large Language Models", "Dual-Space Knowledge Distillation for Large Language Models", "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "Structured Optimal Brain Pruning for Large Language Models", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models", "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "MTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging", "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy"], "Speech Recognition": ["SpeechQE: Estimating the Quality of Direct Speech Translation", "950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages", "MOSEL: Inference Serving Using Dynamic Modality Selection", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "Modeling Gender and Dialect Bias in Automatic Speech Recognition", "Are Modern Neural ASR Architectures Robust for Polysynthetic Languages?", "Scaling Properties of Speech Language Models", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "STTATTS: Unified Speech-To-Text And Text-To-Speech Model", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "AudioVSR: Enhancing Video Speech Recognition with Audio Data", "LaRA: Large Rank Adaptation for Speech and Text Cross-Modal Learning in Large Language Models", "EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning", "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges", "Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech", "VHASR: A Multimodal Speech Recognition System With Vision Hotwords", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps", "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects"], "Gsm8k": ["Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "Task Oriented In-Domain Data Augmentation", "Weak-to-Strong Reasoning", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "Self-Consistency Boosts Calibration for Math Reasoning", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "Reasoning Robustness of LLMs to Adversarial Typographical Errors", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "MODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning", "Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "LLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning", "Skills-in-Context: Unlocking Compositionality in Large Language Models", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning"], "Logical Reasoning": ["FOLIO: Natural Language Reasoning with First-Order Logic", "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "Symbolic Working Memory Enhances Language Models for Complex Rule Application", "Retrieved In-Context Principles from Previous Mistakes", "Chain of Condition: Construct, Verify and Solve Conditions for Conditional Question Answering", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "METAREFLECTION: Learning Instructions for Language Agents using Past Reflections", "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning", "Conditional and Modal Reasoning in Large Language Models", "Boosting Logical Fallacy Reasoning in LLMs via Logical Structure Tree", "Puzzle Solving using Reasoning of Large Language Models: A Survey", "Abstraction-of-Thought Makes Language Models Better Reasoners", "Learning to Plan by Updating Natural Language", "SALMON: A Structure-Aware Language Model with logicality and densification strategy for Temporal Knowledge Graph Reasoning", "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars"], "Reading Comprehension": ["FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition", "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models", "Functionality learning through specification instructions", "LawBench: Benchmarking Legal Knowledge of Large Language Models", "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals", "Fine-Grained Prediction of Reading Comprehension from Eye Movements", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "Revealing the Parallel Multilingual Learning within Large Language Models", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks", "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training", "AMPO: Automatic Multi-Branched Prompt Optimization", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "Mitigating the Alignment Tax of RLHF", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models"], "Deep Learning": ["MEANT: Multimodal Encoder for Antecedent Information", "Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients", "Tending Towards Stability: Convergence Challenges in Small Language Models", "ROBERT2VecTM: A Novel Approach for Topic Extraction in Islamic Studies", "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check", "Unifying Multimodal Retrieval via Document Screenshot Embedding", "Mental Disorder Classification via Temporal Representation of Text", "Improving LLM Attributions with Randomized Path-Integration", "ASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction", "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "Applying Contrastive Learning to Code Vulnerability Type Classification", "A Deep Analysis of the Impact of Multiword Expressions and Named Entities on Chinese-English Machine Translations", "Financial Forecasting from Textual and Tabular Time Series", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "A Survey on In-context Learning", "Towards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs", "RAt: Injecting Implicit Bias for Text-To-Image Prompt Refinement Models", "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification", "Enhancing Multi-Label Text Classification under Label-Dependent Noise: A Label-Specific Denoising Framework", "TAB2TEXT - A framework for deep learning with tabular data", "Latent Concept-based Explanation of NLP Models"], "Factuality": ["Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models", "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models", "Estimating Knowledge in Large Language Models Without Generating a Single Token", "Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM", "Reformatted Alignment", "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "FACTALIGN: Long-form Factuality Alignment of Large Language Models", "Improving Multi-Agent Debate with Sparse Communication Topology", "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation", "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "How Entangled is Factuality and Deception in German?", "Analysis of Plan-based Retrieval for Grounded Text Generation", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Temporally Consistent Factuality Probing for Large Language Models", "LUQ: Long-text Uncertainty Quantification for LLMs", "BOOKWORM: A Dataset for Character Description and Analysis", "F2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "An Analysis of Multilingual FActScore", "Factuality of Large Language Models: A Survey"], "Text Summarization": ["Divide and Conquer: Legal Concept-guided Criminal Court View Generation", "Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles", "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?", "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "Aligning Large Language Models with Diverse Political Viewpoints", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "Semformer: Transformer Language Models with Semantic Planning", "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions", "ALIGNSUM: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "HealthAlignSumm: Utilizing Alignment for Multimodal Summarization of Code-Mixed Healthcare Dialogues", "Presentations are not always linear! GNN meets LLM for Text Document-to-Presentation Transformation with Attribution", "FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "RoQLlama: A Lightweight Romanian Adapted Language Model", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "A Decoding Algorithm for Length-Control Summarization Based on Directed Acyclic Transformers"], "Knowledge Graphs": ["LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments", "Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction", "What Would Happen Next? Predicting Consequences from An Event Causality Graph", "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering", "MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs", "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering", "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs", "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints", "A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval", "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs", "Knowledge Graph Enhanced Large Language Model Editing", "SALMON: A Structure-Aware Language Model with logicality and densification strategy for Temporal Knowledge Graph Reasoning", "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text", "Evidence Retrieval for Fact Verification using Multi-stage Reranking", "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?"], "Knowledge Base": ["Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation", "ROCEL: Advancing Table Entity Linking through Distinctive Row and Column Contexts", "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs", "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science", "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases", "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering", "What are the Generator Preferences for End-to-end Task-Oriented Dialog System?", "EvOR: Evolving Retrieval for Code Generation", "HoneyComb: A Flexible LLM-Based Agent System for Materials Science", "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation", "Contrastive Entity Coreference and Disambiguation for Historical Texts", "Extractive Medical Entity Disambiguation with Memory Mechanism and Memorized Entity Information", "Temporally Consistent Factuality Probing for Large Language Models", "DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "Do Large Language Models Know How Much They Know?", "Efficient Overshadowed Entity Disambiguation by Mitigating Shortcut Learning", "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting", "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs"], "Graph Neural Networks": ["OpenGraph: Towards Open Graph Foundation Models", "DGLF: A Dual Graph-based Learning Framework for Multi-modal Sarcasm Detection", "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "Enhancing Emotion-Cause Pair Extraction in Conversations via Center Event Detection and Reasoning", "Bridging Local Details and Global Context in Text-Attributed Graphs", "What Would Happen Next? Predicting Consequences from An Event Causality Graph", "Can Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction", "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning", "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective for Molecular Property Prediction", "Enhancing High-order Interaction Awareness in LLM-based Recommender Model", "XRec: Large Language Models for Explainable Recommendation", "CONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules", "Comparing Edge-based and Node-based Methods on a Citation Prediction Task", "InsertGNN: A Hierarchical Graph Neural Network for the TOEFL Sentence Insertion Problem", "LEGOBENCH: Scientific Leaderboard Generation Benchmark", "Presentations are not always linear! GNN meets LLM for Text Document-to-Presentation Transformation with Attribution", "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs", "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Transferability of Syntax-Aware Graph Neural Networks in Zero-Shot Cross-Lingual Semantic Role Labeling", "Knowledge Graph Enhanced Large Language Model Editing", "Message Passing on Semantic-Anchor-Graphs for Fine-grained Emotion Representation Learning and Classification"], "Model Editing": ["To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models", "LEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models", "Local Contrastive Editing of Gender Stereotypes", "FAME: Towards Factual Multi-Task Model Editing", "On the Robustness of Editing Large Language Models", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "The Fall of ROME: Understanding the Collapse of LLMs in Model Editing", "Dense Passage Retrieval: Is it Retrieving?", "Better Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization", "Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning", "Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing", "A Unified Framework for Model Editing", "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis", "Knowledge Graph Enhanced Large Language Model Editing", "Consecutive Batch Model Editing with HooK Layers", "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries"], "Hate Speech Detection": ["Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection", "Functionality learning through specification instructions", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Computational Meme Understanding: A Survey", "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models", "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation", "LLM generated responses to mitigate the impact of hate speech", "PREDICT: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection", "Annotator-Centric Active Learning for Subjective NLP Tasks", "Revisiting Supervised Contrastive Learning for Microblog Classification", "MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification", "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze", "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Hateful Word in Context Classification", "Hate Personified: Investigating the role of LLMs in content moderation", "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation", "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning"], "Self-supervised Learning": ["Towards Robust Speech Representation Learning for Thousands of Languages", "Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training", "Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training", "Generative Deduplication For Socia Media Data Selection", "MolTRES: Improving Chemical Language Representation Learning for Molecular Property Prediction", "EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models", "Mitigating Matthew Effect: Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation", "Exploring Question Guidance and Answer Calibration for Visually Grounded Video Question Answering", "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach", "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability", "LLM Explainability via Attributive Masking Learning", "Phonetic and Lexical Discovery of Canine Vocalization", "NCPrompt: NSP-Based Prompt Learning and Contrastive Learning for Implicit Discourse Relation Recognition", "AudioVSR: Enhancing Video Speech Recognition with Audio Data", "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free-Word-Ordered and Morphologically-Rich Low-Resource Languages", "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs", "EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning", "Entity or Relation Embeddings? An Analysis of Encoding Strategies for Relation Extraction", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "Enhancing Polyglot Voices by Leveraging Cross-Lingual Fine-Tuning in Any-to-One Voice Conversion"], "Arithmetic Reasoning": ["Large Language Models Can Self-Correct with Key Condition Verification", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "Self-training Language Models for Arithmetic Reasoning", "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Re-Reading Improves Reasoning in Large Language Models", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models"], "Multimodal": ["MMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos", "In-Context Compositional Generalization for Large Vision-Language Models", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Rethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning", "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading", "MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification", "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought", "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning", "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "Financial Forecasting from Textual and Tabular Time Series", "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation", "ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos", "I-AM-G: Interest Augmented Multimodal Generator for Item Personalization", "M\u00b3D: MultiModal MultiDocument Fine-Grained Inconsistency Detection", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause"], "Catastrophic Forgetting": ["Unlocking Continual Learning Abilities in Language Models", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging", "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "Lifelong Event Detection via Optimal Transport", "LEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models", "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs", "Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting", "ICL: Iterative Continual Learning for Multi-domain Neural Machine Translation", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation", "SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models", "Domain adapted machine translation: What does catastrophic forgetting forget and why?", "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance", "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs", "Self-training Large Language Models through Knowledge Detection", "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models"], "Visual Reasoning": ["MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables", "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "Large Language Models Are Challenged by Habitat-Centered Reasoning", "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "Enhancing Advanced Visual Reasoning Ability of Large Language Models", "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model", "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis", "PropTest: Automatic Property Testing for Improved Visual Programming", "MM-ChatAlign: A Novel Multimodal Reasoning Framework based on Large Language Models for Entity Alignment", "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering"], "Safety": ["Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging", "KidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions", "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "CMD: a framework for Context-aware Model self-Detoxification", "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness", "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations", "The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm", "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "Please note that I'm just an AI: Analysis of Behavior Patterns of LLMs in (Non-)offensive Speech Identification", "Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective", "VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values", "Ask the experts: Sourcing a high-quality nutrition counseling dataset through Human-AI collaboration", "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "TrustAgent: Towards Safe and Trustworthy LLM-based Agents", "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "Preference Tuning For Toxicity Mitigation Generalizes Across Languages", "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming"], "Supervised Fine-tuning": ["Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?", "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback", "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales", "Self-training Language Models for Arithmetic Reasoning", "LIONS: An Empirically Optimized Approach to Align Language Models", "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "Instruction Fine-Tuning: Does Prompt Loss Matter?", "ORPO: Monolithic Preference Optimization without Reference Model", " FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking ", "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging", "Unlocking the Potential of Model Merging for Low-Resource Languages", "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks", "Self-Evolution Fine-Tuning for Policy Optimization", "KNN-INSTRUCT: Automatic Instruction Construction with K Nearest Neighbor Deduction", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "Learning from Relevant Subgoals in Successful Dialogs using Iterative Training for Task-oriented Dialog Systems", "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models"], "Preference Learning": ["Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Enhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing", "Towards Tool Use Alignment of Large Language Models", "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning", "Step-level Value Preference Optimization for Mathematical Reasoning", "What are the Generator Preferences for End-to-end Task-Oriented Dialog System?", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Unsupervised Human Preference Learning", "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "DEFT: Distribution-guided Efficient Fine-Tuning for Human Alignment", "EPO: Hierarchical LLM Agents with Environment Preference Optimization", "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation", "SWAG: Storytelling With Action Guidance", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "Learning from Relevant Subgoals in Successful Dialogs using Iterative Training for Task-oriented Dialog Systems", "Speechworthy Instruction-tuned Language Models"], "Rag": ["Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation", "Unified Active Retrieval for Retrieval Augmented Generation", "SATYRN: A Platform for Analytics Augmented Generation", "Investigating LLMs as Voting Assistants via Contextual Augmentation: A Case Study on the European Parliament Elections 2024", "Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA", "Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models", "Improve Dense Passage Retrieval with Entailment Tuning", "EvOR: Evolving Retrieval for Code Generation", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "QuBE: Question-based Belief Enhancement for Agentic LLM Reasoning", "Ranking Manipulation for Conversational Search Engines", "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation", "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "More Bang for your Context: Virtual Documents for Question Answering over Long Documents", "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "Learning Autonomous Driving Tasks via Human Feedbacks with Large Language Models", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain"], "Quantization": ["MobileQuant: Mobile-friendly Quantization for On-device Language Models", "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization", "How Does Quantization Affect Multilingual LLMs?", "ATQ: Activation Transformation for Weight-Activation Quantization of Large Language Models", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "QEFT: Quantization for Efficient Fine-Tuning of LLMs", "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models", "Fast Matrix Multiplications for Lookup Table-Quantized LLMs", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "QUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware", "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models", "QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models"], "LLM Alignment": ["Reward Difference Optimization For Sample Reweighting In Offline RLHF", "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents", "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch", "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion", "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization", "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline", "Aligners: Decoupling LLMs and Alignment", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks", "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization", "Self-Evolution Fine-Tuning for Policy Optimization", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "On Diversified Preferences of Large Language Model Alignment"], "Unsupervised Learning": ["LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning", "COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities", "Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval", "Cluster-Norm for Unsupervised Probing of Knowledge", "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts", "Unsupervised Human Preference Learning", "Class Name Guided Out-of-Scope Intent Classification", "Toeing the Party Line: Election Manifestos as a Key to Understand Political Discourse on Twitter", "Unsupervised Extraction of Dialogue Policies from Conversations", "A Simple Angle-based Approach for Contrastive Learning of Unsupervised Sentence Representation", "An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "Voices in a Crowd: Searching for Clusters of Unique Perspectives", "Revealing COVID-19's Social Dynamics: Diachronic Semantic Analysis of Vaccine and Symptom Discourse on Twitter", "Unsupervised Named Entity Disambiguation for Low Resource Domains", "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions", "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of the Noisy Channel"], "Tokenization": ["BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training", "Unsupervised Discrete Representations of American Sign Language", "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation", "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR", "Lexically Grounded Subword Segmentation", "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models", "Tokenization Falling Short: On Subword Robustness in Large Language Models", "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference", "Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia", "Exploring Design Choices for Building Language-Specific LLMs", "CUTE: Measuring LLMs' Understanding of Their Tokens", "Tokenization Is More Than Compression", "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning", "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs", "Distributional Properties of Subword Regularization", "Where is the signal in tokenization space?", "Finding the Optimal Byte-Pair Encoding Merge Operations for Neural Machine Translation in a Low-Resource Setting", "On the Proper Treatment of Tokenization in Psycholinguistics"], "Knowledge Graph": ["Unsupervised Named Entity Disambiguation for Low Resource Domains", "CNEQ: Incorporating numbers into Knowledge Graph Reasoning", "MedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures", "StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for Children's Story-Based Learning", "R\u00b3-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL", "A Usage-centric Take on Intent Understanding in E-Commerce", "TKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "Mitigating Hallucination in Fictional Character Role-Play", "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature", "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models", "Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards", "Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective", "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism", "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "Knowledge-based Consistency Testing of Large Language Models", "NALA: an Effective and Interpretable Entity Alignment Method"], "Translation": ["A Thorough Examination of Decoding Methods in the Era of LLMs", "Mitigating the Alignment Tax of RLHF", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models", "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Using Language Models to Disambiguate Lexical Choices in Translation", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "An Analysis and Mitigation of the Reversal Curse", "Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters", "MedINST: Meta Dataset of Biomedical Instructions"], "Rlhf": ["Rethinking the Role of Proxy Rewards in Language Model Alignment", "Mitigating the Alignment Tax of RLHF", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "Filtered Direct Preference Optimization", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "WPO: Enhancing RLHF with Weighted Preference Optimization", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "Calibrating the Confidence of Large Language Models by Eliciting Fidelity", "Reward Modeling Requires Automatic Adjustment Based on Data Quality", "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring"], "Synthetic Data Generation": ["Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "Better Alignment with Instruction Back-and-Forth Translation", "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "GOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory", "Pedagogical Alignment of Large Language Models", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "Aligners: Decoupling LLMs and Alignment", "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners", "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "LLM-based Code-Switched Text Generation for Grammatical Error Correction", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection", "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server", "Synthetic Multimodal Question Generation", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains"], "Annotation": ["Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues", "Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature", "Let's discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment", "CEAMC: Corpus and Empirical Study of Argument Analysis in Education via LLMs", "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays", "Designing Logic Pattern Templates for Counter-Argument Logical Structure Analysis", "What's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs", "Flee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse", "Leveraging pre-trained language models for linguistic analysis: A case of argument structure constructions", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "The Empirical Variability of Narrative Perceptions of Social Media Texts", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "Grounding Partially-Defined Events in Multimodal Data", "Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas", "Large Language Models for Propaganda Span Annotation"], "Knowledge Editing": ["Interpretability-based Tailored Knowledge Editing in Transformers", "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing", "Commonsense Knowledge Editing Based on Free-Text in LLMs", ">Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "AKEW: Assessing Knowledge Editing in the Wild", "Updating Large Language Models' Memories with Time Constraints", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion", "Editing Conceptual Knowledge for Large Language Models", "EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities", "Cross-Lingual Multi-Hop Knowledge Editing"], "Low-rank Adaptation": ["Advancing Vision-Language Models with Adapter Ensemble Strategies", "Mixture-of-Subspaces in Low-Rank Adaptation", "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "RepMatch: Quantifying Cross-Instance Similarities in Representation Space", "Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation", "On Mitigating Performance Disparities in Multilingual Speech Recognition", "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly", "Fast Forwarding Low-Rank Training", "LoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning", "Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "LaRA: Large Rank Adaptation for Speech and Text Cross-Modal Learning in Large Language Models"], "Generation": ["Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model", "COGEN: Learning from Feedback with Coupled Comprehension and Generation", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation", "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models", "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "Reformatted Alignment", "MATHWELL: Generating Educational Math Word Problems Using Teacher Annotations", "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation"], "Video Understanding": ["Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "Personalized Video Comment Generation", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "MatchTime: Towards Automatic Soccer Game Commentary Generation", "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP", "A Simple LLM Framework for Long-Range Video Question-Answering", "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection", "VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models", "GRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization", "Encoding and Controlling Global Semantics for Long-form Video Question Answering", "Enhancing Temporal Modeling of Video LLMs via Time Gating", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation", "Grounding Partially-Defined Events in Multimodal Data", "ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos", "Navigating Hallucinations for Reasoning of Unintentional Activities", "Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge"], "Privacy": ["Towards Robust Evaluation of Unlearning in LLMs via Data Transformations", "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model", "MedAdapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning", "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models", "Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients", "Granular Privacy Control for Geolocation with Vision Language Models", "Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4", "RECALL: Membership Inference via Relative Conditional Log-Likelihoods", "Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective", "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models", "Privacy Evaluation Benchmarks for NLP Models", "PERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts", "An Inversion Attack Against Obfuscated Embedding Matrix in Language Model Inference", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Order of Magnitude Speedups for LLM Membership Inference", "CSLM: A Framework for Question Answering Dataset Generation through Collaborative Small Language Models", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains", "Rethinking Evaluation Methods for Machine Unlearning"], "Attention Mechanism": ["All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction", "Varying Sentence Representations via Condition-Specified Routers", "External Knowledge-Driven Argument Mining: Leveraging Attention-Enhanced Multi-Network Models", "ROCEL: Advancing Table Entity Linking through Distinctive Row and Column Contexts", "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs", "Query-based Cross-Modal Projector Bolstering Mamba Multimodal LLM", "DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination", "Stanceformer: Target-Aware Transformer for Stance Detection", "Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation", "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction", "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression", "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning", "Memorize Step by Step: Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk", "SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering", "Position Engineering: Boosting Large Language Models through Positional Information Manipulation", "Chain and Causal Attention for Efficient Entity Tracking", "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions"], "Dialogue Generation": ["How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support", "Mixed-Session Conversation with Egocentric Memory", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "Towards Aligning Language Models with Textual Feedback", "Securing Multi-turn Conversational Language Models From Distributed Backdoor Triggers", "Ontologically Faithful Generation of Non-Player Character Dialogues", "HOLLMWOOD: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "Paraphrase Types Elicit Prompt Engineering Capabilities", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models", "CACTUS: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory", "TRIP NEGOTIATOR: A Travel Persona-aware Reinforced Dialogue Generation Model for Personalized Integrative Negotiation in Tourism", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "ABLE: Personalized Disability Support with Politeness and Empathy Integration"], "Error Analysis": ["A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks", "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion", "To Err Is Human, but Llamas Can Learn It Too", "NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models", "Revisiting Automated Evaluation for Long-form Table Question Answering", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "A Deep Analysis of the Impact of Multiword Expressions and Named Entities on Chinese-English Machine Translations", "What's under the hood: Investigating Automatic Metrics on Meeting Summarization", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "ASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?", "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language"], "Datasets": ["All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "MEANT: Multimodal Encoder for Antecedent Information", "Let's discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "Datasets for Multilingual Answer Sentence Selection", "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback", "Puzzle Solving using Reasoning of Large Language Models: A Survey", "Automated Essay Scoring: A Reflection on the State of the Art", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents", "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery", "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization", "Rethinking Evaluation Methods for Machine Unlearning"], "Adversarial Attacks": ["CROWD: Certified Robustness via Weight Distribution for Smoothed Classifiers against Backdoor Attack", "Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis", "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation", "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking", "Authorship Obfuscation in Multilingual Machine-Generated Text Detection", "Thinking Outside of the Differential Privacy Box: A Case Study in Text Privatization with Language Model Prompting", "Prompts have evil twins", "Detecting Machine-Generated Long-Form Content with Latent-Space Variables", "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming", "Robust Text Classification: Analyzing Prototype-Based Networks", "Reconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with GPT Models", "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers", "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate", "Adversarial Math Word Problem Generation", "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection"], "Language Understanding": ["Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game", "MalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "Leveraging Grammar Induction for Language Understanding and Generation", "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "LLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks", "Enhancing Agent Learning through World Dynamics Modeling", "Autoregressive Pre-Training on Pixels and Texts", "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "MUSCLE: A Model Update Strategy for Compatible LLM Evolution", "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "Large Language Models are Students at Various Levels: Zero-shot Question Difficulty Estimation"], "Uncertainty": ["The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning", "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?", "On the Rigour of Scientific Writing: Criteria, Analysis, and Insights", "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "Uncertainty Calibration for Tool-Using Language Agents", "Uncertainty in Language Models: Assessment through Rank-Calibration", "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models", "CURE: Context- and Uncertainty-Aware Mental Disorder Detection", "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "Calibrating the Confidence of Large Language Models by Eliciting Fidelity", "Statistical Uncertainty in Word Embeddings: GloVe-V", "Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning", "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts", "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation", "Characterizing LLM Abstention Behavior in Science QA with Context Perturbations", "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process"], "Reward Model": ["Reward Difference Optimization For Sample Reweighting In Offline RLHF", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "Filtered Direct Preference Optimization", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "Semi-Supervised Reward Modeling via Iterative Self-Training", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "Preference-Guided Reflective Sampling for Aligning Language Models", "Sing it, Narrate it: Quality Musical Lyrics Translation", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence", "Reward Modeling Requires Automatic Adjustment Based on Data Quality", "Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "On Diversified Preferences of Large Language Model Alignment"], "Gender Bias": ["Modeling Gender and Dialect Bias in Automatic Speech Recognition", "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "Evaluating Gender Bias of LLMs in Making Morality Judgements", "On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models", "What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study", "Gender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts", "TWBias: A Benchmark for Assessing Social Bias in Traditional Chinese Large Language Models through a Taiwan Cultural Lens", "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective", "Local Contrastive Editing of Gender Stereotypes", "Humans or LLMs as the Judge? A Study on Judgement Bias", "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework", "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models", "Does Context Help Mitigate Gender Bias in Neural Machine Translation?", "\"You Gotta be a Doctor, Lin\": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations"], "Planning": ["MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making", "An Evaluation Mechanism of LLM-based Agents on Manipulating APIs", "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models", "CAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans", "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models", "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "Explaining Mixtures of Sources in News Articles", "Experience as Source for Anticipation and Planning: Experiential Policy Learning for Target-driven Recommendation Dialogues", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "On the Empirical Complexity of Reasoning and Planning in LLMs", "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "ASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "Analysis of Plan-based Retrieval for Grounded Text Generation", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "Thoughts to Target: Enhance Planning for Target-driven Conversation", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context"], "Math": ["Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Nash CoT: Multi-Path Inference with Preference Equilibrium", "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "SCIAGENT: Tool-augmented Language Models for Scientific Reasoning", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "Task Oriented In-Domain Data Augmentation", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision", "Weak-to-Strong Reasoning", "Skills-in-Context: Unlocking Compositionality in Large Language Models", "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks"], "Retrieval": ["KAR\u00b3L: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students", "R\u00b3-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering", "Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision", "Lost in Translation: Chemical Language Models and the Misunderstanding of Molecule Structures", "From RAG to RICHES: Retrieval Interlaced with Sequence Generation", "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "Analysis of Plan-based Retrieval for Grounded Text Generation", "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval", "LONGEMBED: Extending Embedding Models for Long Context Retrieval", "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach"], "Adversarial Attack": ["Ranking Manipulation for Conversational Search Engines", "Enhancing Learning-Based Binary Code Similarity Detection Model through Adversarial Training with Multiple Function Variants", "BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting", "RAFT: Realistic Attacks to Fool Text Detectors", "RAt: Injecting Implicit Bias for Text-To-Image Prompt Refinement Models", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models", "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions", "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "DA\u00b3: A Distribution-Aware Adversarial Attack against Language Models", "Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology", "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations", "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks", "Robust Text Classification: Analyzing Prototype-Based Networks", "Reasoning Robustness of LLMs to Adversarial Typographical Errors", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models"], "Representation Learning": ["HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs", "Bridging Local Details and Global Context in Text-Attributed Graphs", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "Gender Identity in Pretrained Language Models: An Inclusive Approach to Data Creation and Probing", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Automated Tone Transcription and Clustering with Tone2Vec", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement", "MINERS: Multilingual Language Models as Semantic Retrievers", "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval", "Can Transformers Learn n-gram Language Models?", "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs", "Leveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training", "From Text Segmentation to Enhanced Representation Learning: A Novel Approach to Multi-Label Classification for Long Texts", "Modeling Historical Relevant and Local Frequency Context for Representation-Based Temporal Knowledge Graph Forecasting"], "Error Correction": ["C-LLM: Learn to Check Chinese Spelling Errors Character by Character", "Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method", "Retrieved In-Context Principles from Previous Mistakes", "ARM: An Alignment-and-Replacement Module for Chinese Spelling Check Based on LLMs", "Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models", "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check", "E2CL: Exploration-based Error Correction Learning for Embodied Agents", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "COFFEE-GYM: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "Resilience of Large Language Models for Noisy Instructions", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving", "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models", "XTOWER: A Multilingual LLM for Explaining and Correcting Translation Errors", "C-ICL: Contrastive In-context Learning for Information Extraction"], "Continual Learning": ["ICL: Iterative Continual Learning for Multi-domain Neural Machine Translation", "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging", "Unlocking Continual Learning Abilities in Language Models", "Lifelong Event Detection via Optimal Transport", "COGEN: Learning from Feedback with Coupled Comprehension and Generation", "Gradient Localization Improves Lifelong Pretraining of Language Models", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging", "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models", "SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models", "Explicit Memory Learning with Expectation Maximization", "Preserving Generalization of Language Models in Few-shot Continual Relation Extraction", "Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech", "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "TL-CL: Task And Language Incremental Continual Learning", "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis", "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting"], "Prompt Optimization": ["GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "METAREFLECTION: Learning Instructions for Language Agents using Past Reflections", "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "LLM as a metric critic for low resource relation identification", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "AMPO: Automatic Multi-Branched Prompt Optimization", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "Prompts have evil twins", "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling", "Dual-Phase Accelerated Prompt Optimization", "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "Repairing Catastrophic-Neglect in Text-to-Image Diffusion Models via Attention-Guided Feature Enhancement"], "Ranking": ["R\u00b3-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "Make Large Language Model a Better Ranker", "TROTR: A Framework for Evaluating the Recontextualization of Text", "VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models", "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems", "Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation", "How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?", "AGRAME: Any-Granularity Ranking with Multi-Vector Embeddings", "Exploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering", "A Study of Implicit Ranking Unfairness in Large Language Models", "LEGOBENCH: Scientific Leaderboard Generation Benchmark", "Efficient LLM Comparative Assessment: A Product of Experts Framework for Pairwise Comparisons", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing"], "Clustering": ["Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Unsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance", "Story Morals: Surfacing value-driven narrative schemas using large language models", "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing", "FASTTRACK: Reliable Fact Tracing via Clustering and LLM-Powered Evidence Validation", "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA", "Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation", "A Generic Method for Fine-grained Category Discovery in Natural Language Texts", "Pseudo-Label Enhanced Prototypical Contrastive Learning for Uniformed Intent Discovery", "Voices in a Crowd: Searching for Clusters of Unique Perspectives", "Tracking the perspectives of interacting language models", "Leveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training", "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning", "Open-world Multi-label Text Classification with Extremely Weak Supervision", "Latent Concept-based Explanation of NLP Models"], "Mixture Of Experts": ["BiMediX: Bilingual Medical Mixture of Experts LLM", "Efficient and Interpretable Grammatical Error Correction with Mixture of Experts", "MODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning", "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models", "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "MedCoT: Medical Chain of Thought via Hierarchical Expert", "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models", "FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models", "AdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "OPEN-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models", "LEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models", "Turn Waste into Worth: Rectifying Top-k Router of MoE", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "MMOE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts"], "Long Context": ["LLOCO: Learning Long Contexts Offline", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs", "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "A Simple and Effective L2 Norm-Based Strategy for KV Cache Compression", "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "Memorize Step by Step: Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "LONGEMBED: Extending Embedding Models for Long Context Retrieval", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP", "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "LONGHEADS: Multi-Head Attention is Secretly a Long Context Processor"], "Clip": ["EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP", "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models", "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "Updating CLIP to Prefer Descriptions Over Captions", "VPL: Visual Proxy Learning Framework for Zero-Shot Medical Image Diagnosis", "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification", "RWKV-CLIP: A Robust Vision-Language Representation Learner", "MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification", "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought", "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models", "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP"], "Toxicity": ["Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "Style-Shifting Behaviour of the Manosphere on Reddit", "Promoting Constructive Deliberation: Reframing for Receptiveness", "Preference Tuning For Toxicity Mitigation Generalizes Across Languages", "Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "Evaluating Psychological Safety of Large Language Models", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Data, Data Everywhere: A Guide for Pretraining Dataset Construction", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Style-Specific Neurons for Steering LLMs in Text Style Transfer", "SAFETY-J: Evaluating Safety with Critique", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models"], "Automatic Speech Recognition": ["Modeling Gender and Dialect Bias in Automatic Speech Recognition", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models", "Unveiling the Role of Pretraining in Direct Speech Translation", "On Mitigating Performance Disparities in Multilingual Speech Recognition", "Optimized Speculative Sampling for GPU Hardware Accelerators", "Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding", "Exploring the Potential of Multimodal LLM with Knowledge-Intensive Multimodal ASR", "Are Modern Neural ASR Architectures Robust for Polysynthetic Languages?", "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper", "Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Advancing Test-Time Adaptation in Wild Acoustic Test Settings", "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models", "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects"], "Security": ["BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting", "SecureSQL: Evaluating Data Leakage of Large Language Models as Natural Language Interfaces to Databases", "Jailbreaking LLMs with Arabic Transliteration and Arabizi", "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model", "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation", "Where Am I From? Identifying Origin of LLM-generated Content", "Securing Multi-turn Conversational Language Models From Distributed Backdoor Triggers", "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models", "Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models", "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking", "Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion", "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks", "CLEANGEN: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models", "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers", "Look Who's Talking Now: Covert Channels From Biased LLMs", "Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients"], "Benchmarks Dataset": ["Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis", "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues", "DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting", "BOOLQUESTIONS: Does Dense Retrieval Understand Boolean Logic in Language?", "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "Holistic Evaluation for Interleaved Text-and-Image Generation", "Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights", "Editing Conceptual Knowledge for Large Language Models", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Do LLMs Know to Respect Copyright Notice?", "GLOBESUMM: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization", "Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?", "DATATALES: A Benchmark for Real-World Intelligent Data Narration", "FAME: Towards Factual Multi-Task Model Editing", "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice"], "Active Learning": ["Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "ALVIN: Active Learning Via INterpolation", "On the Fragility of Active Learners for Text Classification", "Annotator-Centric Active Learning for Subjective NLP Tasks", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "Efficient Active Learning with Adapters", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "An L* Algorithm for Deterministic Weighted Regular Languages", "ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "Active Learning for Abstractive Text Summarization via LLM-Determined Curriculum and Certainty Gain Maximization", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model"], "Factual Knowledge": ["On the Robustness of Editing Large Language Models", "A Thorough Examination of Decoding Methods in the Era of LLMs", "FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition", "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?", ">Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "To Know or Not to Know? Analyzing Self-Consistency of Large Language Models under Ambiguity", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models"], "Topic Modeling": ["ROBERT2VecTM: A Novel Approach for Topic Extraction in Islamic Studies", "PhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study", "Topic Modeling: Contextual Token Embeddings Are All You Need", "Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature", "Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora", "Unsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance", "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs", "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature", "Characterizing Text Datasets with Psycholinguistic Features", "NeuroMax: Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization", "The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?", "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement", "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments", "On Fake News Detection with LLM Enhanced Semantics Mining", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias"], "Emotion Recognition": ["Towards Robust Speech Representation Learning for Thousands of Languages", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Visual Prompting in LLMs for Enhancing Emotion Recognition", "Emosical: An Emotion-Annotated Musical Theatre Dataset", "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "DetectiveNN: Imitating Human Emotional Reasoning with a Recall-Detect-Predict Framework for Emotion Recognition in Conversations", "PALM: Few-Shot Prompt Learning for Audio Language Models", "Worry Words: Norms of Anxiety Association for over 44k English Words", "BLSP-Emo: Towards Empathetic Large Speech-Language Models", "Vanessa : Visual Connotation and Aesthetic Attributes Understanding Network for Multimodal Aspect-based Sentiment Analysis", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages", "UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause"], "Explainable Ai": ["Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting", "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning", "Evaluating Readability and Faithfulness of Concept-based Explanations", "CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems", "Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach", "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems", "Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse", "Improving LLM Attributions with Randomized Path-Integration", "Multilingual Fine-Grained News Headline Hallucination Detection", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "Generative Models for Automatic Medical Decision Rule Extraction from Text", "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI", "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments", "Enhancing Advanced Visual Reasoning Ability of Large Language Models"], "Multi-modal Learning": ["Creative Problem Solving in Large Language and Vision Models \u2013 What Would it Take?", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "MatchTime: Towards Automatic Soccer Game Commentary Generation", "Towards One-to-Many Visual Question Answering", "FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models", "GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation", "MOSEL: Inference Serving Using Dynamic Modality Selection", "Multi-Level Information Retrieval Augmented Generation for Knowledge-based Visual Question Answering", "SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering", "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "Fine-Grained Prediction of Reading Comprehension from Eye Movements", "RECANTFormer: Referring Expression Comprehension with Varying Numbers of Targets", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "Enhancing Advanced Visual Reasoning Ability of Large Language Models", "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality"], "ML": ["Towards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs", "C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits", "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?", "Verba volant, scripta volant? Don't worry! There are computational solutions for protoword reconstruction", "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models", "On the Rigour of Scientific Writing: Criteria, Analysis, and Insights", "Large Language Models for Data Annotation and Synthesis: A Survey", "Understanding \u201cDemocratization\u201d in NLP and ML Research", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "Knowledge Mechanisms in Large Language Models: A Survey and Perspective", "Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions", "OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants", "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts", "A Survey on In-context Learning"], "Open-domain QA": ["Large Language Models Can Self-Correct with Key Condition Verification", "RaFe: Ranking Feedback Improves Query Rewriting for RAG", "Chain-of-Rewrite: Aligning Question and Documents for Open-Domain Question Answering", "BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering", "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering", "From RAG to RICHES: Retrieval Interlaced with Sequence Generation", "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "Improving Zero-shot LLM Re-Ranker with Risk Minimization", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts", "Evidence Retrieval for Fact Verification using Multi-stage Reranking", "Exploring Hint Generation Approaches in Open-Domain Question Answering", "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation"], "Dialogue Systems": ["Evaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding", "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "Efficient Sequential Decision Making with Large Language Models", "From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues", "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions", "LLMs as Collaborator: Demands-Guided Collaborative Retrieval-Augmented Generation for Commonsense Knowledge-Grounded Open-Domain Dialogue Systems", "Class Name Guided Out-of-Scope Intent Classification", "DC-Instruct: An Effective Framework for Generative Multi-intent Spoken Language Understanding", "Be Helpful but Don't Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support", "Large Language Models Know What To Say But Not When to Speak"], "Self-consistency": ["A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Calibrating Long-form Generations from Large Language Models", "Tree of Problems: Improving structured problem solving with compositionality", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts", "Regression-aware Inference with LLMs", "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "Atomic Self-Consistency for Better Long Form Generations", "Self-Consistency Boosts Calibration for Math Reasoning", "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "To Know or Not to Know? Analyzing Self-Consistency of Large Language Models under Ambiguity", "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees"], "Prompt Tuning": ["Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization", "LLM as a metric critic for low resource relation identification", "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "Understanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs", "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "Divide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning", "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "Achieving Stronger Generation via Simple Contrastive Tuning", "IntCoOp: Interpretability-Aware Vision-Language Prompt Tuning"], "Vqa": ["VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "Visual Question Decomposition on Multimodal Large Language Models", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "Towards One-to-Many Visual Question Answering", "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding", "A Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect", "CommVQA: Situating Visual Question Answering in Communicative Contexts", "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis", "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments", "In-Context Compositional Generalization for Large Vision-Language Models", "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models", "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM", "Improving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design"], "Safety Alignment": ["DATA ADVISOR: Dynamic Data Curation for Safety Alignment of Large Language Models", "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models", "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States", "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking", "Can Textual Unlearning Solve Cross-Modality Safety Alignment?", "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks", "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers", "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction", "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance", "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch", "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations"], "Language Model Alignment": ["Rethinking the Role of Proxy Rewards in Language Model Alignment", "Evolutionary Contrastive Distillation for Language Model Alignment", "Reverse-Engineering the Reader", "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "Filtered Direct Preference Optimization", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "Inference-Time Language Model Alignment via Integrated Value Guidance", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "LIONS: An Empirically Optimized Approach to Align Language Models", "Enhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing", "Preference-Guided Reflective Sampling for Aligning Language Models", "FACTALIGN: Long-form Factuality Alignment of Large Language Models", "COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization"], "Transferability": ["Rethinking the Evaluation of In-Context Learning for LLMs", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Transferability of Syntax-Aware Graph Neural Networks in Zero-Shot Cross-Lingual Semantic Role Labeling", "DA\u00b3: A Distribution-Aware Adversarial Attack against Language Models", "Prompts have evil twins", "Demystifying Verbatim Memorization in Large Language Models", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings", "MIXTURE-OF-SKILLS: Learning to Optimize Data Usage for Fine-Tuning Large Language Models", "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability", "Extracting Prompts by Inverting LLM Outputs", "On the Fragility of Active Learners for Text Classification", "Adversarial Math Word Problem Generation", "When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models"], "Debiasing": ["Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference", "Recent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models", "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization", "Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue", "A Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect", "Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding", "Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias", "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "Efficient Overshadowed Entity Disambiguation by Mitigating Shortcut Learning", "Decoding Matters: Addressing Amplification Bias and Homogeneity Issue for LLM-based Recommendation", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding"], "Asr": ["Modeling Gender and Dialect Bias in Automatic Speech Recognition", "Towards Robust Speech Representation Learning for Thousands of Languages", "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR", "Unveiling the Role of Pretraining in Direct Speech Translation", "Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding", "Are Modern Neural ASR Architectures Robust for Polysynthetic Languages?", "LLMs Are Zero-Shot Context-Aware Simultaneous Translators", "Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "STTATTS: Unified Speech-To-Text And Text-To-Speech Model", "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations"], "Pruning": ["FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping", "Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation", "Pruning Multilingual Large Language Models for Multilingual Inference", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "Rethinking Token Reduction for State Space Models", "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging", "Threshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "Pruning Foundation Models for High Accuracy without Retraining", "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models", "Local Contrastive Editing of Gender Stereotypes"], "Multi-hop Reasoning": ["CNEQ: Incorporating numbers into Knowledge Graph Reasoning", "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "Augmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering", "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models", "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "Cross-Lingual Multi-Hop Knowledge Editing"], "Mechanistic Interpretability": ["On the Similarity of Circuits across Languages: A Case Study on the Subject-verb Agreement Task", "Information Flow Routes: Automatically Interpreting Language Models at Scale", "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis", "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models", "Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning", "Activation Scaling for Steering and Interpreting Language Models", "Preference Tuning For Toxicity Mitigation Generalizes Across Languages", "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning", "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models", "Neuron-Level Knowledge Attribution in Large Language Models", "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions"], "Knowledge Retrieval": ["Large Language Models are Limited in Out-of-Context Knowledge Reasoning", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs", "H-LegalKI: A Hierarchical Legal Knowledge Integration Framework for Legal Community Question Answering", "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature", "Atomic Self-Consistency for Better Long Form Generations", "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance", "An LLM-Enabled Knowledge Elicitation and Retrieval Framework for Zero-Shot Cross-Lingual Stance Identification", "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering", "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling", "Learning to Match Representations is Better for End-to-End Task-Oriented Dialog System", "Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System", "Dissecting Fine-Tuning Unlearning in Large Language Models", "SCIPROMPT: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics"], "Knowledge Transfer": ["ICL: Iterative Continual Learning for Multi-domain Neural Machine Translation", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation", "An LLM-Enabled Knowledge Elicitation and Retrieval Framework for Zero-Shot Cross-Lingual Stance Identification", "PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering", "Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting", "Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "Data-Centric AI in the Age of Large Language Models"], "Topic Classification": ["Transfer Learning for Text Classification via Model Risk Analysis", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "In-Context Learning with Iterative Demonstration Selection", "Multilingual Topic Classification in X: Dataset and Analysis", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "Inference and Verbalization Functions During In-Context Learning", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "Dual-Phase Accelerated Prompt Optimization", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage"], "Generative Models": ["Altogether: Image Captioning via Re-aligning Alt-text", "Compare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY", "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs", "Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments", "GREEN: Generative Radiology Report Evaluation and Error Notation", "GRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "MINERS: Multilingual Language Models as Semantic Retrievers", "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "AudioVSR: Enhancing Video Speech Recognition with Audio Data", "Generative Models for Automatic Medical Decision Rule Extraction from Text", "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP", "Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension"], "Evaluation Metric": ["BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation", "FAITHSCORE: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models", "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation", "LONG2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall", "Learning to Extract Structured Entities Using Language Models", "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models", "Unveiling the Invisible: Captioning Videos with Metaphors", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Holistic Evaluation for Interleaved Text-and-Image Generation", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation", "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric", "Reference-based Metrics Disprove Themselves in Question Generation", "RaTEScore: A Metric for Radiology Report Generation"], "Medical QA": ["MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "MedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures", "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs.", "AMPO: Automatic Multi-Branched Prompt Optimization", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures", "RoQLlama: A Lightweight Romanian Adapted Language Model", "Large Language Models are In-context Teachers for Knowledge Reasoning", "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks", "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration", "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate", "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering", "Few shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering", "LM\u00b2: A Simple Society of Language Models Solves Complex Reasoning"], "Data Generation": ["LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts", "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "Table Question Answering for Low-resourced Indic Languages", "MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification", "Leveraging pre-trained language models for linguistic analysis: A case of argument structure constructions", "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese", "Incubating Text Classifiers Following User Instruction with Nothing but LLM"], "Text-to-image Generation": ["Altogether: Image Captioning via Re-aligning Alt-text", "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation", "GOME: Grounding-based Metaphor Binding With Conceptual Elaboration For Figurative Language Illustration", "Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training", "Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training", "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation", "ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation", "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "Precision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model", "Repairing Catastrophic-Neglect in Text-to-Image Diffusion Models via Attention-Guided Feature Enhancement", "Multimodal Procedural Planning via Dual Text-Image Prompting"], "Relevance": ["Topic-Oriented Open Relation Extraction with A Priori Seed Generation", "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics", "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation", "Learning to Rank Salient Content for Query-focused Summarization", "Improve Dense Passage Retrieval with Entailment Tuning", "Evaluating D-MERIT of Partial-annotation on Information Retrieval", "Threshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval", "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Factuality of Large Language Models: A Survey", "Be Helpful but Don't Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support", "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing"], "Perplexity": ["ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "Evaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "Adversarial Text Generation using Large Language Models for Dementia Detection", "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models", "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "Social Bias Probing: Fairness Benchmarking for Language Models", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "Monotonic Paraphrasing Improves Generalization of Language Model Prompting", "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models", "How to Compute the Probability of a Word", "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging"], "Dataset Creation": ["Detecting Temporal Ambiguity in Questions", "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "Emosical: An Emotion-Annotated Musical Theatre Dataset", "COCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds", "From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues", "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.", "Ask the experts: Sourcing a high-quality nutrition counseling dataset through Human-AI collaboration", "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "CACTUS: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory", "ARTS: Assessing Readability & Text Simplicity", "CoCoHD: Congress Committee Hearing Dataset"], "Large Multimodal Models": ["Towards Low-Resource Harmful Meme Detection with LMM Agents", "M5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks", "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification", "Recent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models", "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models", "DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "Mitigating Language Bias of LMMs in Social Intelligence Understanding with Virtual Counterfactual Calibration", "Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "UNICORN: A Unified Causal Video-Oriented Language-Modeling Framework for Temporal Video-Language Tasks", "Navigating Hallucinations for Reasoning of Unintentional Activities", "MULTISKILL: Evaluating Large Multimodal Models for Fine-grained Alignment Skills"], "Trustworthiness": ["Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "Generating Media Background Checks for Automated Source Critical Reasoning", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "Knowledge Conflicts for LLMs: A Survey", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation", "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration", "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate", "Knowledge Mechanisms in Large Language Models: A Survey and Perspective", "Factuality of Large Language Models: A Survey", "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models", "TrustAgent: Towards Safe and Trustworthy LLM-based Agents"], "Chinese": ["CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "CEAMC: Corpus and Empirical Study of Argument Analysis in Education via LLMs", "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays", "Employing Glyphic Information for Chinese Event Extraction with Vision-Language Model", "OEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary", "ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations", "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models", "Re-Evaluating Evaluation for Multilingual Summarization", "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework", "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs", "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction"], "Scalability": ["Waterfall: Scalable Framework for Robust Text Watermarking and Provenance for LLMs", "Bridging Local Details and Global Context in Text-Attributed Graphs", "Efficient Pointwise-Pairwise Learning-to-Rank for News Recommendation", "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation", "Optimizing Code Retrieval: High-Quality and Scalable Dataset Annotation through Large Language Models", "HyQE: Ranking Contexts with Hypothetical Query Embeddings", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "MisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"", "Working Memory Identifies Reasoning Limits in Language Models", "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning", "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts"], "Generalizability": ["Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models", "What is \u201cTypological Diversity\u201d in NLP?", "Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding", "Text Fluoroscopy: Detecting LLM-Generated Text through Intrinsic Features", "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets", "Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "SecCoder: Towards Generalizable and Robust Secure Code Generation", "On Training Data Influence of GPT Models", "LM\u00b2: A Simple Society of Language Models Solves Complex Reasoning", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context"], "Downstream Tasks": ["TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Moral Foundations of Large Language Models", "Textual Dataset Distillation via Language Model Embedding", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Downstream Trade-offs of a Family of Text Watermarks", "Collaborative Performance Prediction for Large Language Models", "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning?", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models"], "Knowledge Representation": ["Explicit Inductive Inference using Large Language Models", "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs", "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering", "Enhancing Incremental Summarization with Structured Representations", "SATYRN: A Platform for Analytics Augmented Generation", "Dense Passage Retrieval: Is it Retrieving?", "Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study", "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "KAR\u00b3L: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students", "CONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules", "Inference Helps PLMs' Conceptual Understanding: Improving the Abstract Inference Ability with Hierarchical Conceptual Entailment Graphs"], "Multilingual Llms": ["Representational Isomorphism and Alignment of Multilingual Large Language Models", "Teaching LLMs to Abstain across Languages via Multilingual Feedback", "Preference Tuning For Toxicity Mitigation Generalizes Across Languages", "Exploring Multilingual Concepts of Human Values in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "Pruning Multilingual Large Language Models for Multilingual Inference", "An Analysis of Multilingual FActScore", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Understanding and Mitigating Language Confusion in LLMs", "Concept Space Alignment in Multilingual LLMs", "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?", "How Does Quantization Affect Multilingual LLMs?", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language"], "Pretraining": ["Tending Towards Stability: Convergence Challenges in Small Language Models", "Gradient Localization Improves Lifelong Pretraining of Language Models", "Unveiling the Role of Pretraining in Direct Speech Translation", "Is Child-Directed Speech Effective Training Data for Language Models?", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging", "VIMI: Grounding Video Generation through Multi-modal Instruction", "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection", "BLSP-Emo: Towards Empathetic Large Speech-Language Models", "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients", "Data-Centric AI in the Age of Large Language Models"], "Misinformation": ["On the Relationship between Truth and Political Bias in Language Models", "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "Can Language Models Recognize Convincing Arguments?", "Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach", "F2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation", "Can Large Language Models Identify Authorship?", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "SAFETY-J: Evaluating Safety with Critique", "MisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"", "ECON: On the Detection and Resolution of Evidence Conflicts", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models"], "Continual Pre-training": ["Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "Unlocking Continual Learning Abilities in Language Models", "Improving Referring Ability for Biomedical Language Models", "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models", "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models", "Task Oriented In-Domain Data Augmentation", "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "Unlocking the Potential of Model Merging for Low-Resource Languages", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "Pretraining Language Models Using Translationese"], "Fact Verification": ["\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "How Entangled is Factuality and Deception in German?", "Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification", "Mitigating Hallucination in Fictional Character Role-Play", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "Zero-Shot Fact Verification via Natural Logic and Large Language Models", "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization", "Evidence Retrieval for Fact Verification using Multi-stage Reranking", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "Cross-Lingual Multi-Hop Knowledge Editing"]};
    window.addEventListener('DOMContentLoaded', () => {
        const plotDiv = document.querySelector(".plotly-graph-div");

        const titleBox = document.createElement("div");
        titleBox.id = "titleBox";
        titleBox.style.width = "90%";
        titleBox.style.maxHeight = "300px";
        titleBox.style.margin = "20px auto";
        titleBox.style.border = "1px solid gray";
        titleBox.style.overflowY = "scroll";
        titleBox.style.padding = "10px";
        titleBox.style.fontSize = "16px";
        titleBox.innerHTML = "Click the Bars! You can see papers of the specific keyword";
        document.body.appendChild(titleBox);

        plotDiv.on('plotly_click', function(data) {
            const displayLabel = data.points[0].x;
            const titles = ngramMap[displayLabel] || [];
            let content = `<b style='font-size:18px;'>${displayLabel}</b><br><br>`;
            titles.forEach((t, i) => {
                content += `${i+1}. ${t}<br>`;
            });
            titleBox.innerHTML = content;
        });
    });
    </script>
    </body></html>
