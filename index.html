<html><head><meta charset='utf-8'><title>Keyword Plot</title></head><body><div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script>                <div id="95e2ccaa-5fee-4474-9bdd-7a7d67572a8b" class="plotly-graph-div" style="height:700px; width:3200px;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("95e2ccaa-5fee-4474-9bdd-7a7d67572a8b")) {                    Plotly.newPlot(                        "95e2ccaa-5fee-4474-9bdd-7a7d67572a8b",                        [{"hoverinfo":"text","marker":{"line":{"width":0}},"text":["\u003cb\u003eQA\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eEvaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection\u003cbr\u003eEvaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection\u003cbr\u003eBlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering\u003cbr\u003eLookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps\u003cbr\u003eLookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps\u003cbr\u003eTriad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering\u003cbr\u003eSelf-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering\u003cbr\u003eAligning Language Models to Explicitly Handle Ambiguity\u003cbr\u003eAligning Language Models to Explicitly Handle Ambiguity\u003cbr\u003e... and 258 more","\u003cb\u003eIn-context Learning\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eOn Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices\u003cbr\u003eFLIRT: Feedback Loop In-context Red Teaming\u003cbr\u003eIn-context Contrastive Learning for Event Causality Identification\u003cbr\u003eWhat's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs\u003cbr\u003eA Survey on In-context Learning\u003cbr\u003eEffective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eSTANDARDIZE: Aligning Language Models with Expert-Defined Standards for Content Generation\u003cbr\u003e... and 162 more","\u003cb\u003eFine-tuning\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eSystematic Biases in LLM Simulations of Debates\u003cbr\u003eFine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?\u003cbr\u003eOn Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eModel Balancing Helps Low-data Training and Fine-tuning\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eLogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models\u003cbr\u003eDissecting Fine-Tuning Unlearning in Large Language Models\u003cbr\u003e... and 122 more","\u003cb\u003eNatural Language Processing\u003c\u002fb\u003e\u003cbr\u003eCOCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds\u003cbr\u003eA Survey on In-context Learning\u003cbr\u003eA Survey on In-context Learning\u003cbr\u003eModel Balancing Helps Low-data Training and Fine-tuning\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003eSurveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese\u003cbr\u003eQUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios\u003cbr\u003eEvaluating Large Language Models via Linguistic Profiling\u003cbr\u003eGOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory\u003cbr\u003e... and 115 more","\u003cb\u003eZero-shot Learning\u003c\u002fb\u003e\u003cbr\u003eUNIGEN: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation\u003cbr\u003eTable Question Answering for Low-resourced Indic Languages\u003cbr\u003eLLMs Are Zero-Shot Context-Aware Simultaneous Translators\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eReuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment\u003cbr\u003ePredicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement\u003cbr\u003ePsFuture: A Pseudo-Future-based Zero-Shot Adaptive Policy for Simultaneous Machine Translation\u003cbr\u003eTCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003e... and 114 more","\u003cb\u003eRAG\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eRULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models\u003cbr\u003eMAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering\u003cbr\u003eTowards Low-Resource Harmful Meme Detection with LMM Agents\u003cbr\u003eKnowledge Verification to Nip Hallucination in the Bud\u003cbr\u003eSEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation\u003cbr\u003eEfficientRAG: Efficient Retriever for Multi-Hop Question Answering\u003cbr\u003eEfficientRAG: Efficient Retriever for Multi-Hop Question Answering\u003cbr\u003eCoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering\u003cbr\u003e... and 111 more","\u003cb\u003eReasoning\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eAfrican or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification\u003cbr\u003ePhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study\u003cbr\u003eTeaching LLMs to Abstain across Languages via Multilingual Feedback\u003cbr\u003eWhen Reasoning Meets Information Aggregation: A Case Study with Sports Narratives\u003cbr\u003eHierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization\u003cbr\u003eMulti-Granularity History and Entity Similarity Learning for Temporal Knowledge Graph Reasoning\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eTo Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models\u003cbr\u003e... and 101 more","\u003cb\u003eInstruction Tuning\u003c\u002fb\u003e\u003cbr\u003eClustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation\u003cbr\u003eParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\u003cbr\u003eParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\u003cbr\u003eLLMs Are Zero-Shot Context-Aware Simultaneous Translators\u003cbr\u003eChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval\u003cbr\u003eQUDSELECT: Selective Decoding for Questions Under Discussion Parsing\u003cbr\u003eTowards Tool Use Alignment of Large Language Models\u003cbr\u003eInstruction Pre-Training: Language Models are Supervised Multitask Learners\u003cbr\u003eKnowledge Verification to Nip Hallucination in the Bud\u003cbr\u003eGOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory\u003cbr\u003e... and 98 more","\u003cb\u003eText Classification\u003c\u002fb\u003e\u003cbr\u003eOn Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices\u003cbr\u003eOn Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices\u003cbr\u003eIntegrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification\u003cbr\u003eA Generic Method for Fine-grained Category Discovery in Natural Language Texts\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003eText Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification\u003cbr\u003eText Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification\u003cbr\u003eIncubating Text Classifiers Following User Instruction with Nothing but LLM\u003cbr\u003eIncubating Text Classifiers Following User Instruction with Nothing but LLM\u003cbr\u003e... and 97 more","\u003cb\u003eGeneralization\u003c\u002fb\u003e\u003cbr\u003eParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\u003cbr\u003eLanguage Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS\u003cbr\u003eRethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization\u003cbr\u003eChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval\u003cbr\u003eMetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic\u003cbr\u003eInstruction Pre-Training: Language Models are Supervised Multitask Learners\u003cbr\u003eInstruction Pre-Training: Language Models are Supervised Multitask Learners\u003cbr\u003eThe Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models\u003cbr\u003eTeaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use\u003cbr\u003eA Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners\u003cbr\u003e... and 92 more","\u003cb\u003eMachine Translation\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?\u003cbr\u003eFine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?\u003cbr\u003eChain-of-Dictionary Prompting Elicits Translation in Large Language Models\u003cbr\u003eLLMs Are Zero-Shot Context-Aware Simultaneous Translators\u003cbr\u003eWord Alignment as Preference for Machine Translation\u003cbr\u003eWord Alignment as Preference for Machine Translation\u003cbr\u003eMiTTenS: A Dataset for Evaluating Gender Mistranslation\u003cbr\u003eVoices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects\u003cbr\u003eVoices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects\u003cbr\u003eMultiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing\u003cbr\u003e... and 91 more","\u003cb\u003eFew-shot Learning\u003c\u002fb\u003e\u003cbr\u003eA Survey on In-context Learning\u003cbr\u003eA Survey on In-context Learning\u003cbr\u003eAn Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification\u003cbr\u003eSelf-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003eIntegrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training\u003cbr\u003eTowards Low-Resource Harmful Meme Detection with LMM Agents\u003cbr\u003eTake Off the Training Wheels! Progressive In-Context Learning for Effective Alignment\u003cbr\u003eEvaluating Large Language Models via Linguistic Profiling\u003cbr\u003e... and 90 more","\u003cb\u003eData Augmentation\u003c\u002fb\u003e\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003eGeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation\u003cbr\u003eExploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors\u003cbr\u003eExploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors\u003cbr\u003eAn Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification\u003cbr\u003eAn Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification\u003cbr\u003eInstruction Pre-Training: Language Models are Supervised Multitask Learners\u003cbr\u003eDeciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning\u003cbr\u003eMultiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing\u003cbr\u003eMuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning\u003cbr\u003e... and 87 more","\u003cb\u003eEvaluation\u003c\u002fb\u003e\u003cbr\u003eEmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eLogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models\u003cbr\u003eLogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models\u003cbr\u003eCUTE: Measuring LLMs' Understanding of Their Tokens\u003cbr\u003eA User-Centric Multi-Intent Benchmark for Evaluating Large Language Models\u003cbr\u003eMiTTenS: A Dataset for Evaluating Gender Mistranslation\u003cbr\u003ePROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models\u003cbr\u003eAcademics Can Contribute to Domain-Specialized Language Models\u003cbr\u003e... and 84 more","\u003cb\u003eBenchmarks\u003c\u002fb\u003e\u003cbr\u003eRoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning\u003cbr\u003eEmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models\u003cbr\u003eBridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eTOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners\u003cbr\u003eVIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values\u003cbr\u003eCan visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003eCUTE: Measuring LLMs' Understanding of Their Tokens\u003cbr\u003eA User-Centric Multi-Intent Benchmark for Evaluating Large Language Models\u003cbr\u003e... and 83 more","\u003cb\u003eSummarization\u003c\u002fb\u003e\u003cbr\u003eMULTI-NEWS+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation\u003cbr\u003eFIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document\u003cbr\u003eLONGEMBED: Extending Embedding Models for Long Context Retrieval\u003cbr\u003eReuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment\u003cbr\u003eReuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment\u003cbr\u003eLookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps\u003cbr\u003eLookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps\u003cbr\u003eDancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models\u003cbr\u003eBayesian Calibration of Win Rate Estimation with LLM Evaluators\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003e... and 78 more","\u003cb\u003eText Generation\u003c\u002fb\u003e\u003cbr\u003ePrompts have evil twins\u003cbr\u003eSHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation\u003cbr\u003eASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings\u003cbr\u003eEvaluating Large Language Models via Linguistic Profiling\u003cbr\u003eStyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements\u003cbr\u003ePromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval\u003cbr\u003eKidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions\u003cbr\u003eContext-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models\u003cbr\u003ePERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts\u003cbr\u003eGold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs\u003cbr\u003e... and 73 more","\u003cb\u003eVisual QA\u003c\u002fb\u003e\u003cbr\u003eExploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors\u003cbr\u003eExploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors\u003cbr\u003eMAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering\u003cbr\u003eMAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering\u003cbr\u003eSelf-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering\u003cbr\u003eTag-grounded Visual Instruction Tuning with Retrieval Augmentation\u003cbr\u003eDocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models\u003cbr\u003eWorld to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering\u003cbr\u003eWorld to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering\u003cbr\u003eFrom the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis\u003cbr\u003e... and 73 more","\u003cb\u003eSentiment Analysis\u003c\u002fb\u003e\u003cbr\u003e\u201cWe Demand Justice!\u201d: Towards Social Context Grounding of Political Texts\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eOvercome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue\u003cbr\u003eIntegrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification\u003cbr\u003eCryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading\u003cbr\u003eDA\u00b3: A Distribution-Aware Adversarial Attack against Language Models\u003cbr\u003eHidden Persuaders: LLMs' Political Leaning and Their Influence on Voters\u003cbr\u003eAdaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks\u003cbr\u003eAdaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks\u003cbr\u003e... and 72 more","\u003cb\u003eRobustness\u003c\u002fb\u003e\u003cbr\u003ePrompts have evil twins\u003cbr\u003eRoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning\u003cbr\u003eImpeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation\u003cbr\u003eEvaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection\u003cbr\u003eChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eAn Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making\u003cbr\u003eAdvancing Large Language Model Attribution through Self-Improving\u003cbr\u003eSOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning\u003cbr\u003eIDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding\u003cbr\u003e... and 69 more","\u003cb\u003eNatural Language Inference\u003c\u002fb\u003e\u003cbr\u003eFIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document\u003cbr\u003eEffective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process\u003cbr\u003eQUDSELECT: Selective Decoding for Questions Under Discussion Parsing\u003cbr\u003eLogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models\u003cbr\u003eFuseGen: PLM Fusion for Data-generation based Zero-shot Learning\u003cbr\u003eIn Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search\u003cbr\u003eQUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios\u003cbr\u003eVerification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving\u003cbr\u003eHow Hard is this Test Set?\nNLI Characterization by Exploiting Training Dynamics\u003cbr\u003eHow Hard is this Test Set?\nNLI Characterization by Exploiting Training Dynamics\u003cbr\u003e... and 68 more","\u003cb\u003eLanguage Modeling\u003c\u002fb\u003e\u003cbr\u003eLanguage Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS\u003cbr\u003eRethinking Token Reduction for State Space Models\u003cbr\u003eInstruction Pre-Training: Language Models are Supervised Multitask Learners\u003cbr\u003eLeading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Probabilities\u003cbr\u003eWhen Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages\u003cbr\u003eTeaching LLMs to Abstain across Languages via Multilingual Feedback\u003cbr\u003eKidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions\u003cbr\u003eContext-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models\u003cbr\u003eCItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling\u003cbr\u003eMitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing\u003cbr\u003e... and 68 more","\u003cb\u003eCommonsense Reasoning\u003c\u002fb\u003e\u003cbr\u003eParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eTeaching Small Language Models Reasoning through Counterfactual Distillation\u003cbr\u003eInvestigating Mysteries of CoT-Augmented Distillation\u003cbr\u003eInvestigating Mysteries of CoT-Augmented Distillation\u003cbr\u003e... and 67 more","\u003cb\u003eContrastive Learning\u003c\u002fb\u003e\u003cbr\u003eSuccessfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections\u003cbr\u003eIn-context Contrastive Learning for Event Causality Identification\u003cbr\u003eChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval\u003cbr\u003eWatch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement\u003cbr\u003eDecoding the Echoes of Vision from fMRI: Memory Disentangling for Past Semantic Information\u003cbr\u003eIntegrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training\u003cbr\u003eSurveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese\u003cbr\u003eMiniConGTS: A Near Ultimate Minimalist Contrastive Grid Tagging Scheme for Aspect Sentiment Triplet Extraction\u003cbr\u003eA Generic Method for Fine-grained Category Discovery in Natural Language Texts\u003cbr\u003eDeciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning\u003cbr\u003e... and 65 more","\u003cb\u003eInterpretability\u003c\u002fb\u003e\u003cbr\u003eFIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document\u003cbr\u003eLearning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation\u003cbr\u003eBackward Lens: Projecting Language Model Gradients into the Vocabulary Space\u003cbr\u003eBackward Lens: Projecting Language Model Gradients into the Vocabulary Space\u003cbr\u003eFrom Insights to Actions: The Impact of Interpretability and Analysis Research on NLP\u003cbr\u003eHow do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning\u003cbr\u003eInterpretability-based Tailored Knowledge Editing in Transformers\u003cbr\u003eStyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements\u003cbr\u003eAn LLM Feature-based Framework for Dialogue Constructiveness Assessment\u003cbr\u003eDiscovering Knowledge-Critical Subnetworks in Pretrained Language Models\u003cbr\u003e... and 63 more","\u003cb\u003eReinforcement Learning\u003c\u002fb\u003e\u003cbr\u003eStrength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation\u003cbr\u003eMitigating the Alignment Tax of RLHF\u003cbr\u003eControllable Preference Optimization: Toward Controllable Multi-Objective Alignment\u003cbr\u003eBe Helpful but Don't Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support\u003cbr\u003eVIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation\u003cbr\u003eVIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation\u003cbr\u003eDirect Multi-Turn Preference Optimization for Language Agents\u003cbr\u003eImproving Multi-party Dialogue Generation via Topic and Rhetorical Coherence\u003cbr\u003eTeaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use\u003cbr\u003eTeaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use\u003cbr\u003e... and 54 more","\u003cb\u003eInstruction Following\u003c\u002fb\u003e\u003cbr\u003eEvaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection\u003cbr\u003eSuccessfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eAMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation\u003cbr\u003eDirect Multi-Turn Preference Optimization for Language Agents\u003cbr\u003eLarge Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course\u003cbr\u003eInstruction Pre-Training: Language Models are Supervised Multitask Learners\u003cbr\u003eDancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models\u003cbr\u003eDancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models\u003cbr\u003eBayesian Calibration of Win Rate Estimation with LLM Evaluators\u003cbr\u003e... and 54 more","\u003cb\u003eCode Generation\u003c\u002fb\u003e\u003cbr\u003eGeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation\u003cbr\u003eAMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation\u003cbr\u003eAMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation\u003cbr\u003eMuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning\u003cbr\u003eAutomatic Instruction Evolving for Large Language Models\u003cbr\u003eEAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees\u003cbr\u003eUnlocking Memorization in Large Language Models with Dynamic Soft Prompting\u003cbr\u003eI Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses\u003cbr\u003eOuroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding\u003cbr\u003eDA-Code: Agent Data Science Code Generation Benchmark for Large Language Models\u003cbr\u003e... and 54 more","\u003cb\u003eInformation Retrieval\u003c\u002fb\u003e\u003cbr\u003eBridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval\u003cbr\u003eEvaluating D-MERIT of Partial-annotation on Information Retrieval\u003cbr\u003eDo Large Language Models Know How Much They Know?\u003cbr\u003eUnifying Multimodal Retrieval via Document Screenshot Embedding\u003cbr\u003eGENRA: Enhancing Zero-shot Retrieval with Rank Aggregation\u003cbr\u003eGENRA: Enhancing Zero-shot Retrieval with Rank Aggregation\u003cbr\u003eFIRST: Faster Improved Listwise Reranking with Single Token Decoding\u003cbr\u003eFIRST: Faster Improved Listwise Reranking with Single Token Decoding\u003cbr\u003eOmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer\u003cbr\u003eOmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer\u003cbr\u003e... and 53 more","\u003cb\u003eVision-language Models\u003c\u002fb\u003e\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003eSuccessfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections\u003cbr\u003ePredicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement\u003cbr\u003eTOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners\u003cbr\u003eEnhancing Advanced Visual Reasoning Ability of Large Language Models\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003eTowards Low-Resource Harmful Meme Detection with LMM Agents\u003cbr\u003eAfrican or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification\u003cbr\u003eDoes Object Grounding Really Reduce Hallucination of Large Vision-Language Models?\u003cbr\u003eDecompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison\u003cbr\u003e... and 52 more","\u003cb\u003eDataset\u003c\u002fb\u003e\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003eCOCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds\u003cbr\u003eDocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing\u003cbr\u003eTOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners\u003cbr\u003eC3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits\u003cbr\u003eMiTTenS: A Dataset for Evaluating Gender Mistranslation\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eLeveraging Conflicts in Social Media Posts: Unintended Offense Dataset\u003cbr\u003eMulti-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges\u003cbr\u003eSTORYSUMM: Evaluating Faithfulness in Story Summarization\u003cbr\u003e... and 52 more","\u003cb\u003eBenchmarksing\u003c\u002fb\u003e\u003cbr\u003eEvaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection\u003cbr\u003eDocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing\u003cbr\u003eAfrican or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification\u003cbr\u003eAn Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eBenchmarking Vision Language Models for Cultural Understanding\u003cbr\u003eFrom Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models\u003cbr\u003eFROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models\u003cbr\u003eASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?\u003cbr\u003ePrecise Model Benchmarking with Only a Few Observations\u003cbr\u003e... and 47 more","\u003cb\u003eMathematical Reasoning\u003c\u002fb\u003e\u003cbr\u003eTable Question Answering for Low-resourced Indic Languages\u003cbr\u003eLearning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing\u003cbr\u003eLearning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing\u003cbr\u003eFrom Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning\u003cbr\u003eFrom Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning\u003cbr\u003eHow Do Humans Write Code? Large Models Do It the Same Way Too\u003cbr\u003eHow Do Humans Write Code? Large Models Do It the Same Way Too\u003cbr\u003eMuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning\u003cbr\u003eMuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003e... and 46 more","\u003cb\u003eChain-of-thought\u003c\u002fb\u003e\u003cbr\u003eMULTI-NEWS+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eChain-of-Dictionary Prompting Elicits Translation in Large Language Models\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eCoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering\u003cbr\u003eARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback\u003cbr\u003eARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback\u003cbr\u003eHow Do Humans Write Code? Large Models Do It the Same Way Too\u003cbr\u003eHow Do Humans Write Code? Large Models Do It the Same Way Too\u003cbr\u003eINDUCT-LEARN: Short Phrase Prompting with Instruction Induction\u003cbr\u003e... and 42 more","\u003cb\u003eLow-resource Languages\u003c\u002fb\u003e\u003cbr\u003eTable Question Answering for Low-resourced Indic Languages\u003cbr\u003eTable Question Answering for Low-resourced Indic Languages\u003cbr\u003eChain-of-Dictionary Prompting Elicits Translation in Large Language Models\u003cbr\u003eZero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection\u003cbr\u003eWhen Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages\u003cbr\u003eTeaching LLMs to Abstain across Languages via Multilingual Feedback\u003cbr\u003eAn Analysis of Multilingual FActScore\u003cbr\u003eMultiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing\u003cbr\u003eUsing Language Models to Disambiguate Lexical Choices in Translation\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003e... and 42 more","\u003cb\u003eTransfer Learning\u003c\u002fb\u003e\u003cbr\u003eTable Question Answering for Low-resourced Indic Languages\u003cbr\u003eLanguage Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS\u003cbr\u003eA Survey on In-context Learning\u003cbr\u003ePre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation\u003cbr\u003eReusing Transferable Weight Increments for Low-resource Style Generation\u003cbr\u003eKB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases\u003cbr\u003eEXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning\u003cbr\u003eConcept Space Alignment in Multilingual LLMs\u003cbr\u003eContext-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models\u003cbr\u003eBoosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?\u003cbr\u003e... and 42 more","\u003cb\u003eAlignment\u003c\u002fb\u003e\u003cbr\u003eEliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence\u003cbr\u003eReuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment\u003cbr\u003eTowards Tool Use Alignment of Large Language Models\u003cbr\u003eAligning Language Models to Explicitly Handle Ambiguity\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eTake Off the Training Wheels! Progressive In-Context Learning for Effective Alignment\u003cbr\u003eAlignment-Enhanced Decoding: Defending Jailbreaks via Token-Level Adaptive Refining of Probability Distributions\u003cbr\u003eSynergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems\u003cbr\u003eADELIE: Aligning Large Language Models on Information Extraction\u003cbr\u003eCan Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?\u003cbr\u003e... and 42 more","\u003cb\u003eKnowledge Distillation\u003c\u002fb\u003e\u003cbr\u003eAMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation\u003cbr\u003eAMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation\u003cbr\u003eDecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models\u003cbr\u003eDocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003eAlignCap: Aligning Speech Emotion Captioning to Human Preferences\u003cbr\u003eMTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval\u003cbr\u003eMTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval\u003cbr\u003eKnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server\u003cbr\u003e... and 42 more","\u003cb\u003eHallucination\u003c\u002fb\u003e\u003cbr\u003eWhispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models\u003cbr\u003eDoes Object Grounding Really Reduce Hallucination of Large Vision-Language Models?\u003cbr\u003eWord Alignment as Preference for Machine Translation\u003cbr\u003eCoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering\u003cbr\u003eWhen Context Leads but Parametric Memory Follows in Large Language Models\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eSaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales\u003cbr\u003eDo Large Language Models Know How Much They Know?\u003cbr\u003eGAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities\u003cbr\u003eAn Audit on the Perspectives and Challenges of Hallucinations in NLP\u003cbr\u003e... and 39 more","\u003cb\u003eParameter-efficient Fine-tuning\u003c\u002fb\u003e\u003cbr\u003eLet the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models\u003cbr\u003eLet the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models\u003cbr\u003eAdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eFrom Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning\u003cbr\u003ePERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts\u003cbr\u003eDemocratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning\u003cbr\u003eMixture-of-Subspaces in Low-Rank Adaptation\u003cbr\u003eFast Forwarding Low-Rank Training\u003cbr\u003eLoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models\u003cbr\u003e... and 38 more","\u003cb\u003eNamed Entity Recognition\u003c\u002fb\u003e\u003cbr\u003eMTLS: Making Texts into Linguistic Symbols\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eADELIE: Aligning Large Language Models on Information Extraction\u003cbr\u003eBio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eNuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data\u003cbr\u003eNuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data\u003cbr\u003eSciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents\u003cbr\u003eZero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages\u003cbr\u003eZero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages\u003cbr\u003e... and 38 more","\u003cb\u003eImage Captioning\u003c\u002fb\u003e\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003eUniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation\u003cbr\u003eHELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding\u003cbr\u003eTag-grounded Visual Instruction Tuning with Retrieval Augmentation\u003cbr\u003eDoes Object Grounding Really Reduce Hallucination of Large Vision-Language Models?\u003cbr\u003eDoes Object Grounding Really Reduce Hallucination of Large Vision-Language Models?\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eHow Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?\u003cbr\u003eBenchmarking Vision Language Models for Cultural Understanding\u003cbr\u003e... and 36 more","\u003cb\u003eExplainability\u003c\u002fb\u003e\u003cbr\u003eEyes Don't Lie: Subjective Hate Annotation and Detection with Gaze\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eAn Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records\u003cbr\u003eAn Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records\u003cbr\u003eSaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales\u003cbr\u003eModel Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation\u003cbr\u003eFiner: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models\u003cbr\u003eXplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs\u003cbr\u003eIf CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions\u003cbr\u003ePANDA: Persona Attributes Navigation for Detecting and Alleviating Overuse Problem in Large Language Models\u003cbr\u003e... and 32 more","\u003cb\u003eBias\u003c\u002fb\u003e\u003cbr\u003eSystematic Biases in LLM Simulations of Debates\u003cbr\u003eAGENTREVIEW: Exploring Peer Review Dynamics with LLM Agents\u003cbr\u003eMiTTenS: A Dataset for Evaluating Gender Mistranslation\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eFormality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge\u003cbr\u003eChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context\u003cbr\u003ePARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003eThe Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention\u003cbr\u003eSusu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.\u003cbr\u003e... and 32 more","\u003cb\u003eSynthetic Data\u003c\u002fb\u003e\u003cbr\u003eScaling Properties of Speech Language Models\u003cbr\u003eFine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?\u003cbr\u003eAdvancing Large Language Model Attribution through Self-Improving\u003cbr\u003eQuality Matters: Evaluating Synthetic Data for Tool-Using LLMs\u003cbr\u003ePretraining Language Models Using Translationese\u003cbr\u003ePersonas as a Way to Model Truthfulness in Language Models\u003cbr\u003eCareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation\u003cbr\u003eEnhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs\u003cbr\u003eEmoknob: Enhance Voice Cloning with Fine-Grained Emotion Control\u003cbr\u003eMiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents\u003cbr\u003e... and 32 more","\u003cb\u003eClassification\u003c\u002fb\u003e\u003cbr\u003eEffective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process\u003cbr\u003eDocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models\u003cbr\u003eCareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation\u003cbr\u003eSURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information\u003cbr\u003eAttribute or Abstain: Large Language Models as Long Document Assistants\u003cbr\u003eIs It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?\u003cbr\u003eMajor Entity Identification: A Generalizable Alternative to Coreference Resolution\u003cbr\u003eApplying Contrastive Learning to Code Vulnerability Type Classification\u003cbr\u003eUnveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism\u003cbr\u003ePcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity\u003cbr\u003e... and 31 more","\u003cb\u003eMulti-task Learning\u003c\u002fb\u003e\u003cbr\u003eLarge Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment\u003cbr\u003eLarge Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment\u003cbr\u003eMetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic\u003cbr\u003eMetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eMODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning\u003cbr\u003eDancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models\u003cbr\u003eDeciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning\u003cbr\u003eRetrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003e... and 31 more","\u003cb\u003eDirect Preference Optimization\u003c\u002fb\u003e\u003cbr\u003eLearning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing\u003cbr\u003eEliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence\u003cbr\u003eEvaluating Psychological Safety of Large Language Models\u003cbr\u003eDirect Multi-Turn Preference Optimization for Language Agents\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eWord Alignment as Preference for Machine Translation\u003cbr\u003eVLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment\u003cbr\u003eADELIE: Aligning Large Language Models on Information Extraction\u003cbr\u003eMDPO: Conditional Preference Optimization for Multimodal Large Language Models\u003cbr\u003eLIONS: An Empirically Optimized Approach to Align Language Models\u003cbr\u003e... and 30 more","\u003cb\u003eEvaluation Metrics\u003c\u002fb\u003e\u003cbr\u003eEvaluating Readability and Faithfulness of Concept-based Explanations\u003cbr\u003eI Could've Asked That: Reformulating Unanswerable Questions\u003cbr\u003eForgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-context Models\u003cbr\u003eBeyond Reference: Evaluating High Quality Translations Better than Human References\u003cbr\u003eUnderstanding and Mitigating Language Confusion in LLMs\u003cbr\u003eIs LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment\u003cbr\u003eEmoknob: Enhance Voice Cloning with Fine-Grained Emotion Control\u003cbr\u003eAPPLS: Evaluating Evaluation Metrics for Plain Language Summarization\u003cbr\u003eSTORYSUMM: Evaluating Faithfulness in Story Summarization\u003cbr\u003eWhat is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations\u003cbr\u003e... and 30 more","\u003cb\u003eMultimodal Learning\u003c\u002fb\u003e\u003cbr\u003eWhen LLMs Meet Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection\u003cbr\u003eWhen LLMs Meet Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection\u003cbr\u003eEyes Don't Lie: Subjective Hate Annotation and Detection with Gaze\u003cbr\u003eExploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors\u003cbr\u003eTOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners\u003cbr\u003eAutoregressive Pre-Training on Pixels and Texts\u003cbr\u003eEmpowering Large Language Model for Continual Video Question Answering with Collaborative Prompting\u003cbr\u003eVIMI: Grounding Video Generation through Multi-modal Instruction\u003cbr\u003eAnalyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts\u003cbr\u003eDistilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP\u003cbr\u003e... and 29 more","\u003cb\u003eEfficiency\u003c\u002fb\u003e\u003cbr\u003eRethinking Token Reduction for State Space Models\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003ePredicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model\u003cbr\u003eLearning to Extract Structured Entities Using Language Models\u003cbr\u003eMulti-pass Decoding for Grammatical Error Correction\u003cbr\u003eTowards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters\u003cbr\u003eSegment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation\u003cbr\u003eRA2FD: Distilling Faithfulness into Efficient Dialogue Systems\u003cbr\u003eFIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation\u003cbr\u003e... and 26 more","\u003cb\u003eFairness\u003c\u002fb\u003e\u003cbr\u003e\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eModular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context\u003cbr\u003e\"You Gotta be a Doctor, Lin\": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations\u003cbr\u003eStrategic Demonstration Selection for Improved Fairness in LLM In-Context Learning\u003cbr\u003eOptimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning\u003cbr\u003eUnlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization\u003cbr\u003eMitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment\u003cbr\u003e... and 25 more","\u003cb\u003eTransformer\u003c\u002fb\u003e\u003cbr\u003eIntegrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification\u003cbr\u003eForgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-context Models\u003cbr\u003eSeeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients\u003cbr\u003eUnlocking Memorization in Large Language Models with Dynamic Soft Prompting\u003cbr\u003eUnveiling the Role of Pretraining in Direct Speech Translation\u003cbr\u003eRationalizing Transformer Predictions via End-To-End Differentiable Self-Training\u003cbr\u003eSegment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation\u003cbr\u003eScalable Efficient Training of Large Language Models with Low-dimensional Projected Attention\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003eVarying Sentence Representations via Condition-Specified Routers\u003cbr\u003e... and 25 more","\u003cb\u003eHallucination Detection\u003c\u002fb\u003e\u003cbr\u003eFIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document\u003cbr\u003eSuccessfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections\u003cbr\u003eLookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps\u003cbr\u003eEmbedding and Gradient Say Wrong: A White-Box Method for Hallucination Detection\u003cbr\u003eEmbedding and Gradient Say Wrong: A White-Box Method for Hallucination Detection\u003cbr\u003eWhen Context Leads but Parametric Memory Follows in Large Language Models\u003cbr\u003eKnowledge-Centric Hallucination Detection\u003cbr\u003ePelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification\u003cbr\u003eCliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios\u003cbr\u003eDetecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors\u003cbr\u003e... and 24 more","\u003cb\u003eCalibration\u003c\u002fb\u003e\u003cbr\u003eConsolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing\u003cbr\u003eRULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models\u003cbr\u003eCalibrating the Confidence of Large Language Models by Eliciting Fidelity\u003cbr\u003eTeaching LLMs to Abstain across Languages via Multilingual Feedback\u003cbr\u003eBayesian Calibration of Win Rate Estimation with LLM Evaluators\u003cbr\u003eLLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives\u003cbr\u003eEnhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding\u003cbr\u003eEnhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding\u003cbr\u003eFIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation\u003cbr\u003eFIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation\u003cbr\u003e... and 24 more","\u003cb\u003eCross-lingual Transfer\u003c\u002fb\u003e\u003cbr\u003eZero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection\u003cbr\u003eCross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages\u003cbr\u003eBreaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale\u003cbr\u003ePREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment\u003cbr\u003eBreaking the Curse of Multilinguality with Cross-lingual Expert Language Models\u003cbr\u003eBreaking the Curse of Multilinguality with Cross-lingual Expert Language Models\u003cbr\u003eThe Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm\u003cbr\u003eTL-CL: Task And Language Incremental Continual Learning\u003cbr\u003eREADME++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment\u003cbr\u003eREADME++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment\u003cbr\u003e... and 24 more","\u003cb\u003eMultilingual\u003c\u002fb\u003e\u003cbr\u003eReuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment\u003cbr\u003eMTLS: Making Texts into Linguistic Symbols\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eQuantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval\u003cbr\u003ePython is Not Always the Best Choice: Embracing Multilingual Program of Thoughts\u003cbr\u003ePARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data\u003cbr\u003eSignCLIP: Connecting Text and Sign Language by Contrastive Learning\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eWhat is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations\u003cbr\u003eUnlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering\u003cbr\u003e... and 23 more","\u003cb\u003eRelation Extraction\u003c\u002fb\u003e\u003cbr\u003eIntegrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training\u003cbr\u003eADELIE: Aligning Large Language Models on Information Extraction\u003cbr\u003eModeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding\u003cbr\u003eBio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints\u003cbr\u003eSciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents\u003cbr\u003eSciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents\u003cbr\u003eGrasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction\u003cbr\u003eGrasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction\u003cbr\u003eLarge Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark\u003cbr\u003ePreserving Generalization of Language Models in Few-shot Continual Relation Extraction\u003cbr\u003e... and 23 more","\u003cb\u003eInformation Extraction\u003c\u002fb\u003e\u003cbr\u003eIntegrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training\u003cbr\u003eLearning to Extract Structured Entities Using Language Models\u003cbr\u003eA Survey of AMR Applications\u003cbr\u003eA Survey of AMR Applications\u003cbr\u003eADELIE: Aligning Large Language Models on Information Extraction\u003cbr\u003eEfficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards\u003cbr\u003eExploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights\u003cbr\u003eText-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction\u003cbr\u003eStructure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text\u003cbr\u003eModeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding\u003cbr\u003e... and 23 more","\u003cb\u003ePre-training\u003c\u002fb\u003e\u003cbr\u003eNumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning\u003cbr\u003eCross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective\u003cbr\u003eIntegrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training\u003cbr\u003eAutoregressive Pre-Training on Pixels and Texts\u003cbr\u003eMTLS: Making Texts into Linguistic Symbols\u003cbr\u003eWhen Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages\u003cbr\u003eRelevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System\u003cbr\u003eDialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction\u003cbr\u003ePretraining Language Models Using Translationese\u003cbr\u003eA Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery\u003cbr\u003e... and 22 more","\u003cb\u003eDomain Adaptation\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eCross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective\u003cbr\u003eAdvancing Test-Time Adaptation in Wild Acoustic Test Settings\u003cbr\u003eTask Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition\u003cbr\u003eTask Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition\u003cbr\u003eDomain adapted machine translation: What does catastrophic forgetting forget and why?\u003cbr\u003eDomain adapted machine translation: What does catastrophic forgetting forget and why?\u003cbr\u003eCMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models\u003cbr\u003eGeneration with Dynamic Vocabulary\u003cbr\u003eGeneration with Dynamic Vocabulary\u003cbr\u003e... and 22 more","\u003cb\u003eKnowledge Editing\u003c\u002fb\u003e\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eInterpretability-based Tailored Knowledge Editing in Transformers\u003cbr\u003eInterpretability-based Tailored Knowledge Editing in Transformers\u003cbr\u003eEVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation\u003cbr\u003eEVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation\u003cbr\u003e\u003eWhy Does New Knowledge Create Messy Ripple Effects in LLMs?\u003cbr\u003e\u003eWhy Does New Knowledge Create Messy Ripple Effects in LLMs?\u003cbr\u003eCommonsense Knowledge Editing Based on Free-Text in LLMs\u003cbr\u003eCommonsense Knowledge Editing Based on Free-Text in LLMs\u003cbr\u003e... and 22 more","\u003cb\u003eReinforcement Learning From Human Feedback\u003c\u002fb\u003e\u003cbr\u003eEliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence\u003cbr\u003eEliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence\u003cbr\u003eWPO: Enhancing RLHF with Weighted Preference Optimization\u003cbr\u003eOptimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning\u003cbr\u003eBPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment\u003cbr\u003eA Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors\u003cbr\u003eImproving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning\u003cbr\u003eImproving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning\u003cbr\u003eNot Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003e... and 22 more","\u003cb\u003eFaithfulness\u003c\u002fb\u003e\u003cbr\u003eEvaluating Readability and Faithfulness of Concept-based Explanations\u003cbr\u003eDancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models\u003cbr\u003eDancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models\u003cbr\u003eF2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation\u003cbr\u003eCan Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?\u003cbr\u003eAPPLS: Evaluating Evaluation Metrics for Plain Language Summarization\u003cbr\u003eSynchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation\u003cbr\u003eSTORYSUMM: Evaluating Faithfulness in Story Summarization\u003cbr\u003eAtomic Inference for NLI with Generated Facts as Atoms\u003cbr\u003eMore Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation\u003cbr\u003e... and 21 more","\u003cb\u003eHuman Evaluation\u003c\u002fb\u003e\u003cbr\u003eSuccessfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections\u003cbr\u003eIs Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering\u003cbr\u003eModular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration\u003cbr\u003eRAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval-Augmented Question Answering\u003cbr\u003eEvaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts\u003cbr\u003eEvaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts\u003cbr\u003eAligning Large Language Models with Diverse Political Viewpoints\u003cbr\u003ePARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data\u003cbr\u003eSTORYSUMM: Evaluating Faithfulness in Story Summarization\u003cbr\u003eAn image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance\u003cbr\u003e... and 21 more","\u003cb\u003eBias Mitigation\u003c\u002fb\u003e\u003cbr\u003e\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models\u003cbr\u003eStudying and Mitigating Biases in Sign Language Understanding Models\u003cbr\u003eRSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework\u003cbr\u003eStrategic Demonstration Selection for Improved Fairness in LLM In-Context Learning\u003cbr\u003eThe Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention\u003cbr\u003eLLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives\u003cbr\u003eSplit and Merge: Aligning Position Biases in LLM-based Evaluators\u003cbr\u003eGrasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction\u003cbr\u003eCluster-Norm for Unsupervised Probing of Knowledge\u003cbr\u003eBiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs\u003cbr\u003e... and 20 more","\u003cb\u003eBias Detection\u003c\u002fb\u003e\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003eSTOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003eChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context\u003cbr\u003eLocating Information Gaps and Narrative Inconsistencies Across Languages: A Case Study of LGBT People Portrayals on Wikipedia\u003cbr\u003eTowards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights\u003cbr\u003eVoices in a Crowd: Searching for Clusters of Unique Perspectives\u003cbr\u003e\"Global is Good, Local is Bad?\u201d: Understanding Brand Bias in LLMs\u003cbr\u003eBiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs\u003cbr\u003e... and 20 more","\u003cb\u003eReading Comprehension\u003c\u002fb\u003e\u003cbr\u003eMitigating the Alignment Tax of RLHF\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003eFine-Grained Prediction of Reading Comprehension from Eye Movements\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003eLawBench: Benchmarking Legal Knowledge of Large Language Models\u003cbr\u003eMore Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation\u003cbr\u003eMore Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation\u003cbr\u003eFAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition\u003cbr\u003eNull-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination\u003cbr\u003e... and 20 more","\u003cb\u003eNatural Language Understanding\u003c\u002fb\u003e\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eAdvancing Event Causality Identification via Heuristic Semantic Dependency Inquiry Network\u003cbr\u003eOn Training Data Influence of GPT Models\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003eRevealing the Parallel Multilingual Learning within Large Language Models\u003cbr\u003eFROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models\u003cbr\u003eFROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models\u003cbr\u003ePerceptions of Linguistic Uncertainty by Language Models and Humans\u003cbr\u003ePragmatic Norms Are All You Need \u2013 Why The Symbol Grounding Problem Does Not Apply to LLMs\u003cbr\u003e... and 20 more","\u003cb\u003eLLM Evaluation\u003c\u002fb\u003e\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eA User-Centric Multi-Intent Benchmark for Evaluating Large Language Models\u003cbr\u003eEvaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts\u003cbr\u003eChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context\u003cbr\u003eMAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration\u003cbr\u003ePARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data\u003cbr\u003eSplit and Merge: Aligning Position Biases in LLM-based Evaluators\u003cbr\u003eA Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations\u003cbr\u003eTowards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey\u003cbr\u003eFinding Blind Spots in Evaluator LLMs with Interpretable Checklists\u003cbr\u003e... and 20 more","\u003cb\u003ePreference Optimization\u003c\u002fb\u003e\u003cbr\u003eControllable Preference Optimization: Toward Controllable Multi-Objective Alignment\u003cbr\u003eWord Alignment as Preference for Machine Translation\u003cbr\u003eAdvancing Large Language Model Attribution through Self-Improving\u003cbr\u003eAlignCap: Aligning Speech Emotion Captioning to Human Preferences\u003cbr\u003eEPO: Hierarchical LLM Agents with Environment Preference Optimization\u003cbr\u003eAligning Large Language Models with Diverse Political Viewpoints\u003cbr\u003eAligning Large Language Models with Diverse Political Viewpoints\u003cbr\u003eMDPO: Conditional Preference Optimization for Multimodal Large Language Models\u003cbr\u003eWPO: Enhancing RLHF with Weighted Preference Optimization\u003cbr\u003eBPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment\u003cbr\u003e... and 20 more","\u003cb\u003eModel Compression\u003c\u002fb\u003e\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003eMTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval\u003cbr\u003eStructured Optimal Brain Pruning for Large Language Models\u003cbr\u003eStructured Optimal Brain Pruning for Large Language Models\u003cbr\u003eLayer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models\u003cbr\u003eMentor-KD: Making Small Language Models Better Multi-step Reasoners\u003cbr\u003ePruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging\u003cbr\u003ePruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging\u003cbr\u003eIs C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning\u003cbr\u003eDual-Space Knowledge Distillation for Large Language Models\u003cbr\u003e... and 20 more","\u003cb\u003eLogical Reasoning\u003c\u002fb\u003e\u003cbr\u003eLearning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing\u003cbr\u003eLearning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing\u003cbr\u003eLogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models\u003cbr\u003eLogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models\u003cbr\u003eConditional and Modal Reasoning in Large Language Models\u003cbr\u003eScaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars\u003cbr\u003eScaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars\u003cbr\u003eRetrieved In-Context Principles from Previous Mistakes\u003cbr\u003eMETAREFLECTION: Learning Instructions for Language Agents using Past Reflections\u003cbr\u003ePuzzle Solving using Reasoning of Large Language Models: A Survey\u003cbr\u003e... and 18 more","\u003cb\u003eSpeech Recognition\u003c\u002fb\u003e\u003cbr\u003eScaling Properties of Speech Language Models\u003cbr\u003eVoices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects\u003cbr\u003eEH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning\u003cbr\u003eMulti-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges\u003cbr\u003eMulti-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges\u003cbr\u003eMuting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models\u003cbr\u003eMOSEL: Inference Serving Using Dynamic Modality Selection\u003cbr\u003eTask Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition\u003cbr\u003eWhat is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations\u003cbr\u003eSelf-Powered LLM Modality Expansion for Large Speech-Text Models\u003cbr\u003e... and 18 more","\u003cb\u003eTransformers\u003c\u002fb\u003e\u003cbr\u003eDocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing\u003cbr\u003eModel Balancing Helps Low-data Training and Fine-tuning\u003cbr\u003eAutoregressive Pre-Training on Pixels and Texts\u003cbr\u003eHow do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning\u003cbr\u003eInterpretability-based Tailored Knowledge Editing in Transformers\u003cbr\u003eSemantic Training Signals Promote Hierarchical Syntactic Generalization in Transformers\u003cbr\u003eInvestigating Mysteries of CoT-Augmented Distillation\u003cbr\u003eMOSEL: Inference Serving Using Dynamic Modality Selection\u003cbr\u003eModeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding\u003cbr\u003eBirdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives\u003cbr\u003e... and 18 more","\u003cb\u003eMultimodal Llms\u003c\u002fb\u003e\u003cbr\u003eEFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models\u003cbr\u003eMAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering\u003cbr\u003eTinyChart: Efficient Chart Understanding with Program-of-Thoughts Learning and Visual Token Merging\u003cbr\u003eTag-grounded Visual Instruction Tuning with Retrieval Augmentation\u003cbr\u003eBy My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting\u003cbr\u003eM\u00b2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning\u003cbr\u003eHow Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?\u003cbr\u003eTo Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models\u003cbr\u003eMMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model\u003cbr\u003eTowards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale\u003cbr\u003e... and 18 more","\u003cb\u003eHallucination Mitigation\u003c\u002fb\u003e\u003cbr\u003eEFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models\u003cbr\u003eLookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps\u003cbr\u003eHELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding\u003cbr\u003eKnowledge Verification to Nip Hallucination in the Bud\u003cbr\u003eWhispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models\u003cbr\u003eWord Alignment as Preference for Machine Translation\u003cbr\u003eLearn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism\u003cbr\u003ePelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification\u003cbr\u003eAtomic Self-Consistency for Better Long Form Generations\u003cbr\u003eAnalysis of Plan-based Retrieval for Grounded Text Generation\u003cbr\u003e... and 18 more","\u003cb\u003eQuantization\u003c\u002fb\u003e\u003cbr\u003ePrefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization\u003cbr\u003eQUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models\u003cbr\u003eQUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models\u003cbr\u003eVPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models\u003cbr\u003eApiQ: Finetuning of 2-Bit Quantized Large Language Model\u003cbr\u003exCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics\u003cbr\u003exCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics\u003cbr\u003eQUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware\u003cbr\u003eKV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches\u003cbr\u003eRoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization\u003cbr\u003e... and 18 more","\u003cb\u003eModel Editing\u003c\u002fb\u003e\u003cbr\u003eLEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models\u003cbr\u003eInterpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003eLifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning\u003cbr\u003eConsecutive Batch Model Editing with HooK Layers\u003cbr\u003eConsecutive Batch Model Editing with HooK Layers\u003cbr\u003eHopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries\u003cbr\u003eMitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing\u003cbr\u003eFAME: Towards Factual Multi-Task Model Editing\u003cbr\u003eOn the Robustness of Editing Large Language Models\u003cbr\u003e... and 18 more","\u003cb\u003eText Summarization\u003c\u002fb\u003e\u003cbr\u003eAligning Large Language Models with Diverse Political Viewpoints\u003cbr\u003eEnhancing Reinforcement Learning with Dense Rewards from Language Model Critic\u003cbr\u003eDetecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors\u003cbr\u003eDetecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors\u003cbr\u003eOuroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding\u003cbr\u003eLeveraging Large Language Models for NLG Evaluation: Advances and Challenges\u003cbr\u003eCan We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?\u003cbr\u003eCan We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?\u003cbr\u003eFFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping\u003cbr\u003eSemformer: Transformer Language Models with Semantic Planning\u003cbr\u003e... and 18 more","\u003cb\u003eHate Speech Detection\u003c\u002fb\u003e\u003cbr\u003eHateful Word in Context Classification\u003cbr\u003eHateful Word in Context Classification\u003cbr\u003eEyes Don't Lie: Subjective Hate Annotation and Detection with Gaze\u003cbr\u003eEyes Don't Lie: Subjective Hate Annotation and Detection with Gaze\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eBridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning\u003cbr\u003eBridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning\u003cbr\u003eThe Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification\u003cbr\u003eUnlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization\u003cbr\u003e... and 17 more","\u003cb\u003eDiversity\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eEffective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process\u003cbr\u003eExploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors\u003cbr\u003eAn Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification\u003cbr\u003eAn Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making\u003cbr\u003eTeaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eHow Far Can We Extract Diverse Perspectives from Large Language Models?\u003cbr\u003eMIRRORSTORIES: Reflecting Diversity through Personalized Narrative Generation with Large Language Models\u003cbr\u003eAutomatic Instruction Evolving for Large Language Models\u003cbr\u003e... and 16 more","\u003cb\u003eConsistency\u003c\u002fb\u003e\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eAn Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification\u003cbr\u003eDecompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison\u003cbr\u003eFormality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge\u003cbr\u003eConsistent Bidirectional Language Modelling: Expressive Power and Representational Conciseness\u003cbr\u003eOn the Reliability of Psychological Scales on Large Language Models\u003cbr\u003eSplit and Merge: Aligning Position Biases in LLM-based Evaluators\u003cbr\u003eQGEval: Benchmarking Multi-dimensional Evaluation for Question Generation\u003cbr\u003eSubword Segmentation in LLMs: Looking at Inflection and Consistency\u003cbr\u003eTemporally Consistent Factuality Probing for Large Language Models\u003cbr\u003e... and 16 more","\u003cb\u003eArithmetic Reasoning\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eEmpowering Multi-step Reasoning across Languages via Program-Aided Language Models\u003cbr\u003eTowards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models\u003cbr\u003eLarge Language Models Can Self-Correct with Key Condition Verification\u003cbr\u003eNull-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination\u003cbr\u003e... and 15 more","\u003cb\u003eFactuality\u003c\u002fb\u003e\u003cbr\u003eRULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models\u003cbr\u003eThe Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models\u003cbr\u003eThe Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models\u003cbr\u003eEstimating Knowledge in Large Language Models Without Generating a Single Token\u003cbr\u003eAn Analysis of Multilingual FActScore\u003cbr\u003eF2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation\u003cbr\u003eLUQ: Long-text Uncertainty Quantification for LLMs\u003cbr\u003eExplaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM\u003cbr\u003eEnhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding\u003cbr\u003eAnalysis of Plan-based Retrieval for Grounded Text Generation\u003cbr\u003e... and 15 more","\u003cb\u003eVisual Reasoning\u003c\u002fb\u003e\u003cbr\u003eEnhancing Advanced Visual Reasoning Ability of Large Language Models\u003cbr\u003eFrom the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis\u003cbr\u003eFiner: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models\u003cbr\u003eBeyond Embeddings: The Promise of Visual Table in Visual Reasoning\u003cbr\u003ePelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification\u003cbr\u003eOmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer\u003cbr\u003eFineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension\u003cbr\u003eThe Instinctive Bias: Spurious Images lead to Illusion in MLLMs\u003cbr\u003eMultimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model\u003cbr\u003eMultimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model\u003cbr\u003e... and 15 more","\u003cb\u003eBenchmarkss\u003c\u002fb\u003e\u003cbr\u003eQuality Matters: Evaluating Synthetic Data for Tool-Using LLMs\u003cbr\u003eAcademics Can Contribute to Domain-Specialized Language Models\u003cbr\u003eTo Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models\u003cbr\u003eDo Text-to-Vis Benchmarks Test Real Use of Visualisations?\u003cbr\u003eModeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding\u003cbr\u003ePuzzle Solving using Reasoning of Large Language Models: A Survey\u003cbr\u003eFAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition\u003cbr\u003eA Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations\u003cbr\u003eDistractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation\u003cbr\u003eBuilding Resources for Emakhuwa: Machine Translation and News Classification Benchmarks\u003cbr\u003e... and 15 more","\u003cb\u003eSocial Media\u003c\u002fb\u003e\u003cbr\u003e\u201cWe Demand Justice!\u201d: Towards Social Context Grounding of Political Texts\u003cbr\u003eF2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation\u003cbr\u003eLeveraging Conflicts in Social Media Posts: Unintended Offense Dataset\u003cbr\u003eThe Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse\u003cbr\u003eBridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning\u003cbr\u003eMisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"\u003cbr\u003eWords Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models\u003cbr\u003eSPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness\u003cbr\u003eA Closer Look at Multidimensional Online Political Incivility\u003cbr\u003eDecoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach\u003cbr\u003e... and 14 more","\u003cb\u003eLora\u003c\u002fb\u003e\u003cbr\u003eDA\u00b3: A Distribution-Aware Adversarial Attack against Language Models\u003cbr\u003eSelf-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering\u003cbr\u003eReusing Transferable Weight Increments for Low-resource Style Generation\u003cbr\u003eInterpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis\u003cbr\u003eFrom Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning\u003cbr\u003ePSC: Extending Context Window of Large Language Models via Phase Shift Calibration\u003cbr\u003eMore Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs\u003cbr\u003eSegment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation\u003cbr\u003eLLOCO: Learning Long Contexts Offline\u003cbr\u003eGPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning\u003cbr\u003e... and 14 more","\u003cb\u003eLLM Alignment\u003c\u002fb\u003e\u003cbr\u003eTake Off the Training Wheels! Progressive In-Context Learning for Effective Alignment\u003cbr\u003eBPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment\u003cbr\u003eGlobal Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents\u003cbr\u003eDon't Forget Your Reward Values: Language Model Alignment via Value-based Calibration\u003cbr\u003eContrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion\u003cbr\u003eNegating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization\u003cbr\u003eNegating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization\u003cbr\u003eSELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards\u003cbr\u003eReward Difference Optimization For Sample Reweighting In Offline RLHF\u003cbr\u003eReward Difference Optimization For Sample Reweighting In Offline RLHF\u003cbr\u003e... and 14 more","\u003cb\u003eContinual Learning\u003c\u002fb\u003e\u003cbr\u003eSEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models\u003cbr\u003eSEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models\u003cbr\u003eEmpowering Large Language Model for Continual Video Question Answering with Collaborative Prompting\u003cbr\u003eEmpowering Large Language Model for Continual Video Question Answering with Collaborative Prompting\u003cbr\u003eTL-CL: Task And Language Incremental Continual Learning\u003cbr\u003eLifelong Event Detection via Optimal Transport\u003cbr\u003eCOGEN: Learning from Feedback with Coupled Comprehension and Generation\u003cbr\u003eA Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models\u003cbr\u003ePreserving Generalization of Language Models in Few-shot Continual Relation Extraction\u003cbr\u003eExplicit Memory Learning with Expectation Maximization\u003cbr\u003e... and 14 more","\u003cb\u003eBert\u003c\u002fb\u003e\u003cbr\u003ePixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models\u003cbr\u003eNoise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature\u003cbr\u003eThe effects of distance on NPI illusive effects in BERT\u003cbr\u003eWhen Generative Adversarial Networks Meet Sequence Labeling Challenges\u003cbr\u003eRevisiting Supertagging for Faster HPSG Parsing\u003cbr\u003eKAR\u00b3L: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students\u003cbr\u003eLeveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training\u003cbr\u003eSparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers\u003cbr\u003eNeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries\u003cbr\u003eComparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media\u003cbr\u003e... and 14 more","\u003cb\u003eGsm8k\u003c\u002fb\u003e\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eMODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning\u003cbr\u003eFrom Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning\u003cbr\u003eAdaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning\u003cbr\u003eADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning\u003cbr\u003eReasoning Robustness of LLMs to Adversarial Typographical Errors\u003cbr\u003eExploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems\u003cbr\u003eLLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks\u003cbr\u003e... and 13 more","\u003cb\u003eDeep Learning\u003c\u002fb\u003e\u003cbr\u003eIntegrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification\u003cbr\u003eA Survey on In-context Learning\u003cbr\u003eSeeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients\u003cbr\u003eUnifying Multimodal Retrieval via Document Screenshot Embedding\u003cbr\u003eThe Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples\u003cbr\u003eMEANT: Multimodal Encoder for Antecedent Information\u003cbr\u003eTask Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition\u003cbr\u003eTowards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs\u003cbr\u003eApplying Contrastive Learning to Code Vulnerability Type Classification\u003cbr\u003eLatent Concept-based Explanation of NLP Models\u003cbr\u003e... and 12 more","\u003cb\u003ePreference Learning\u003c\u002fb\u003e\u003cbr\u003eTowards Tool Use Alignment of Large Language Models\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003eUnsupervised Human Preference Learning\u003cbr\u003eVLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment\u003cbr\u003eEPO: Hierarchical LLM Agents with Environment Preference Optimization\u003cbr\u003eSpeechworthy Instruction-tuned Language Models\u003cbr\u003eWhat are the Generator Preferences for End-to-end Task-Oriented Dialog System?\u003cbr\u003eModeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation\u003cbr\u003eDon't Forget Your Reward Values: Language Model Alignment via Value-based Calibration\u003cbr\u003e... and 12 more","\u003cb\u003eSafety\u003c\u002fb\u003e\u003cbr\u003eCMD: a framework for Context-aware Model self-Detoxification\u003cbr\u003eCMD: a framework for Context-aware Model self-Detoxification\u003cbr\u003eVIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values\u003cbr\u003eKidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions\u003cbr\u003eVLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment\u003cbr\u003eRevisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective\u003cbr\u003eThe Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm\u003cbr\u003eHouseholder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective\u003cbr\u003eCoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference\u003cbr\u003ePlease note that I'm just an AI: Analysis of Behavior Patterns of LLMs in (Non-)offensive Speech Identification\u003cbr\u003e... and 12 more","\u003cb\u003eError Correction\u003c\u002fb\u003e\u003cbr\u003eVerification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving\u003cbr\u003eVerification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving\u003cbr\u003eC-LLM: Learn to Check Chinese Spelling Errors Character by Character\u003cbr\u003eRetrieved In-Context Principles from Previous Mistakes\u003cbr\u003eARM: An Alignment-and-Replacement Module for Chinese Spelling Check Based on LLMs\u003cbr\u003eRepairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models\u003cbr\u003eLearn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning\u003cbr\u003eRe-ReST: Reflection-Reinforced Self-Training for Language Agents\u003cbr\u003eEHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records\u003cbr\u003eCOFFEE-GYM: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code\u003cbr\u003e... and 12 more","\u003cb\u003eSupervised Fine-tuning\u003c\u002fb\u003e\u003cbr\u003eARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback\u003cbr\u003eSaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales\u003cbr\u003eLIONS: An Empirically Optimized Approach to Align Language Models\u003cbr\u003eLIONS: An Empirically Optimized Approach to Align Language Models\u003cbr\u003eKNN-INSTRUCT: Automatic Instruction Construction with K Nearest Neighbor Deduction\u003cbr\u003eORPO: Monolithic Preference Optimization without Reference Model\u003cbr\u003eBeyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models\u003cbr\u003eHow to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective\u003cbr\u003eA Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences\u003cbr\u003eA Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences\u003cbr\u003e... and 12 more","\u003cb\u003eError Analysis\u003c\u002fb\u003e\u003cbr\u003eLiar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models\u003cbr\u003eASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?\u003cbr\u003eRevisiting Automated Evaluation for Long-form Table Question Answering\u003cbr\u003eNOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition\u003cbr\u003eA linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks\u003cbr\u003eDISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers\u003cbr\u003eDISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers\u003cbr\u003eError Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation\u003cbr\u003eError Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation\u003cbr\u003eStep-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?\u003cbr\u003e... and 12 more","\u003cb\u003eKnowledge Graphs\u003c\u002fb\u003e\u003cbr\u003eXplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs\u003cbr\u003eStructure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text\u003cbr\u003eBio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints\u003cbr\u003eEvidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering\u003cbr\u003eDKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction\u003cbr\u003e1+1\u003e2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?\u003cbr\u003eContext-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs\u003cbr\u003eMP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs\u003cbr\u003eGenerate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering\u003cbr\u003eKnowledge Graph Enhanced Large Language Model Editing\u003cbr\u003e... and 12 more","\u003cb\u003eTokenization\u003c\u002fb\u003e\u003cbr\u003eNumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning\u003cbr\u003eTokenization Is More Than Compression\u003cbr\u003eCUTE: Measuring LLMs' Understanding of Their Tokens\u003cbr\u003eWhere is the signal in tokenization space?\u003cbr\u003eLexically Grounded Subword Segmentation\u003cbr\u003eLexically Grounded Subword Segmentation\u003cbr\u003eToken Erasure as a Footprint of Implicit Vocabulary Items in LLMs\u003cbr\u003eToken Erasure as a Footprint of Implicit Vocabulary Items in LLMs\u003cbr\u003eDistributional Properties of Subword Regularization\u003cbr\u003eFishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models\u003cbr\u003e... and 11 more","\u003cb\u003eKnowledge Base\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eDyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities\u003cbr\u003eTriad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering\u003cbr\u003eKB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases\u003cbr\u003eDo Large Language Models Know How Much They Know?\u003cbr\u003eDo You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation\u003cbr\u003eContrastive Entity Coreference and Disambiguation for Historical Texts\u003cbr\u003eWhat are the Generator Preferences for End-to-end Task-Oriented Dialog System?\u003cbr\u003eOneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting\u003cbr\u003eDynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG\u003cbr\u003e... and 11 more","\u003cb\u003eMultimodal\u003c\u002fb\u003e\u003cbr\u003eRULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models\u003cbr\u003eUniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eQuantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval\u003cbr\u003eBridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning\u003cbr\u003eSciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading\u003cbr\u003eECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos\u003cbr\u003eYesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models\u003cbr\u003eMemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification\u003cbr\u003eIn-Context Compositional Generalization for Large Vision-Language Models\u003cbr\u003e... and 11 more","\u003cb\u003eGraph Neural Networks\u003c\u002fb\u003e\u003cbr\u003eIntegrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training\u003cbr\u003eMessage Passing on Semantic-Anchor-Graphs for Fine-grained Emotion Representation Learning and Classification\u003cbr\u003eDGLF: A Dual Graph-based Learning Framework for Multi-modal Sarcasm Detection\u003cbr\u003eEnhancing High-order Interaction Awareness in LLM-based Recommender Model\u003cbr\u003eBridging Local Details and Global Context in Text-Attributed Graphs\u003cbr\u003eCan Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction\u003cbr\u003eTowards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering\u003cbr\u003eKnowledge Graph Enhanced Large Language Model Editing\u003cbr\u003eTransferability of Syntax-Aware Graph Neural Networks in Zero-Shot Cross-Lingual Semantic Role Labeling\u003cbr\u003eInsertGNN: A Hierarchical Graph Neural Network for the TOEFL Sentence Insertion Problem\u003cbr\u003e... and 11 more","\u003cb\u003eDialogue Generation\u003c\u002fb\u003e\u003cbr\u003eSynergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems\u003cbr\u003eOntologically Faithful Generation of Non-Player Character Dialogues\u003cbr\u003eOntologically Faithful Generation of Non-Player Character Dialogues\u003cbr\u003eOptimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eBeyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models\u003cbr\u003eTransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities\u003cbr\u003eHow to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective\u003cbr\u003eLeveraging Large Language Models for NLG Evaluation: Advances and Challenges\u003cbr\u003eGDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets\u003cbr\u003e... and 11 more","\u003cb\u003eActive Learning\u003c\u002fb\u003e\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003eEffective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003ePerformance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale\u003cbr\u003eAn L* Algorithm for Deterministic Weighted Regular Languages\u003cbr\u003eSelf-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models\u003cbr\u003eSelf-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models\u003cbr\u003eAnnotator-Centric Active Learning for Subjective NLP Tasks\u003cbr\u003eAnnotator-Centric Active Learning for Subjective NLP Tasks\u003cbr\u003eDISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers\u003cbr\u003e... and 10 more","\u003cb\u003eSelf-supervised Learning\u003c\u002fb\u003e\u003cbr\u003eEmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models\u003cbr\u003eMitigating Matthew Effect: Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation\u003cbr\u003eImproving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach\u003cbr\u003eEH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning\u003cbr\u003eCSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free-Word-Ordered and Morphologically-Rich Low-Resource Languages\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eMolTRES: Improving Chemical Language Representation Learning for Molecular Property Prediction\u003cbr\u003eAudioVSR: Enhancing Video Speech Recognition with Audio Data\u003cbr\u003eUnleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training\u003cbr\u003eNCPrompt: NSP-Based Prompt Learning and Contrastive Learning for Implicit Discourse Relation Recognition\u003cbr\u003e... and 10 more","\u003cb\u003eTopic Modeling\u003c\u002fb\u003e\u003cbr\u003eOn Fake News Detection with LLM Enhanced Semantics Mining\u003cbr\u003ePhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study\u003cbr\u003eNoise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature\u003cbr\u003eNoise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature\u003cbr\u003eAutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments\u003cbr\u003eLLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement\u003cbr\u003eLLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement\u003cbr\u003eThe LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?\u003cbr\u003eReap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora\u003cbr\u003eUnsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance\u003cbr\u003e... and 10 more","\u003cb\u003eSynthetic Data Generation\u003c\u002fb\u003e\u003cbr\u003eSuccessfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections\u003cbr\u003eZero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection\u003cbr\u003eGOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory\u003cbr\u003eA Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners\u003cbr\u003eKnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server\u003cbr\u003eLLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives\u003cbr\u003eLLM-based Code-Switched Text Generation for Grammatical Error Correction\u003cbr\u003eLink, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval\u003cbr\u003eSynthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models\u003cbr\u003eSYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists\u003cbr\u003e... and 10 more","\u003cb\u003eOpen-domain QA\u003c\u002fb\u003e\u003cbr\u003eBlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering\u003cbr\u003eBlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering\u003cbr\u003eREAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering\u003cbr\u003eFrom RAG to RICHES: Retrieval Interlaced with Sequence Generation\u003cbr\u003eLarge Language Models Can Self-Correct with Key Condition Verification\u003cbr\u003eWhere am I? Large Language Models Wandering between Semantics and Structures in Long Contexts\u003cbr\u003eWhere am I? Large Language Models Wandering between Semantics and Structures in Long Contexts\u003cbr\u003eModel Editing Harms General Abilities of Large Language Models: Regularization to the Rescue\u003cbr\u003eImproving Zero-shot LLM Re-Ranker with Risk Minimization\u003cbr\u003eRE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation\u003cbr\u003e... and 10 more","\u003cb\u003eCatastrophic Forgetting\u003c\u002fb\u003e\u003cbr\u003ePre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation\u003cbr\u003eLEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models\u003cbr\u003eSEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models\u003cbr\u003eEmpowering Large Language Model for Continual Video Question Answering with Collaborative Prompting\u003cbr\u003eGold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs\u003cbr\u003eMore Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs\u003cbr\u003eBreaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale\u003cbr\u003eBreaking the Curse of Multilinguality with Cross-lingual Expert Language Models\u003cbr\u003eLifelong Event Detection via Optimal Transport\u003cbr\u003eDomain adapted machine translation: What does catastrophic forgetting forget and why?\u003cbr\u003e... and 10 more","\u003cb\u003eRag\u003c\u002fb\u003e\u003cbr\u003eGlue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models\u003cbr\u003eRetrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation\u003cbr\u003eInvestigating LLMs as Voting Assistants via Contextual Augmentation: A Case Study on the European Parliament Elections 2024\u003cbr\u003eDo You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation\u003cbr\u003eDo You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation\u003cbr\u003eSATYRN: A Platform for Analytics Augmented Generation\u003cbr\u003eRanking Manipulation for Conversational Search Engines\u003cbr\u003eImprove Dense Passage Retrieval with Entailment Tuning\u003cbr\u003eFINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents\u003cbr\u003eGPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning\u003cbr\u003e... and 10 more","\u003cb\u003eAdversarial Attack\u003c\u002fb\u003e\u003cbr\u003eGlue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models\u003cbr\u003eDA\u00b3: A Distribution-Aware Adversarial Attack against Language Models\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003eAdaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks\u003cbr\u003eAdaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks\u003cbr\u003eIs LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment\u003cbr\u003eMuting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models\u003cbr\u003eRanking Manipulation for Conversational Search Engines\u003cbr\u003eReasoning Robustness of LLMs to Adversarial Typographical Errors\u003cbr\u003eLarge Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks\u003cbr\u003e... and 10 more","\u003cb\u003eVideo Understanding\u003c\u002fb\u003e\u003cbr\u003eMatchTime: Towards Automatic Soccer Game Commentary Generation\u003cbr\u003eVideo-LLaVA: Learning United Visual Representation by Alignment Before Projection\u003cbr\u003eEncoding and Controlling Global Semantics for Long-form Video Question Answering\u003cbr\u003eTraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering\u003cbr\u003eEfficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge\u003cbr\u003eOmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer\u003cbr\u003eOmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer\u003cbr\u003eECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos\u003cbr\u003eVideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models\u003cbr\u003eGRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization\u003cbr\u003e... and 10 more","\u003cb\u003ePrompt Optimization\u003c\u002fb\u003e\u003cbr\u003ePrompts have evil twins\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003ePRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling\u003cbr\u003eMETAREFLECTION: Learning Instructions for Language Agents using Past Reflections\u003cbr\u003eOptimizing Instructions and Demonstrations for Multi-Stage Language Model Programs\u003cbr\u003eOptimizing Instructions and Demonstrations for Multi-Stage Language Model Programs\u003cbr\u003eMedical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?\u003cbr\u003eFewer is More: Boosting Math Reasoning with Reinforced Context Pruning\u003cbr\u003e... and 9 more","\u003cb\u003eAdversarial Attacks\u003c\u002fb\u003e\u003cbr\u003ePrompts have evil twins\u003cbr\u003eEvaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection\u003cbr\u003eThinking Outside of the Differential Privacy Box: A Case Study in Text Privatization with Language Model Prompting\u003cbr\u003eReconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with GPT Models\u003cbr\u003eTowards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis\u003cbr\u003eAdvancing Adversarial Suffix Transfer Learning on Aligned Large Language Models\u003cbr\u003eIM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method\u003cbr\u003eFrom LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking\u003cbr\u003eBe a Multitude to Itself: A Prompt Evolution Framework for Red Teaming\u003cbr\u003eBe a Multitude to Itself: A Prompt Evolution Framework for Red Teaming\u003cbr\u003e... and 9 more","\u003cb\u003eRlhf\u003c\u002fb\u003e\u003cbr\u003eMitigating the Alignment Tax of RLHF\u003cbr\u003eCalibrating the Confidence of Large Language Models by Eliciting Fidelity\u003cbr\u003eThe Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models\u003cbr\u003eWPO: Enhancing RLHF with Weighted Preference Optimization\u003cbr\u003eOptimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning\u003cbr\u003eRLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs\u003cbr\u003eA Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003eGDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets\u003cbr\u003eFlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization\u003cbr\u003e... and 9 more","\u003cb\u003eAnnotation\u003c\u002fb\u003e\u003cbr\u003eWhat's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs\u003cbr\u003eNoise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature\u003cbr\u003eThe Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse\u003cbr\u003eLeveraging pre-trained language models for linguistic analysis: A case of argument structure constructions\u003cbr\u003eThe Empirical Variability of Narrative Perceptions of Social Media Texts\u003cbr\u003eFlee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling\u003cbr\u003eLet's discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment\u003cbr\u003eAre LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues\u003cbr\u003eCERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays\u003cbr\u003eCEAMC: Corpus and Empirical Study of Argument Analysis in Education via LLMs\u003cbr\u003e... and 9 more","\u003cb\u003eGeneration\u003c\u002fb\u003e\u003cbr\u003eUniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation\u003cbr\u003eFuseGen: PLM Fusion for Data-generation based Zero-shot Learning\u003cbr\u003eVGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation\u003cbr\u003eSafely Learning with Private Data: A Federated Learning Framework for Large Language Model\u003cbr\u003eUnveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism\u003cbr\u003eCOGEN: Learning from Feedback with Coupled Comprehension and Generation\u003cbr\u003eCOGEN: Learning from Feedback with Coupled Comprehension and Generation\u003cbr\u003eHiFT: A Hierarchical Full Parameter Fine-Tuning Strategy\u003cbr\u003eWho is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models\u003cbr\u003eFuse to Forget: Bias Reduction and Selective Memorization through Model Fusion\u003cbr\u003e... and 9 more","\u003cb\u003ePrivacy\u003c\u002fb\u003e\u003cbr\u003eAn Inversion Attack Against Obfuscated Embedding Matrix in Language Model Inference\u003cbr\u003eOrder of Magnitude Speedups for LLM Membership Inference\u003cbr\u003eSeeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients\u003cbr\u003eSeeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients\u003cbr\u003eSafely Learning with Private Data: A Federated Learning Framework for Large Language Model\u003cbr\u003ePERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts\u003cbr\u003eRECALL: Membership Inference via Relative Conditional Log-Likelihoods\u003cbr\u003eRevisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective\u003cbr\u003eGranular Privacy Control for Geolocation with Vision Language Models\u003cbr\u003eFuse to Forget: Bias Reduction and Selective Memorization through Model Fusion\u003cbr\u003e... and 9 more","\u003cb\u003eUnsupervised Learning\u003c\u002fb\u003e\u003cbr\u003eUnsupervised Human Preference Learning\u003cbr\u003eAn Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records\u003cbr\u003eUnsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of the Noisy Channel\u003cbr\u003eMatryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions\u003cbr\u003eUnlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization\u003cbr\u003eREADME++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment\u003cbr\u003eVoices in a Crowd: Searching for Clusters of Unique Perspectives\u003cbr\u003eCluster-Norm for Unsupervised Probing of Knowledge\u003cbr\u003eUnsupervised Named Entity Disambiguation for Low Resource Domains\u003cbr\u003eCOMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities\u003cbr\u003e... and 9 more","\u003cb\u003eSafety Alignment\u003c\u002fb\u003e\u003cbr\u003eAdvancing Adversarial Suffix Transfer Learning on Aligned Large Language Models\u003cbr\u003eDATA ADVISOR: Dynamic Data Curation for Safety Alignment of Large Language Models\u003cbr\u003eDATA ADVISOR: Dynamic Data Curation for Safety Alignment of Large Language Models\u003cbr\u003eLoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models\u003cbr\u003eLarge Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks\u003cbr\u003eHolistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction\u003cbr\u003eHouseholder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective\u003cbr\u003eMLLM-Protector: Ensuring MLLM's Safety without Hurting Performance\u003cbr\u003eFrom LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking\u003cbr\u003eSafety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations\u003cbr\u003e... and 9 more","\u003cb\u003eLanguage Model Alignment\u003c\u002fb\u003e\u003cbr\u003eLIONS: An Empirically Optimized Approach to Align Language Models\u003cbr\u003eLIONS: An Empirically Optimized Approach to Align Language Models\u003cbr\u003eReverse-Engineering the Reader\u003cbr\u003eImproving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning\u003cbr\u003eImproving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003eCOMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities\u003cbr\u003eGDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets\u003cbr\u003eRethinking the Role of Proxy Rewards in Language Model Alignment\u003cbr\u003eRethinking the Role of Proxy Rewards in Language Model Alignment\u003cbr\u003e... and 9 more","\u003cb\u003eLanguage Understanding\u003c\u002fb\u003e\u003cbr\u003eNumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning\u003cbr\u003eAutoregressive Pre-Training on Pixels and Texts\u003cbr\u003eUnraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications\u003cbr\u003eLarge Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark\u003cbr\u003eLLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks\u003cbr\u003eConnecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game\u003cbr\u003eMalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language\u003cbr\u003eMalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language\u003cbr\u003eAC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models\u003cbr\u003eThe Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance\u003cbr\u003e... and 8 more","\u003cb\u003eKnowledge Graph\u003c\u002fb\u003e\u003cbr\u003eA Usage-centric Take on Intent Understanding in E-Commerce\u003cbr\u003eEfficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards\u003cbr\u003eUnsupervised Named Entity Disambiguation for Low Resource Domains\u003cbr\u003eTKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs\u003cbr\u003eStorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for Children's Story-Based Learning\u003cbr\u003eTowards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering\u003cbr\u003eGenerative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation\u003cbr\u003eDALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature\u003cbr\u003eQRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism\u003cbr\u003eTRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation\u003cbr\u003e... and 8 more","\u003cb\u003eRanking\u003c\u002fb\u003e\u003cbr\u003eConsolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing\u003cbr\u003eConsolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing\u003cbr\u003eClustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation\u003cbr\u003eHow Does the Disclosure of AI Assistance Affect the Perceptions of Writing?\u003cbr\u003eEfficient LLM Comparative Assessment: A Product of Experts Framework for Pairwise Comparisons\u003cbr\u003eAGRAME: Any-Granularity Ranking with Multi-Vector Embeddings\u003cbr\u003eTROTR: A Framework for Evaluating the Recontextualization of Text\u003cbr\u003eVideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models\u003cbr\u003eEntity Insertion in Multilingual Linked Corpora: The Case of Wikipedia\u003cbr\u003eMake Large Language Model a Better Ranker\u003cbr\u003e... and 8 more","\u003cb\u003eEmotion Recognition\u003c\u002fb\u003e\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eVisual Prompting in LLMs for Enhancing Emotion Recognition\u003cbr\u003eVisual Prompting in LLMs for Enhancing Emotion Recognition\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eSelf-Powered LLM Modality Expansion for Large Speech-Text Models\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003eWorry Words: Norms of Anxiety Association for over 44k English Words\u003cbr\u003ePALM: Few-Shot Prompt Learning for Audio Language Models\u003cbr\u003eEmotion Granularity from Text: An Aggregate-Level Indicator of Mental Health\u003cbr\u003e... and 8 more","\u003cb\u003eTranslation\u003c\u002fb\u003e\u003cbr\u003eMitigating the Alignment Tax of RLHF\u003cbr\u003eUsing Language Models to Disambiguate Lexical Choices in Translation\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003eQuantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval\u003cbr\u003eA Thorough Examination of Decoding Methods in the Era of LLMs\u003cbr\u003eUnlocking Memorization in Large Language Models with Dynamic Soft Prompting\u003cbr\u003eTowards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters\u003cbr\u003eAn Analysis and Mitigation of the Reversal Curse\u003cbr\u003eLayer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models\u003cbr\u003eLongAlign: A Recipe for Long Context Alignment of Large Language Models\u003cbr\u003e... and 8 more","\u003cb\u003ePlanning\u003c\u002fb\u003e\u003cbr\u003eMSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making\u003cbr\u003eRetrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation\u003cbr\u003eMAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration\u003cbr\u003eUnlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models\u003cbr\u003eASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?\u003cbr\u003eTraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering\u003cbr\u003eAnalysis of Plan-based Retrieval for Grounded Text Generation\u003cbr\u003eMathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models\u003cbr\u003eCAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans\u003cbr\u003eThoughts to Target: Enhance Planning for Target-driven Conversation\u003cbr\u003e... and 8 more","\u003cb\u003eLow-rank Adaptation\u003c\u002fb\u003e\u003cbr\u003eRoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning\u003cbr\u003eOn Mitigating Performance Disparities in Multilingual Speech Recognition\u003cbr\u003eMixture-of-Subspaces in Low-Rank Adaptation\u003cbr\u003eFast Forwarding Low-Rank Training\u003cbr\u003eLoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models\u003cbr\u003eHeterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models\u003cbr\u003eRepMatch: Quantifying Cross-Instance Similarities in Representation Space\u003cbr\u003eApiQ: Finetuning of 2-Bit Quantized Large Language Model\u003cbr\u003eExploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation\u003cbr\u003eIntroducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly\u003cbr\u003e... and 8 more","\u003cb\u003eRepresentation Learning\u003c\u002fb\u003e\u003cbr\u003ePredicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement\u003cbr\u003ePromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval\u003cbr\u003eToken Erasure as a Footprint of Implicit Vocabulary Items in LLMs\u003cbr\u003eIf CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions\u003cbr\u003eCan Transformers Learn n-gram Language Models?\u003cbr\u003eBridging Local Details and Global Context in Text-Attributed Graphs\u003cbr\u003eLeveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003eA Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives\u003cbr\u003eAutomated Tone Transcription and Clustering with Tone2Vec\u003cbr\u003e... and 8 more","\u003cb\u003eDialogue Systems\u003c\u002fb\u003e\u003cbr\u003eBe Helpful but Don't Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support\u003cbr\u003eEfficient Sequential Decision Making with Large Language Models\u003cbr\u003eEvaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding\u003cbr\u003eIntegrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation\u003cbr\u003eIntegrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation\u003cbr\u003eRA2FD: Distilling Faithfulness into Efficient Dialogue Systems\u003cbr\u003ePcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity\u003cbr\u003eDC-Instruct: An Effective Framework for Generative Multi-intent Spoken Language Understanding\u003cbr\u003eAre LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues\u003cbr\u003eClass Name Guided Out-of-Scope Intent Classification\u003cbr\u003e... and 8 more","\u003cb\u003eEvent Extraction\u003c\u002fb\u003e\u003cbr\u003eIntegrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training\u003cbr\u003eADELIE: Aligning Large Language Models on Information Extraction\u003cbr\u003eExplicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments\u003cbr\u003eExplicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments\u003cbr\u003eSPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness\u003cbr\u003eSPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness\u003cbr\u003eMedia Attitude Detection via Framing Analysis with Events and their Relations\u003cbr\u003eGeneral Collaborative Framework between Large Language Model and Experts for Universal Information Extraction\u003cbr\u003eDocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction\u003cbr\u003eDocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction\u003cbr\u003e... and 8 more","\u003cb\u003eMulti-hop Reasoning\u003c\u002fb\u003e\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003eSeemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?\u003cbr\u003eFirst Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning\u003cbr\u003eAdaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations\u003cbr\u003eAdaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations\u003cbr\u003eReasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\u003cbr\u003eCOMPACT: Compressing Retrieved Documents Actively for Question Answering\u003cbr\u003eAn Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models\u003cbr\u003eAn Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models\u003cbr\u003eDetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?\u003cbr\u003e... and 8 more","\u003cb\u003eAttention Mechanism\u003c\u002fb\u003e\u003cbr\u003eHow do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning\u003cbr\u003eExternal Knowledge-Driven Argument Mining: Leveraging Attention-Enhanced Multi-Network Models\u003cbr\u003ePosition Engineering: Boosting Large Language Models through Positional Information Manipulation\u003cbr\u003eDAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination\u003cbr\u003eContextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation\u003cbr\u003eMARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction\u003cbr\u003eDKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction\u003cbr\u003eChain and Causal Attention for Efficient Entity Tracking\u003cbr\u003eROCEL: Advancing Table Entity Linking through Distinctive Row and Column Contexts\u003cbr\u003eInterpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions\u003cbr\u003e... and 8 more","\u003cb\u003eAutomatic Speech Recognition\u003c\u002fb\u003e\u003cbr\u003eVoices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eOn Mitigating Performance Disparities in Multilingual Speech Recognition\u003cbr\u003eOptimized Speculative Sampling for GPU Hardware Accelerators\u003cbr\u003eAdvancing Test-Time Adaptation in Wild Acoustic Test Settings\u003cbr\u003eMuting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models\u003cbr\u003eTask Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition\u003cbr\u003eTowards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights\u003cbr\u003eUnveiling the Role of Pretraining in Direct Speech Translation\u003cbr\u003eInterventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding\u003cbr\u003e... and 8 more","\u003cb\u003eDatasets\u003c\u002fb\u003e\u003cbr\u003eMEANT: Multimodal Encoder for Antecedent Information\u003cbr\u003eA Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery\u003cbr\u003eText2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback\u003cbr\u003ePuzzle Solving using Reasoning of Large Language Models: A Survey\u003cbr\u003eSciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents\u003cbr\u003eMAIR: A Massive Benchmark for Evaluating Instructed Retrieval\u003cbr\u003eDistractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation\u003cbr\u003eAdaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations\u003cbr\u003eAutomated Essay Scoring: A Reflection on the State of the Art\u003cbr\u003eLet's discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment\u003cbr\u003e... and 8 more","\u003cb\u003eText-to-image Generation\u003c\u002fb\u003e\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003eImageInWords: Unlocking Hyper-Detailed Image Descriptions\u003cbr\u003ePre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation\u003cbr\u003ePre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation\u003cbr\u003eWords Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation\u003cbr\u003eWords Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation\u003cbr\u003eEmpowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training\u003cbr\u003eThe Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention\u003cbr\u003eRe-ReST: Reflection-Reinforced Self-Training for Language Agents\u003cbr\u003eGOME: Grounding-based Metaphor Binding With Conceptual Elaboration For Figurative Language Illustration\u003cbr\u003e... and 7 more","\u003cb\u003eUncertainty\u003c\u002fb\u003e\u003cbr\u003eUncertainty in Language Models: Assessment through Rank-Calibration\u003cbr\u003eMaking Large Language Models Better Reasoners with Orchestrated Streaming Experiences\u003cbr\u003eEffective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process\u003cbr\u003eQUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios\u003cbr\u003eWhispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models\u003cbr\u003eCalibrating the Confidence of Large Language Models by Eliciting Fidelity\u003cbr\u003eCan Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?\u003cbr\u003eDoes Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?\u003cbr\u003eStatistical Uncertainty in Word Embeddings: GloVe-V\u003cbr\u003eThe Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning\u003cbr\u003e... and 7 more","\u003cb\u003eReward Model\u003c\u002fb\u003e\u003cbr\u003eLearning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing\u003cbr\u003eEliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence\u003cbr\u003eThe Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models\u003cbr\u003eWeak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems\u003cbr\u003eA Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors\u003cbr\u003eImproving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003eDon't Forget Your Reward Values: Language Model Alignment via Value-based Calibration\u003cbr\u003ePreference-Guided Reflective Sampling for Aligning Language Models\u003cbr\u003eFiltered Direct Preference Optimization\u003cbr\u003e... and 7 more","\u003cb\u003eGender Bias\u003c\u002fb\u003e\u003cbr\u003eOn the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models\u003cbr\u003e\"You Gotta be a Doctor, Lin\": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations\u003cbr\u003eHumans or LLMs as the Judge? A Study on Judgement Bias\u003cbr\u003eApplying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation\u003cbr\u003eImages Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective\u003cbr\u003eFrom Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment\u003cbr\u003eWhat the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study\u003cbr\u003eLocal Contrastive Editing of Gender Stereotypes\u003cbr\u003eA Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models\u003cbr\u003eJobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models\u003cbr\u003e... and 7 more","\u003cb\u003eMath\u003c\u002fb\u003e\u003cbr\u003eParameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eAdvancing Process Verification for Large Language Models via Tree-Based Preference Learning\u003cbr\u003ePython is Not Always the Best Choice: Embracing Multilingual Program of Thoughts\u003cbr\u003eNash CoT: Multi-Path Inference with Preference Equilibrium\u003cbr\u003eDogeRM: Equipping Reward Models with Domain Knowledge through Model Merging\u003cbr\u003eSCIAGENT: Tool-augmented Language Models for Scientific Reasoning\u003cbr\u003eExploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems\u003cbr\u003eReasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\u003cbr\u003eTask Oriented In-Domain Data Augmentation\u003cbr\u003e... and 7 more","\u003cb\u003eRetrieval\u003c\u002fb\u003e\u003cbr\u003eLONGEMBED: Extending Embedding Models for Long Context Retrieval\u003cbr\u003eUniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation\u003cbr\u003eHow Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?\u003cbr\u003eQuantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval\u003cbr\u003eFrom RAG to RICHES: Retrieval Interlaced with Sequence Generation\u003cbr\u003eOptimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach\u003cbr\u003eAnalysis of Plan-based Retrieval for Grounded Text Generation\u003cbr\u003eKAR\u00b3L: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students\u003cbr\u003ePcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity\u003cbr\u003eEnhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition\u003cbr\u003e... and 7 more","\u003cb\u003ePruning\u003c\u002fb\u003e\u003cbr\u003eRethinking Token Reduction for State Space Models\u003cbr\u003eFFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping\u003cbr\u003ePruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging\u003cbr\u003ePruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging\u003cbr\u003eThreshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval\u003cbr\u003eExploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation\u003cbr\u003eLocal Contrastive Editing of Gender Stereotypes\u003cbr\u003exCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics\u003cbr\u003exCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics\u003cbr\u003eLlama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection\u003cbr\u003e... and 7 more","\u003cb\u003eFact Verification\u003c\u002fb\u003e\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003eNormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization\u003cbr\u003eMolecular Facts: Desiderata for Decontextualization in LLM Fact Verification\u003cbr\u003eMolecular Facts: Desiderata for Decontextualization in LLM Fact Verification\u003cbr\u003ePROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context\u003cbr\u003eRIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning\u003cbr\u003eEvidence Retrieval for Fact Verification using Multi-stage Reranking\u003cbr\u003eEvidence Retrieval for Fact Verification using Multi-stage Reranking\u003cbr\u003eDyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs\u003cbr\u003eHow Entangled is Factuality and Deception in German?\u003cbr\u003e... and 7 more","\u003cb\u003eMedical QA\u003c\u002fb\u003e\u003cbr\u003eMedical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?\u003cbr\u003eLM\u00b2: A Simple Society of Language Models Solves Complex Reasoning\u003cbr\u003eCasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures\u003cbr\u003eAMPO: Automatic Multi-Branched Prompt Optimization\u003cbr\u003eFew shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering\u003cbr\u003eFew shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering\u003cbr\u003eAugmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering\u003cbr\u003eEnhancing Healthcare LLM Trust with Atypical Presentations Recalibration\u003cbr\u003eRoQLlama: A Lightweight Romanian Adapted Language Model\u003cbr\u003eMultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate\u003cbr\u003e... and 7 more","\u003cb\u003eClustering\u003c\u002fb\u003e\u003cbr\u003eClustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation\u003cbr\u003eTracking the perspectives of interacting language models\u003cbr\u003eA Generic Method for Fine-grained Category Discovery in Natural Language Texts\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eLeave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA\u003cbr\u003eStrategic Demonstration Selection for Improved Fairness in LLM In-Context Learning\u003cbr\u003eLatent Concept-based Explanation of NLP Models\u003cbr\u003eVoices in a Crowd: Searching for Clusters of Unique Perspectives\u003cbr\u003eStory Morals: Surfacing value-driven narrative schemas using large language models\u003cbr\u003eDynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG\u003cbr\u003e... and 6 more","\u003cb\u003eMixture Of Experts\u003c\u002fb\u003e\u003cbr\u003eLet the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models\u003cbr\u003eIntegrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification\u003cbr\u003eLEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models\u003cbr\u003eMODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning\u003cbr\u003eScaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models\u003cbr\u003eFEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models\u003cbr\u003eMMOE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts\u003cbr\u003eUnraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications\u003cbr\u003eTurn Waste into Worth: Rectifying Top-k Router of MoE\u003cbr\u003eMedCoT: Medical Chain of Thought via Hierarchical Expert\u003cbr\u003e... and 6 more","\u003cb\u003eLong Context\u003c\u002fb\u003e\u003cbr\u003eLONGEMBED: Extending Embedding Models for Long Context Retrieval\u003cbr\u003eSummary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems\u003cbr\u003eWhere am I? Large Language Models Wandering between Semantics and Structures in Long Contexts\u003cbr\u003eFINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents\u003cbr\u003eInfiniPot: Infinite Context Processing on Memory-Constrained LLMs\u003cbr\u003eIs It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP\u003cbr\u003eLLOCO: Learning Long Contexts Offline\u003cbr\u003eA Simple and Effective L2 Norm-Based Strategy for KV Cache Compression\u003cbr\u003eMemorize Step by Step: Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk\u003cbr\u003eCOMPACT: Compressing Retrieved Documents Actively for Question Answering\u003cbr\u003e... and 6 more","\u003cb\u003eML\u003c\u002fb\u003e\u003cbr\u003eLarge Language Models for Data Annotation and Synthesis: A Survey\u003cbr\u003eA Survey on In-context Learning\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003eUnderstanding \u201cDemocratization\u201d in NLP and ML Research\u003cbr\u003eC3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits\u003cbr\u003eVerba volant, scripta volant? Don't worry! There are computational solutions for protoword reconstruction\u003cbr\u003eTowards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs\u003cbr\u003eOATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants\u003cbr\u003eDA-Code: Agent Data Science Code Generation Benchmark for Large Language Models\u003cbr\u003eDA-Code: Agent Data Science Code Generation Benchmark for Large Language Models\u003cbr\u003e... and 6 more","\u003cb\u003eClip\u003c\u002fb\u003e\u003cbr\u003eEFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models\u003cbr\u003eAfrican or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification\u003cbr\u003eRWKV-CLIP: A Robust Vision-Language Representation Learner\u003cbr\u003eDistilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP\u003cbr\u003eIf CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions\u003cbr\u003eVLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models\u003cbr\u003eVision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification\u003cbr\u003eMemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification\u003cbr\u003eInvestigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models\u003cbr\u003eUpdating CLIP to Prefer Descriptions Over Captions\u003cbr\u003e... and 6 more","\u003cb\u003eQuestion Generation\u003c\u002fb\u003e\u003cbr\u003eQUDSELECT: Selective Decoding for Questions Under Discussion Parsing\u003cbr\u003eCross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages\u003cbr\u003eParaphrase Types Elicit Prompt Engineering Capabilities\u003cbr\u003eQGEval: Benchmarking Multi-dimensional Evaluation for Question Generation\u003cbr\u003eMeasuring the Robustness of NLP Models to Domain Shifts\u003cbr\u003eTranslation of Multifaceted Data without Re-Training of Machine Translation Systems\u003cbr\u003eLearning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain\u003cbr\u003eLearning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain\u003cbr\u003eEvaluation of Question Answer Generation for Portuguese: Insights and Datasets\u003cbr\u003eEvaluation of Question Answer Generation for Portuguese: Insights and Datasets\u003cbr\u003e... and 6 more","\u003cb\u003eToxicity\u003c\u002fb\u003e\u003cbr\u003eEvaluating Psychological Safety of Large Language Models\u003cbr\u003eLLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives\u003cbr\u003eFine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models\u003cbr\u003eData, Data Everywhere: A Guide for Pretraining Dataset Construction\u003cbr\u003eStyle-Specific Neurons for Steering LLMs in Text Style Transfer\u003cbr\u003eHouseholder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective\u003cbr\u003eThe Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis\u003cbr\u003eIntrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis\u003cbr\u003eStyle-Shifting Behaviour of the Manosphere on Reddit\u003cbr\u003eSAFETY-J: Evaluating Safety with Critique\u003cbr\u003e... and 6 more","\u003cb\u003eFact-checking\u003c\u002fb\u003e\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003e\u201cImage, Tell me your story!\" Predicting the original meta-context of visual misinformation\u003cbr\u003eMisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"\u003cbr\u003eMiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents\u003cbr\u003eMiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents\u003cbr\u003eUnknown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data\u003cbr\u003eUnknown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data\u003cbr\u003eMIPD: Exploring Manipulation and Intention in A Novel Corpus of Polish Disinformation\u003cbr\u003eM\u00b3D: MultiModal MultiDocument Fine-Grained Inconsistency Detection\u003cbr\u003e... and 6 more","\u003cb\u003eText-to-sql\u003c\u002fb\u003e\u003cbr\u003eI Need Help! Evaluating LLM\u2019s Ability to Ask for Users\u2019 Support: A Case Study on Text-to-SQL Generation\u003cbr\u003ePTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL\u003cbr\u003ePTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL\u003cbr\u003eMiddleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments\u003cbr\u003eImproving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning\u003cbr\u003eBayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities\u003cbr\u003eImproving Demonstration Diversity by Human-Free Fusing for Text-to-SQL\u003cbr\u003eImproving Demonstration Diversity by Human-Free Fusing for Text-to-SQL\u003cbr\u003eSYNTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA\u003cbr\u003eSYNTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA\u003cbr\u003e... and 6 more","\u003cb\u003eSemantic Parsing\u003c\u002fb\u003e\u003cbr\u003eQUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios\u003cbr\u003eQUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios\u003cbr\u003eLearning to Retrieve Iteratively for In-Context Learning\u003cbr\u003eLearning to Retrieve Iteratively for In-Context Learning\u003cbr\u003eImproving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning\u003cbr\u003eLanguage-to-Code Translation with a Single Labeled Example\u003cbr\u003eLanguage-to-Code Translation with a Single Labeled Example\u003cbr\u003eStrengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations\u003cbr\u003eStrengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations\u003cbr\u003eCross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing\u003cbr\u003e... and 6 more","\u003cb\u003eVqa\u003c\u002fb\u003e\u003cbr\u003eDocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models\u003cbr\u003eFrom the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis\u003cbr\u003eSURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information\u003cbr\u003eCommVQA: Situating Visual Question Answering in Communicative Contexts\u003cbr\u003eCommVQA: Situating Visual Question Answering in Communicative Contexts\u003cbr\u003eERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments\u003cbr\u003eIn-Context Compositional Generalization for Large Vision-Language Models\u003cbr\u003eSnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM\u003cbr\u003eVGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning\u003cbr\u003eVisual Question Decomposition on Multimodal Large Language Models\u003cbr\u003e... and 6 more","\u003cb\u003eSecurity\u003c\u002fb\u003e\u003cbr\u003eSeeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients\u003cbr\u003eSafely Learning with Private Data: A Federated Learning Framework for Large Language Model\u003cbr\u003eTaylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion\u003cbr\u003eCLEANGEN: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models\u003cbr\u003eFishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models\u003cbr\u003eWhere Am I From? Identifying Origin of LLM-generated Content\u003cbr\u003eLarge Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks\u003cbr\u003eBaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting\u003cbr\u003eFrom LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking\u003cbr\u003eJailbreaking LLMs with Arabic Transliteration and Arabizi\u003cbr\u003e... and 6 more","\u003cb\u003eBenchmarks Dataset\u003c\u002fb\u003e\u003cbr\u003eNLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian\u003cbr\u003eText-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction\u003cbr\u003eDATATALES: A Benchmark for Real-World Intelligent Data Narration\u003cbr\u003eGLOBESUMM: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization\u003cbr\u003eTowards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights\u003cbr\u003eUnlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering\u003cbr\u003eDECOR: Improving Coherence in L2 English Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting\u003cbr\u003eFAME: Towards Factual Multi-Task Model Editing\u003cbr\u003eStill Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis\u003cbr\u003eRevealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues\u003cbr\u003e... and 6 more","\u003cb\u003eContinual Pre-training\u003c\u002fb\u003e\u003cbr\u003ePretraining Language Models Using Translationese\u003cbr\u003eBreaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale\u003cbr\u003eBreaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale\u003cbr\u003eA Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models\u003cbr\u003eLLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training\u003cbr\u003eCMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models\u003cbr\u003eCMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models\u003cbr\u003eTask Oriented In-Domain Data Augmentation\u003cbr\u003eSynthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models\u003cbr\u003eBSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain\u003cbr\u003e... and 6 more","\u003cb\u003eEvaluation Metric\u003c\u002fb\u003e\u003cbr\u003eLearning to Extract Structured Entities Using Language Models\u003cbr\u003eVLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models\u003cbr\u003eQGEval: Benchmarking Multi-dimensional Evaluation for Question Generation\u003cbr\u003eRaTEScore: A Metric for Radiology Report Generation\u003cbr\u003eRaTEScore: A Metric for Radiology Report Generation\u003cbr\u003eHolistic Evaluation for Interleaved Text-and-Image Generation\u003cbr\u003eA Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios\u003cbr\u003eLONG2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall\u003cbr\u003eFAITHSCORE: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models\u003cbr\u003eCan LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric\u003cbr\u003e... and 6 more","\u003cb\u003eData Generation\u003c\u002fb\u003e\u003cbr\u003eTable Question Answering for Low-resourced Indic Languages\u003cbr\u003eTable Question Answering for Low-resourced Indic Languages\u003cbr\u003eExploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors\u003cbr\u003eFuseGen: PLM Fusion for Data-generation based Zero-shot Learning\u003cbr\u003eDocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models\u003cbr\u003eIncubating Text Classifiers Following User Instruction with Nothing but LLM\u003cbr\u003eLeveraging pre-trained language models for linguistic analysis: A case of argument structure constructions\u003cbr\u003eMP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs\u003cbr\u003eCan LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese\u003cbr\u003eEfficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts\u003cbr\u003e... and 5 more","\u003cb\u003eDebiasing\u003c\u002fb\u003e\u003cbr\u003e\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models\u003cbr\u003eOvercome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue\u003cbr\u003eWalking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias\u003cbr\u003eMultimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference\u003cbr\u003eDecoding Matters: Addressing Amplification Bias and Homogeneity Issue for LLM-based Recommendation\u003cbr\u003eUnlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization\u003cbr\u003eEfficient Overshadowed Entity Disambiguation by Mitigating Shortcut Learning\u003cbr\u003eFuse to Forget: Bias Reduction and Selective Memorization through Model Fusion\u003cbr\u003eEvaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models\u003cbr\u003eInterventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding\u003cbr\u003e... and 5 more","\u003cb\u003eFactual Knowledge\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eDoes Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?\u003cbr\u003eA Thorough Examination of Decoding Methods in the Era of LLMs\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003e\u003eWhy Does New Knowledge Create Messy Ripple Effects in LLMs?\u003cbr\u003eFAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition\u003cbr\u003eOn the Robustness of Editing Large Language Models\u003cbr\u003eSH2: Self-Highlighted Hesitation Helps You Decode More Truthfully\u003cbr\u003eKnowledge Editing in Language Models via Adapted Direct Preference Optimization\u003cbr\u003eMechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations\u003cbr\u003e... and 5 more","\u003cb\u003eExplainable Ai\u003c\u002fb\u003e\u003cbr\u003eEvaluating Readability and Faithfulness of Concept-based Explanations\u003cbr\u003eEnhancing Advanced Visual Reasoning Ability of Large Language Models\u003cbr\u003eGenerative Models for Automatic Medical Decision Rule Extraction from Text\u003cbr\u003eRationalizing Transformer Predictions via End-To-End Differentiable Self-Training\u003cbr\u003eFINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents\u003cbr\u003eAutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments\u003cbr\u003eFool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting\u003cbr\u003eTV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning\u003cbr\u003eThe Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems\u003cbr\u003eAdaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse\u003cbr\u003e... and 5 more","\u003cb\u003eMulti-modal Learning\u003c\u002fb\u003e\u003cbr\u003eGeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation\u003cbr\u003eMatchTime: Towards Automatic Soccer Game Commentary Generation\u003cbr\u003eEnhancing Advanced Visual Reasoning Ability of Large Language Models\u003cbr\u003eFine-Grained Prediction of Reading Comprehension from Eye Movements\u003cbr\u003eGAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities\u003cbr\u003eBeyond Embeddings: The Promise of Visual Table in Visual Reasoning\u003cbr\u003eFEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models\u003cbr\u003eMOSEL: Inference Serving Using Dynamic Modality Selection\u003cbr\u003eMulti-Level Information Retrieval Augmented Generation for Knowledge-based Visual Question Answering\u003cbr\u003ePreserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality\u003cbr\u003e... and 5 more","\u003cb\u003eAsr\u003c\u002fb\u003e\u003cbr\u003eLLMs Are Zero-Shot Context-Aware Simultaneous Translators\u003cbr\u003eTowards Robust Speech Representation Learning for Thousands of Languages\u003cbr\u003eWhat is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations\u003cbr\u003eUnveiling the Role of Pretraining in Direct Speech Translation\u003cbr\u003eSelf-Powered LLM Modality Expansion for Large Speech-Text Models\u003cbr\u003eContinual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech\u003cbr\u003eInterventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding\u003cbr\u003eTokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR\u003cbr\u003eTokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR\u003cbr\u003eCasablanca: Data and Models for Multidialectal Arabic Speech Recognition\u003cbr\u003e... and 5 more","\u003cb\u003eDense Retrieval\u003c\u002fb\u003e\u003cbr\u003eChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval\u003cbr\u003eChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval\u003cbr\u003eLarge Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment\u003cbr\u003eLarge Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment\u003cbr\u003eUnifying Multimodal Retrieval via Document Screenshot Embedding\u003cbr\u003eTaxonomy-guided Semantic Indexing for Academic Paper Search\u003cbr\u003eImprove Dense Passage Retrieval with Entailment Tuning\u003cbr\u003eLanguage Concept Erasure for Language-invariant Dense Retrieval\u003cbr\u003eLitSearch: A Retrieval Benchmark for Scientific Literature Search\u003cbr\u003eDense X Retrieval: What Retrieval Granularity Should We Use?\u003cbr\u003e... and 5 more","\u003cb\u003eSelf-consistency\u003c\u002fb\u003e\u003cbr\u003eGLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models\u003cbr\u003eDecompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison\u003cbr\u003ePython is Not Always the Best Choice: Embracing Multilingual Program of Thoughts\u003cbr\u003eEmpowering Multi-step Reasoning across Languages via Program-Aided Language Models\u003cbr\u003eAtomic Self-Consistency for Better Long Form Generations\u003cbr\u003eDynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models\u003cbr\u003eTree of Problems: Improving structured problem solving with compositionality\u003cbr\u003eReasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\u003cbr\u003eMirror-Consistency: Harnessing Inconsistency in Majority Voting\u003cbr\u003eA Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction\u003cbr\u003e... and 5 more","\u003cb\u003ePrompt Tuning\u003c\u002fb\u003e\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003eRetrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation\u003cbr\u003eDivide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning\u003cbr\u003eStablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models\u003cbr\u003eMitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment\u003cbr\u003eIntCoOp: Interpretability-Aware Vision-Language Prompt Tuning\u003cbr\u003eDeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators\u003cbr\u003eUnderstanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs\u003cbr\u003eBiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks\u003cbr\u003ePromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning\u003cbr\u003e... and 5 more","\u003cb\u003eMath Reasoning\u003c\u002fb\u003e\u003cbr\u003eSelf-Refine Instruction-Tuning for Aligning Reasoning in Language Models\u003cbr\u003eStepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors\u003cbr\u003eStepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors\u003cbr\u003eFewer is More: Boosting Math Reasoning with Reinforced Context Pruning\u003cbr\u003eFewer is More: Boosting Math Reasoning with Reinforced Context Pruning\u003cbr\u003eSCIAGENT: Tool-augmented Language Models for Scientific Reasoning\u003cbr\u003eCoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage\u003cbr\u003eCan LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks\u003cbr\u003eMathFish: Evaluating Language Model Math Reasoning via Grounding in Educational Curricula\u003cbr\u003eSelf-Consistency Boosts Calibration for Math Reasoning\u003cbr\u003e... and 5 more","\u003cb\u003eKnowledge Retrieval\u003c\u002fb\u003e\u003cbr\u003eEfficientRAG: Efficient Retriever for Multi-Hop Question Answering\u003cbr\u003eDissecting Fine-Tuning Unlearning in Large Language Models\u003cbr\u003eRelevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System\u003cbr\u003eRelevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System\u003cbr\u003eCItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling\u003cbr\u003eSCIPROMPT: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics\u003cbr\u003eAtomic Self-Consistency for Better Long Form Generations\u003cbr\u003eVisual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant\u003cbr\u003eDALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature\u003cbr\u003eThe Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance\u003cbr\u003e... and 5 more","\u003cb\u003ePersonalization\u003c\u002fb\u003e\u003cbr\u003eUnsupervised Human Preference Learning\u003cbr\u003eUnsupervised Human Preference Learning\u003cbr\u003ePERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts\u003cbr\u003eDemocratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning\u003cbr\u003eMisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"\u003cbr\u003eMisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"\u003cbr\u003eAre Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!\u003cbr\u003eJump Starting Bandits with LLM-Generated Prior Knowledge\u003cbr\u003eJump Starting Bandits with LLM-Generated Prior Knowledge\u003cbr\u003eEDEN: Empathetic Dialogues for English Learning\u003cbr\u003e... and 5 more","\u003cb\u003eKnowledge Transfer\u003c\u002fb\u003e\u003cbr\u003eEmpowering Large Language Model for Continual Video Question Answering with Collaborative Prompting\u003cbr\u003eNeuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation\u003cbr\u003eI Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses\u003cbr\u003ePREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment\u003cbr\u003eMcCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering\u003cbr\u003eICL: Iterative Continual Learning for Multi-domain Neural Machine Translation\u003cbr\u003eInfrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models\u003cbr\u003eMoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition\u003cbr\u003eData-Centric AI in the Age of Large Language Models\u003cbr\u003eData-Centric AI in the Age of Large Language Models\u003cbr\u003e... and 5 more","\u003cb\u003eVisual Grounding\u003c\u002fb\u003e\u003cbr\u003eVIMI: Grounding Video Generation through Multi-modal Instruction\u003cbr\u003eWorld to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering\u003cbr\u003eWorld to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering\u003cbr\u003eUOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models\u003cbr\u003eFrom Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models\u003cbr\u003eShaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling\u003cbr\u003eShaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling\u003cbr\u003eFineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension\u003cbr\u003eInvestigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models\u003cbr\u003eShow and Guide: Instructional-Plan Grounded Vision and Language Model\u003cbr\u003e... and 5 more","\u003cb\u003ePretraining\u003c\u002fb\u003e\u003cbr\u003eVIMI: Grounding Video Generation through Multi-modal Instruction\u003cbr\u003eUnveiling the Role of Pretraining in Direct Speech Translation\u003cbr\u003eSegment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation\u003cbr\u003eGRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients\u003cbr\u003eGRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients\u003cbr\u003eGetting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection\u003cbr\u003eGetting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection\u003cbr\u003eBLSP-Emo: Towards Empathetic Large Speech-Language Models\u003cbr\u003eAdaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?\u003cbr\u003eIs Child-Directed Speech Effective Training Data for Language Models?\u003cbr\u003e... and 5 more","\u003cb\u003eTopic Classification\u003c\u002fb\u003e\u003cbr\u003eSEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages\u003cbr\u003eUniversal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning\u003cbr\u003eRevisiting Supervised Contrastive Learning for Microblog Classification\u003cbr\u003eCorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs\u003cbr\u003eSYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation\u003cbr\u003eMultilingual Topic Classification in X: Dataset and Analysis\u003cbr\u003eCoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage\u003cbr\u003eTransfer Learning for Text Classification via Model Risk Analysis\u003cbr\u003eGeneralists vs. Specialists: Evaluating Large Language Models for Urdu\u003cbr\u003eIn-Context Learning with Iterative Demonstration Selection\u003cbr\u003e... and 5 more","\u003cb\u003eTruthfulness\u003c\u002fb\u003e\u003cbr\u003ePersonas as a Way to Model Truthfulness in Language Models\u003cbr\u003ePersonas as a Way to Model Truthfulness in Language Models\u003cbr\u003eOn the Relationship between Truth and Political Bias in Language Models\u003cbr\u003eEnhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding\u003cbr\u003eHouseholder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective\u003cbr\u003eHouseholder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective\u003cbr\u003eThe Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis\u003cbr\u003eOn the Universal Truthfulness Hyperplane Inside LLMS\u003cbr\u003eMulti-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models\u003cbr\u003eMulti-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models\u003cbr\u003e... and 5 more","\u003cb\u003eNeural Machine Translation\u003c\u002fb\u003e\u003cbr\u003eDomain adapted machine translation: What does catastrophic forgetting forget and why?\u003cbr\u003eDomain adapted machine translation: What does catastrophic forgetting forget and why?\u003cbr\u003eEnhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation\u003cbr\u003eGranularity is crucial when applying differential privacy to text: An investigation for neural machine translation\u003cbr\u003eGranularity is crucial when applying differential privacy to text: An investigation for neural machine translation\u003cbr\u003eContrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation\u003cbr\u003eDual-teacher Knowledge Distillation for Low-frequency Word Translation\u003cbr\u003eTogether We Can: Multilingual Automatic Post-Editing for Low-Resource Languages\u003cbr\u003eDICTDIS: Dictionary Constrained Disambiguation for Improved NMT\u003cbr\u003eDICTDIS: Dictionary Constrained Disambiguation for Improved NMT\u003cbr\u003e... and 5 more","\u003cb\u003eTemporal Reasoning\u003c\u002fb\u003e\u003cbr\u003eTemporally Consistent Factuality Probing for Large Language Models\u003cbr\u003eTemporally Consistent Factuality Probing for Large Language Models\u003cbr\u003eCAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans\u003cbr\u003eVideo-Text Prompting for Weakly Supervised Spatio-Temporal Video Grounding\u003cbr\u003eA Simple LLM Framework for Long-Range Video Question-Answering\u003cbr\u003eMIBench: Evaluating Multimodal Large Language Models over Multiple Images\u003cbr\u003eTemporal Fact Reasoning over Hyper-Relational Knowledge Graphs\u003cbr\u003eExploring Question Guidance and Answer Calibration for Visually Grounded Video Question Answering\u003cbr\u003eDyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs\u003cbr\u003eUpdating Large Language Models' Memories with Time Constraints\u003cbr\u003e... and 5 more","\u003cb\u003eFactual Consistency\u003c\u002fb\u003e\u003cbr\u003eFIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document\u003cbr\u003eEVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation\u003cbr\u003eECON: On the Detection and Resolution of Evidence Conflicts\u003cbr\u003eEvaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works\u003cbr\u003eCliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios\u003cbr\u003eCliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios\u003cbr\u003eDetecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors\u003cbr\u003eRationale-Aware Answer Verification by Pairwise Self-Evaluation\u003cbr\u003eSummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization\u003cbr\u003eLearning from Implicit User Feedback, Emotions and Demographic Information in Task-Oriented and Document-Grounded Dialogues\u003cbr\u003e... and 4 more","\u003cb\u003eTransferability\u003c\u002fb\u003e\u003cbr\u003ePrompts have evil twins\u003cbr\u003eDA\u00b3: A Distribution-Aware Adversarial Attack against Language Models\u003cbr\u003eASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings\u003cbr\u003eConcept-skill Transferability-based Data Selection for Large Vision-Language Models\u003cbr\u003eWhen Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models\u003cbr\u003eDemystifying Verbatim Memorization in Large Language Models\u003cbr\u003eRethinking the Evaluation of In-Context Learning for LLMs\u003cbr\u003eMIXTURE-OF-SKILLS: Learning to Optimize Data Usage for Fine-Tuning Large Language Models\u003cbr\u003eExtracting Prompts by Inverting LLM Outputs\u003cbr\u003eInstruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks\u003cbr\u003e... and 4 more","\u003cb\u003eRelevance\u003c\u002fb\u003e\u003cbr\u003eConsolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing\u003cbr\u003eBe Helpful but Don't Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support\u003cbr\u003eEvaluating D-MERIT of Partial-annotation on Information Retrieval\u003cbr\u003eThe Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models\u003cbr\u003eThe Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models\u003cbr\u003eRetrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation\u003cbr\u003eImprove Dense Passage Retrieval with Entailment Tuning\u003cbr\u003eQGEval: Benchmarking Multi-dimensional Evaluation for Question Generation\u003cbr\u003eTopic-Oriented Open Relation Extraction with A Priori Seed Generation\u003cbr\u003eLearning to Rank Salient Content for Query-focused Summarization\u003cbr\u003e... and 4 more","\u003cb\u003eGpt-4\u003c\u002fb\u003e\u003cbr\u003eGeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation\u003cbr\u003eA User-Centric Multi-Intent Benchmark for Evaluating Large Language Models\u003cbr\u003eTeaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use\u003cbr\u003eLeveraging pre-trained language models for linguistic analysis: A case of argument structure constructions\u003cbr\u003eAnnotation alignment: Comparing LLM and human annotations of conversational safety\u003cbr\u003eStory Morals: Surfacing value-driven narrative schemas using large language models\u003cbr\u003eLitSearch: A Retrieval Benchmark for Scientific Literature Search\u003cbr\u003eGeneralizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4\u003cbr\u003eThe Death and Life of Great Prompts: Analyzing the Evolution of LLM Prompts from the Structural Perspective\u003cbr\u003eCreative and Context-Aware Translation of East Asian Idioms with GPT-4\u003cbr\u003e... and 4 more","\u003cb\u003eEvaluation Benchmarks\u003c\u002fb\u003e\u003cbr\u003eSHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation\u003cbr\u003eSHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation\u003cbr\u003eAKEW: Assessing Knowledge Editing in the Wild\u003cbr\u003eAKEW: Assessing Knowledge Editing in the Wild\u003cbr\u003eMT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models\u003cbr\u003eLONGGENBENCH: Long-context Generation Benchmark\u003cbr\u003eMM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification\u003cbr\u003eUniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs\u003cbr\u003eLosing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts\u003cbr\u003ePSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer\u003cbr\u003e... and 4 more","\u003cb\u003eMechanistic Interpretability\u003c\u002fb\u003e\u003cbr\u003eNeuron-Level Knowledge Attribution in Large Language Models\u003cbr\u003eInterpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis\u003cbr\u003eUnlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models\u003cbr\u003eBeyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning\u003cbr\u003eEncourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective\u003cbr\u003eTowards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models\u003cbr\u003eThe Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis\u003cbr\u003eInterpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions\u003cbr\u003eInformation Flow Routes: Automatically Interpreting Language Models at Scale\u003cbr\u003eDeeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning\u003cbr\u003e... and 4 more","\u003cb\u003ePreference Alignment\u003c\u002fb\u003e\u003cbr\u003eCoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering\u003cbr\u003ePredicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model\u003cbr\u003eVLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment\u003cbr\u003eEncourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective\u003cbr\u003eEncourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective\u003cbr\u003eEvidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering\u003cbr\u003eORPO: Monolithic Preference Optimization without Reference Model\u003cbr\u003eThemis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability\u003cbr\u003eFlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization\u003cbr\u003eFlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization\u003cbr\u003e... and 4 more","\u003cb\u003eKnowledge Representation\u003c\u002fb\u003e\u003cbr\u003eCoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering\u003cbr\u003eSATYRN: A Platform for Analytics Augmented Generation\u003cbr\u003eDiscovering Knowledge-Critical Subnetworks in Pretrained Language Models\u003cbr\u003eBeyond Embeddings: The Promise of Visual Table in Visual Reasoning\u003cbr\u003eUnraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications\u003cbr\u003eKAR\u00b3L: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students\u003cbr\u003eInference Helps PLMs' Conceptual Understanding: Improving the Abstract Inference Ability with Hierarchical Conceptual Entailment Graphs\u003cbr\u003eTemporal Fact Reasoning over Hyper-Relational Knowledge Graphs\u003cbr\u003eCan Large Language Models Understand DL-Lite Ontologies? An Empirical Study\u003cbr\u003eEnhancing Incremental Summarization with Structured Representations\u003cbr\u003e... and 4 more","\u003cb\u003eVideo QA\u003c\u002fb\u003e\u003cbr\u003eEmpowering Large Language Model for Continual Video Question Answering with Collaborative Prompting\u003cbr\u003eVideo-LLaVA: Learning United Visual Representation by Alignment Before Projection\u003cbr\u003eEncoding and Controlling Global Semantics for Long-form Video Question Answering\u003cbr\u003eTraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering\u003cbr\u003eEfficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge\u003cbr\u003eTV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning\u003cbr\u003eA Simple LLM Framework for Long-Range Video Question-Answering\u003cbr\u003eTraining-free Deep Concept Injection Enables Language Models for Video Question Answering\u003cbr\u003eEnhancing Temporal Modeling of Video LLMs via Time Gating\u003cbr\u003eEnhancing Temporal Modeling of Video LLMs via Time Gating\u003cbr\u003e... and 4 more","\u003cb\u003eMultilingual Llms\u003c\u002fb\u003e\u003cbr\u003eTeaching LLMs to Abstain across Languages via Multilingual Feedback\u003cbr\u003eAn Analysis of Multilingual FActScore\u003cbr\u003eConcept Space Alignment in Multilingual LLMs\u003cbr\u003eUnderstanding and Mitigating Language Confusion in LLMs\u003cbr\u003eIs It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?\u003cbr\u003eRLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs\u003cbr\u003eInvestigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?\u003cbr\u003eExploring Multilingual Concepts of Human Values in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?\u003cbr\u003ePruning Multilingual Large Language Models for Multilingual Inference\u003cbr\u003ePreference Tuning For Toxicity Mitigation Generalizes Across Languages\u003cbr\u003e... and 4 more","\u003cb\u003eGenerative Models\u003c\u002fb\u003e\u003cbr\u003eDistilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP\u003cbr\u003eGenerative Models for Automatic Medical Decision Rule Extraction from Text\u003cbr\u003eEnhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension\u003cbr\u003eAn image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance\u003cbr\u003eExplicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments\u003cbr\u003eWeak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems\u003cbr\u003eAudioVSR: Enhancing Video Speech Recognition with Audio Data\u003cbr\u003eGRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization\u003cbr\u003eAltogether: Image Captioning via Re-aligning Alt-text\u003cbr\u003eGREEN: Generative Radiology Report Evaluation and Error Notation\u003cbr\u003e... and 4 more","\u003cb\u003eGrammatical Error Correction\u003c\u002fb\u003e\u003cbr\u003eA Survey of AMR Applications\u003cbr\u003eMulti-pass Decoding for Grammatical Error Correction\u003cbr\u003eMulti-pass Decoding for Grammatical Error Correction\u003cbr\u003eLLM-based Code-Switched Text Generation for Grammatical Error Correction\u003cbr\u003eLLM-based Code-Switched Text Generation for Grammatical Error Correction\u003cbr\u003eTo Err Is Human, but Llamas Can Learn It Too\u003cbr\u003eTo Err Is Human, but Llamas Can Learn It Too\u003cbr\u003eTowards Explainable Chinese Native Learner Essay Fluency Assessment: Dataset, Tasks, and Method\u003cbr\u003eTo Ask LLMs about English Grammaticality, Prompt Them in a Different Language\u003cbr\u003eTo Ask LLMs about English Grammaticality, Prompt Them in a Different Language\u003cbr\u003e... and 4 more","\u003cb\u003eWatermarking\u003c\u002fb\u003e\u003cbr\u003ePOSTMARK: A Robust Blackbox Watermark for Large Language Models\u003cbr\u003ePOSTMARK: A Robust Blackbox Watermark for Large Language Models\u003cbr\u003eWhere Am I From? Identifying Origin of LLM-generated Content\u003cbr\u003eWhere Am I From? Identifying Origin of LLM-generated Content\u003cbr\u003eRevisiting the Robustness of Watermarking to Paraphrasing Attacks\u003cbr\u003eContext-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models\u003cbr\u003eContext-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models\u003cbr\u003eGuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack\u003cbr\u003eGuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack\u003cbr\u003eCODEIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code\u003cbr\u003e... and 4 more","\u003cb\u003eLink Prediction\u003c\u002fb\u003e\u003cbr\u003eMQuinE: a Cure for \"Z-paradox\" in Knowledge Graph Embedding\u003cbr\u003eMQuinE: a Cure for \"Z-paradox\" in Knowledge Graph Embedding\u003cbr\u003eMoCoKGC: Momentum Contrast Entity Encoding for Knowledge Graph Completion\u003cbr\u003eATAP: Automatic Template-Augmented Commonsense Knowledge Graph Completion via Pre-Trained Language Models\u003cbr\u003eCan Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction\u003cbr\u003eCan Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction\u003cbr\u003ePredictive Multiplicity of Knowledge Graph Embeddings in Link Prediction\u003cbr\u003ePredictive Multiplicity of Knowledge Graph Embeddings in Link Prediction\u003cbr\u003eTemporal Fact Reasoning over Hyper-Relational Knowledge Graphs\u003cbr\u003eTemporal Fact Reasoning over Hyper-Relational Knowledge Graphs\u003cbr\u003e... and 4 more","\u003cb\u003eMmlu\u003c\u002fb\u003e\u003cbr\u003eFine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs\u003cbr\u003eMODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning\u003cbr\u003eUnveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement\u003cbr\u003eReasoning Robustness of LLMs to Adversarial Typographical Errors\u003cbr\u003eTurn Waste into Worth: Rectifying Top-k Router of MoE\u003cbr\u003eLLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks\u003cbr\u003eBreaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling\u003cbr\u003eThe Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance\u003cbr\u003eThe Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance\u003cbr\u003eTurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish\u003cbr\u003e... and 3 more","\u003cb\u003ePerplexity\u003c\u002fb\u003e\u003cbr\u003eA Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers\u003cbr\u003eScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws\u003cbr\u003eEvaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding\u003cbr\u003eI Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses\u003cbr\u003eTEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models\u003cbr\u003eSocial Bias Probing: Fairness Benchmarking for Language Models\u003cbr\u003eHow to Compute the Probability of a Word\u003cbr\u003eWho is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models\u003cbr\u003eAdaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?\u003cbr\u003eScalable Data Ablation Approximations for Language Models through Modular Training and Merging\u003cbr\u003e... and 3 more","\u003cb\u003eMeta-evaluation\u003c\u002fb\u003e\u003cbr\u003eEvaluating Readability and Faithfulness of Concept-based Explanations\u003cbr\u003eFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments\u003cbr\u003eAPPLS: Evaluating Evaluation Metrics for Plain Language Summarization\u003cbr\u003eRevisiting Automated Evaluation for Long-form Table Question Answering\u003cbr\u003eRevisiting Automated Evaluation for Long-form Table Question Answering\u003cbr\u003eFinding Blind Spots in Evaluator LLMs with Interpretable Checklists\u003cbr\u003eOffsetBias: Leveraging Debiased Data for Tuning Evaluators\u003cbr\u003eOffsetBias: Leveraging Debiased Data for Tuning Evaluators\u003cbr\u003eSAFETY-J: Evaluating Safety with Critique\u003cbr\u003eHow Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?\u003cbr\u003e... and 3 more","\u003cb\u003eDataset Creation\u003c\u002fb\u003e\u003cbr\u003eCOCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds\u003cbr\u003eSusu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.\u003cbr\u003eClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures\u003cbr\u003eCasablanca: Data and Models for Multidialectal Arabic Speech Recognition\u003cbr\u003eEmosical: An Emotion-Annotated Musical Theatre Dataset\u003cbr\u003eForecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling\u003cbr\u003eDetecting Temporal Ambiguity in Questions\u003cbr\u003eFrom Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues\u003cbr\u003eAsk the experts: Sourcing a high-quality nutrition counseling dataset through Human-AI collaboration\u003cbr\u003eAdversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation\u003cbr\u003e... and 3 more","\u003cb\u003eRed Teaming\u003c\u002fb\u003e\u003cbr\u003eFLIRT: Feedback Loop In-context Red Teaming\u003cbr\u003eFLIRT: Feedback Loop In-context Red Teaming\u003cbr\u003eAdvancing Adversarial Suffix Transfer Learning on Aligned Large Language Models\u003cbr\u003eRed Teaming Language Models for Processing Contradictory Dialogues\u003cbr\u003eHolistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction\u003cbr\u003eDistract Large Language Models for Automatic Jailbreak Attack\u003cbr\u003eCoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference\u003cbr\u003eCoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference\u003cbr\u003eSTAR: SocioTechnical Approach to Red Teaming Language Models\u003cbr\u003eSTAR: SocioTechnical Approach to Red Teaming Language Models\u003cbr\u003e... and 3 more","\u003cb\u003eError Detection\u003c\u002fb\u003e\u003cbr\u003eSuccessfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections\u003cbr\u003eADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning\u003cbr\u003eStepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors\u003cbr\u003eStepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors\u003cbr\u003eJellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing\u003cbr\u003eJellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing\u003cbr\u003eSTORYSUMM: Evaluating Faithfulness in Story Summarization\u003cbr\u003eDetecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors\u003cbr\u003eCan Automatic Metrics Assess High-Quality Translations?\u003cbr\u003eBi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check\u003cbr\u003e... and 3 more","\u003cb\u003eLarge Multimodal Models\u003c\u002fb\u003e\u003cbr\u003eMitigating Language Bias of LMMs in Social Intelligence Understanding with Virtual Counterfactual Calibration\u003cbr\u003eTowards Low-Resource Harmful Meme Detection with LMM Agents\u003cbr\u003eTraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering\u003cbr\u003eTowards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights\u003cbr\u003eUNICORN: A Unified Causal Video-Oriented Language-Modeling Framework for Temporal Video-Language Tasks\u003cbr\u003eDocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding\u003cbr\u003eVisual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant\u003cbr\u003eAn Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models\u003cbr\u003eMM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification\u003cbr\u003eMULTISKILL: Evaluating Large Multimodal Models for Fine-grained Alignment Skills\u003cbr\u003e... and 3 more","\u003cb\u003eCausal Inference\u003c\u002fb\u003e\u003cbr\u003eMitigating Language Bias of LMMs in Social Intelligence Understanding with Virtual Counterfactual Calibration\u003cbr\u003eMultimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference\u003cbr\u003eLLMs Are Prone to Fallacies in Causal Inference\u003cbr\u003eLLMs Are Prone to Fallacies in Causal Inference\u003cbr\u003eSLANG: New Concept Comprehension of Large Language Models\u003cbr\u003eSLANG: New Concept Comprehension of Large Language Models\u003cbr\u003eAutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments\u003cbr\u003eAutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments\u003cbr\u003eThe Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning\u003cbr\u003eDual-oriented Disentangled Network with Counterfactual Intervention for Multimodal Intent Detection\u003cbr\u003e... and 3 more","\u003cb\u003eTrustworthiness\u003c\u002fb\u003e\u003cbr\u003eCan Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?\u003cbr\u003eModel Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation\u003cbr\u003eCan Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?\u003cbr\u003eKnowledge Conflicts for LLMs: A Survey\u003cbr\u003eSynchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation\u003cbr\u003eFactuality of Large Language Models: A Survey\u003cbr\u003eEnhancing Healthcare LLM Trust with Atypical Presentations Recalibration\u003cbr\u003eGenerating Media Background Checks for Automated Source Critical Reasoning\u003cbr\u003eIn-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models\u003cbr\u003eMultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate\u003cbr\u003e... and 3 more","\u003cb\u003eChinese\u003c\u002fb\u003e\u003cbr\u003eDo We Need Language-Specific Fact-Checking Models? The Case of Chinese\u003cbr\u003eToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations\u003cbr\u003eGold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs\u003cbr\u003eRe-Evaluating Evaluation for Multilingual Summarization\u003cbr\u003eDocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction\u003cbr\u003eEmploying Glyphic Information for Chinese Event Extraction with Vision-Language Model\u003cbr\u003eCLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models\u003cbr\u003eOEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary\u003cbr\u003eEvaluating Moral Beliefs across LLMs through a Pluralistic Framework\u003cbr\u003eCERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays\u003cbr\u003e... and 3 more","\u003cb\u003eScalability\u003c\u002fb\u003e\u003cbr\u003eOptimizing Code Retrieval: High-Quality and Scalable Dataset Annotation through Large Language Models\u003cbr\u003eMisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"\u003cbr\u003eBridging Local Details and Global Context in Text-Attributed Graphs\u003cbr\u003eWorking Memory Identifies Reasoning Limits in Language Models\u003cbr\u003eReasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\u003cbr\u003eWaterfall: Scalable Framework for Robust Text Watermarking and Provenance for LLMs\u003cbr\u003eComparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval\u003cbr\u003eLLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning\u003cbr\u003eAdvancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts\u003cbr\u003eTS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models\u003cbr\u003e... and 3 more","\u003cb\u003eGeneralizability\u003c\u002fb\u003e\u003cbr\u003eTowards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models\u003cbr\u003eOn Training Data Influence of GPT Models\u003cbr\u003eWhat is \u201cTypological Diversity\u201d in NLP?\u003cbr\u003eJellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing\u003cbr\u003eVLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models\u003cbr\u003eSecCoder: Towards Generalizable and Robust Secure Code Generation\u003cbr\u003eText Fluoroscopy: Detecting LLM-Generated Text through Intrinsic Features\u003cbr\u003eLM\u00b2: A Simple Society of Language Models Solves Complex Reasoning\u003cbr\u003eInterventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding\u003cbr\u003eEvaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets\u003cbr\u003e... and 3 more","\u003cb\u003eAutomatic Evaluation\u003c\u002fb\u003e\u003cbr\u003eLarge Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course\u003cbr\u003eLarge Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course\u003cbr\u003eIs Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering\u003cbr\u003eARXIVDIGESTABLES: Synthesizing Scientific Literature into Tables using Language Models\u003cbr\u003eMore Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation\u003cbr\u003eWhat the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study\u003cbr\u003eError Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation\u003cbr\u003eCan AI Relate: Testing Large Language Model Response for Mental Health Support\u003cbr\u003eEvaluating Automatic Metrics with Incremental Machine Translation Systems\u003cbr\u003eHow Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?\u003cbr\u003e... and 3 more","\u003cb\u003eDownstream Tasks\u003c\u002fb\u003e\u003cbr\u003eCollaborative Performance Prediction for Large Language Models\u003cbr\u003eTEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models\u003cbr\u003eScalable Efficient Training of Large Language Models with Low-dimensional Projected Attention\u003cbr\u003eA Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives\u003cbr\u003eModel Editing Harms General Abilities of Large Language Models: Regularization to the Rescue\u003cbr\u003eMoral Foundations of Large Language Models\u003cbr\u003eMemory-Efficient Fine-Tuning of Transformers via Token Selection\u003cbr\u003eAxis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings\u003cbr\u003eLayer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models\u003cbr\u003eAre Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning?\u003cbr\u003e... and 3 more","\u003cb\u003eSemantic Textual Similarity\u003c\u002fb\u003e\u003cbr\u003eSurveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese\u003cbr\u003eAdvancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss\u003cbr\u003eAdvancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss\u003cbr\u003ePcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity\u003cbr\u003ePcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity\u003cbr\u003eScaling Sentence Embeddings with Large Language Models\u003cbr\u003eVariational Language Concepts for Interpreting Foundation Language Models\u003cbr\u003eHit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention\u003cbr\u003eAre ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity\u003cbr\u003eAre ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity\u003cbr\u003e... and 3 more","\u003cb\u003eAttribution\u003c\u002fb\u003e\u003cbr\u003ePhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study\u003cbr\u003eAdvancing Large Language Model Attribution through Self-Improving\u003cbr\u003eAttribute or Abstain: Large Language Models as Long Document Assistants\u003cbr\u003eAGRAME: Any-Granularity Ranking with Multi-Vector Embeddings\u003cbr\u003eWhere Am I From? Identifying Origin of LLM-generated Content\u003cbr\u003eLatent Concept-based Explanation of NLP Models\u003cbr\u003eAnalysis of Plan-based Retrieval for Grounded Text Generation\u003cbr\u003eInformation Flow Routes: Automatically Interpreting Language Models at Scale\u003cbr\u003eCoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity\u003cbr\u003eImproving LLM Attributions with Randomized Path-Integration\u003cbr\u003e... and 3 more","\u003cb\u003eInference\u003c\u002fb\u003e\u003cbr\u003eQUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models\u003cbr\u003eQUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models\u003cbr\u003eEAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees\u003cbr\u003eInferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance\u003cbr\u003eHiFT: A Hierarchical Full Parameter Fine-Tuning Strategy\u003cbr\u003eLOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference\u003cbr\u003eNormalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction\u003cbr\u003eFast Matrix Multiplications for Lookup Table-Quantized LLMs\u003cbr\u003eQEFT: Quantization for Efficient Fine-Tuning of LLMs\u003cbr\u003eQEFT: Quantization for Efficient Fine-Tuning of LLMs\u003cbr\u003e... and 3 more","\u003cb\u003eMisinformation\u003c\u002fb\u003e\u003cbr\u003eF2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation\u003cbr\u003eECON: On the Detection and Resolution of Evidence Conflicts\u003cbr\u003eMisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"\u003cbr\u003e\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models\u003cbr\u003eOn the Relationship between Truth and Political Bias in Language Models\u003cbr\u003eIntegrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation\u003cbr\u003eDecoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach\u003cbr\u003eCan Large Language Models Identify Authorship?\u003cbr\u003eSAFETY-J: Evaluating Safety with Critique\u003cbr\u003eCan Language Models Recognize Convincing Arguments?\u003cbr\u003e... and 3 more","\u003cb\u003eSelf-training\u003c\u002fb\u003e\u003cbr\u003eLogicST: A Logical Self-Training Framework for Document-Level Relation Extraction with Incomplete Annotations\u003cbr\u003eRationalizing Transformer Predictions via End-To-End Differentiable Self-Training\u003cbr\u003eSelf-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models\u003cbr\u003eSelf-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models\u003cbr\u003eRe-ReST: Reflection-Reinforced Self-Training for Language Agents\u003cbr\u003eSelf-Training Large Language and Vision Assistant for Medical Question-Answering\u003cbr\u003eCan LLMs Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner\u003cbr\u003eSELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards\u003cbr\u003eSemi-Supervised Reward Modeling via Iterative Self-Training\u003cbr\u003eSelf-training Language Models for Arithmetic Reasoning\u003cbr\u003e... and 3 more","\u003cb\u003eOpen-domain Qa\u003c\u002fb\u003e\u003cbr\u003eREAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering\u003cbr\u003eTowards Verifiable Text Generation with Evolving Memory and Self-Reflection\u003cbr\u003eAGRAME: Any-Granularity Ranking with Multi-Vector Embeddings\u003cbr\u003eImprove Dense Passage Retrieval with Entailment Tuning\u003cbr\u003eChain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models\u003cbr\u003eDense X Retrieval: What Retrieval Granularity Should We Use?\u003cbr\u003eDense X Retrieval: What Retrieval Granularity Should We Use?\u003cbr\u003eNot All Contexts Are Equal: Teaching LLMs Credibility-aware Generation\u003cbr\u003eRaFe: Ranking Feedback Improves Query Rewriting for RAG\u003cbr\u003eLearning to Paraphrase for Alignment with LLM Preference\u003cbr\u003e... and 3 more","\u003cb\u003eReranking\u003c\u002fb\u003e\u003cbr\u003eFIRST: Faster Improved Listwise Reranking with Single Token Decoding\u003cbr\u003eFIRST: Faster Improved Listwise Reranking with Single Token Decoding\u003cbr\u003eEnhancing High-order Interaction Awareness in LLM-based Recommender Model\u003cbr\u003ePcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity\u003cbr\u003eSearching for Best Practices in Retrieval-Augmented Generation\u003cbr\u003ePAIRDISTILL: Pairwise Relevance Distillation for Dense Retrieval\u003cbr\u003eComparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval\u003cbr\u003eEchoSight: Advancing Visual-Language Models with Wiki Knowledge\u003cbr\u003eToolken+: Improving LLM Tool Usage with Reranking and a Reject Option\u003cbr\u003e FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking \u003cbr\u003e... and 3 more","\u003cb\u003eStance Detection\u003c\u002fb\u003e\u003cbr\u003eThe Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification\u003cbr\u003eDiversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets\u003cbr\u003eEnhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research\u003cbr\u003eMemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification\u003cbr\u003eI love pineapple on pizza != I hate pineapple on pizza: Stance-Aware Sentence Transformers for Opinion Mining\u003cbr\u003eI love pineapple on pizza != I hate pineapple on pizza: Stance-Aware Sentence Transformers for Opinion Mining\u003cbr\u003eStanceformer: Target-Aware Transformer for Stance Detection\u003cbr\u003eStanceformer: Target-Aware Transformer for Stance Detection\u003cbr\u003eToeing the Party Line: Election Manifestos as a Key to Understand Political Discourse on Twitter\u003cbr\u003eToeing the Party Line: Election Manifestos as a Key to Understand Political Discourse on Twitter\u003cbr\u003e... and 3 more"],"textposition":"none","x":["QA","In-context Learning","Fine-tuning","Natural Language Processing","Zero-shot Learning","RAG","Reasoning","Instruction Tuning","Text Classification","Generalization","Machine Translation","Few-shot Learning","Data Augmentation","Evaluation","Benchmarks","Summarization","Text Generation","Visual QA","Sentiment Analysis","Robustness","Natural Language Inference","Language Modeling","Commonsense Reasoning","Contrastive Learning","Interpretability","Reinforcement Learning","Instruction Following","Code Generation","Information Retrieval","Vision-language Models","Dataset","Benchmarksing","Mathematical Reasoning","Chain-of-thought","Low-resource Languages","Transfer Learning","Alignment","Knowledge Distillation","Hallucination","Parameter-efficient Fine-tuning","Named Entity Recognition","Image Captioning","Explainability","Bias","Synthetic Data","Classification","Multi-task Learning","Direct Preference Optimization","Evaluation Metrics","Multimodal Learning","Efficiency","Fairness","Transformer","Hallucination Detection","Calibration","Cross-lingual Transfer","Multilingual","Relation Extraction","Information Extraction","Pre-training","Domain Adaptation","Knowledge Editing","Reinforcement Learning From Human Feedback","Faithfulness","Human Evaluation","Bias Mitigation","Bias Detection","Reading Comprehension","Natural Language Understanding","LLM Evaluation","Preference Optimization","Model Compression","Logical Reasoning","Speech Recognition","Transformers","Multimodal Llms","Hallucination Mitigation","Quantization","Model Editing","Text Summarization","Hate Speech Detection","Diversity","Consistency","Arithmetic Reasoning","Factuality","Visual Reasoning","Benchmarkss","Social Media","Lora","LLM Alignment","Continual Learning","Bert","Gsm8k","Deep Learning","Preference Learning","Safety","Error Correction","Supervised Fine-tuning","Error Analysis","Knowledge Graphs","Tokenization","Knowledge Base","Multimodal","Graph Neural Networks","Dialogue Generation","Active Learning","Self-supervised Learning","Topic Modeling","Synthetic Data Generation","Open-domain QA","Catastrophic Forgetting","Rag","Adversarial Attack","Video Understanding","Prompt Optimization","Adversarial Attacks","Rlhf","Annotation","Generation","Privacy","Unsupervised Learning","Safety Alignment","Language Model Alignment","Language Understanding","Knowledge Graph","Ranking","Emotion Recognition","Translation","Planning","Low-rank Adaptation","Representation Learning","Dialogue Systems","Event Extraction","Multi-hop Reasoning","Attention Mechanism","Automatic Speech Recognition","Datasets","Text-to-image Generation","Uncertainty","Reward Model","Gender Bias","Math","Retrieval","Pruning","Fact Verification","Medical QA","Clustering","Mixture Of Experts","Long Context","ML","Clip","Question Generation","Toxicity","Fact-checking","Text-to-sql","Semantic Parsing","Vqa","Security","Benchmarks Dataset","Continual Pre-training","Evaluation Metric","Data Generation","Debiasing","Factual Knowledge","Explainable Ai","Multi-modal Learning","Asr","Dense Retrieval","Self-consistency","Prompt Tuning","Math Reasoning","Knowledge Retrieval","Personalization","Knowledge Transfer","Visual Grounding","Pretraining","Topic Classification","Truthfulness","Neural Machine Translation","Temporal Reasoning","Factual Consistency","Transferability","Relevance","Gpt-4","Evaluation Benchmarks","Mechanistic Interpretability","Preference Alignment","Knowledge Representation","Video QA","Multilingual Llms","Generative Models","Grammatical Error Correction","Watermarking","Link Prediction","Mmlu","Perplexity","Meta-evaluation","Dataset Creation","Red Teaming","Error Detection","Large Multimodal Models","Causal Inference","Trustworthiness","Chinese","Scalability","Generalizability","Automatic Evaluation","Downstream Tasks","Semantic Textual Similarity","Attribution","Inference","Misinformation","Self-training","Open-domain Qa","Reranking","Stance Detection"],"y":[268,172,132,125,124,121,111,108,107,102,101,100,97,94,93,88,83,83,82,79,78,78,77,75,73,64,64,64,63,62,62,57,56,52,52,52,52,52,49,48,48,46,42,42,42,41,41,40,40,39,36,35,35,34,34,34,33,33,33,32,32,32,32,31,31,30,30,30,30,30,30,30,28,28,28,28,28,28,28,28,27,26,26,25,25,25,25,24,24,24,24,24,23,22,22,22,22,22,22,22,21,21,21,21,21,20,20,20,20,20,20,20,20,20,19,19,19,19,19,19,19,19,19,18,18,18,18,18,18,18,18,18,18,18,18,18,18,17,17,17,17,17,17,17,17,17,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,14,14,14,14,14,14,14,14,14,14,14,14,14,14,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13],"type":"bar"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"tickfont":{"size":12},"tickmode":"linear","tickangle":-60},"margin":{"l":50,"r":50,"t":80,"b":300},"font":{"size":14},"title":{"text":"EMNLP2024 Keyword"},"yaxis":{"title":{"text":"Number of papers"}},"height":700,"width":3200,"bargap":0.25},                        {"responsive": true}                    )                };            </script>        </div>
    <script>
    const ngramMap = {"QA": ["Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection", "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection", "BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering", "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "Aligning Language Models to Explicitly Handle Ambiguity", "Aligning Language Models to Explicitly Handle Ambiguity", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "PhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study", "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation", "Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism", "Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism", "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation", "Where is the signal in tokenization space?", "Estimating Knowledge in Large Language Models Without Generating a Single Token", "Estimating Knowledge in Large Language Models Without Generating a Single Token", "When Context Leads but Parametric Memory Follows in Large Language Models", "I Could've Asked That: Reformulating Unanswerable Questions", "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs", "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs", "EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation", "Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "Teaching Small Language Models Reasoning through Counterfactual Distillation", "Boosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?", "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation", "Investigating Mysteries of CoT-Augmented Distillation", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "A Survey of AMR Applications", "A Survey of AMR Applications", "Encoding and Controlling Global Semantics for Long-form Video Question Answering", "Does Large Language Model Contain Task-Specific Neurons?", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs", "An Empirical Study of Multilingual Reasoning Distillation for Question Answering", "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "\u201cImage, Tell me your story!\" Predicting the original meta-context of visual misinformation", "\u201cImage, Tell me your story!\" Predicting the original meta-context of visual misinformation", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "Language-to-Code Translation with a Single Labeled Example", "Attribute or Abstain: Large Language Models as Long Document Assistants", "Learning to Correct for QA Reasoning with Black-box LLMs", "Learning to Correct for QA Reasoning with Black-box LLMs", "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation", "Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation", "MixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity", "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering", "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering", "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering", "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMS", "TroL: Traversal of Layers for Large Language and Vision Models", "Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in LLMs", "Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in LLMs", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "Don't Just Say \"I don't know\"! Self-aligning Large Language Models for Responding to Unknown Questions with Explanations", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "Rethinking the Evaluation of In-Context Learning for LLMs", "Rethinking the Evaluation of In-Context Learning for LLMs", "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models", "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "RepMatch: Quantifying Cross-Instance Similarities in Representation Space", "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "LONGAGENT: Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration", "Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "LLOCO: Learning Long Contexts Offline", "Searching for Best Practices in Retrieval-Augmented Generation", "Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition", "Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition", "Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks", "A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks", "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering", "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering", "Generation with Dynamic Vocabulary", "Generation with Dynamic Vocabulary", "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture", "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "Encoding Spreadsheets for Large Language Models", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "QuBE: Question-based Belief Enhancement for Agentic LLM Reasoning", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA", "Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "Can LLMs Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner", "Can LLMs Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "Measuring the Robustness of NLP Models to Domain Shifts", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity", "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment", "RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment", "API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access", "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models", "Large Language Model-based Human-Agent Collaboration for Complex Task Solving", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models", "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models", "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering", "AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses", "AHP-Powered LLM Reasoning for Multi-Criteria Evaluation of Open-Ended Responses", "Abstraction-of-Thought Makes Language Models Better Reasoners", "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Finance Communities in Long Form Question Answering", "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Finance Communities in Long Form Question Answering", "Are LLMs Aware that Some Questions are not Open-ended?", "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature", "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature", "INTENTIONQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce", "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning", "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "BOOLQUESTIONS: Does Dense Retrieval Understand Boolean Logic in Language?", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering", "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations", "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations", "A Notion of Complexity for Theory of Mind via Discrete World Models", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "HoneyComb: A Flexible LLM-Based Agent System for Materials Science", "Characterizing LLM Abstention Behavior in Science QA with Context Perturbations", "Characterizing LLM Abstention Behavior in Science QA with Context Perturbations", "Language Models Still Struggle to Zero-shot Reason about Time Series", "Language Models Still Struggle to Zero-shot Reason about Time Series", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "Self-Contradictory Reasoning Evaluation and Detection", "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension", "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension", "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "RoQLlama: A Lightweight Romanian Adapted Language Model", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "A Survey on Natural Language Counterfactual Generation", "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism", "Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs", "Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "M2QA: Multi-domain Multilingual Question Answering", "M2QA: Multi-domain Multilingual Question Answering", "LumberChunker: Long-Form Narrative Document Segmentation", "LumberChunker: Long-Form Narrative Document Segmentation", "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs", "Learning Semantic Structure through First-Order-Logic Translation", "Learning Semantic Structure through First-Order-Logic Translation", "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "In-Context Learning with Iterative Demonstration Selection", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation", "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation", "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "MedINST: Meta Dataset of Biomedical Instructions", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "CHIRON: Rich Character Representations in Long-Form Narratives", "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Datasets for Multilingual Answer Sentence Selection", "Datasets for Multilingual Answer Sentence Selection", "LoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "Exploring Hint Generation Approaches in Open-Domain Question Answering", "Detecting Temporal Ambiguity in Questions", "Detecting Temporal Ambiguity in Questions", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice", "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice", "Query Routing for Homogeneous Tools: An Instantiation in the RAG Scenario", "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation", "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation", "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation", "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "MDCR: A Dataset for Multi-Document Conditional Reasoning", "MDCR: A Dataset for Multi-Document Conditional Reasoning", "A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval", "R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation", "CSLM: A Framework for Question Answering Dataset Generation through Collaborative Small Language Models", "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity", "Cross-Lingual Multi-Hop Knowledge Editing", "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning", "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning", "Dual-Phase Accelerated Prompt Optimization", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "Can't Remember Details in Long Documents? You Need Some R&R", "Can't Remember Details in Long Documents? You Need Some R&R", "More Bang for your Context: Virtual Documents for Question Answering over Long Documents", "Synthetic Multimodal Question Generation", "Synthetic Multimodal Question Generation", "Calibrating Long-form Generations from Large Language Models", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables", "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models", "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "XC-CACHE: Cross-Attending to Cached Context for Efficient LLM Inference", "XC-CACHE: Cross-Attending to Cached Context for Efficient LLM Inference", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "M3SCIQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language", "SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering", "Extrinsic Evaluation of Cultural Competence in Large Language Models", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "Zero-Shot Fact Verification via Natural Logic and Large Language Models", "Zero-Shot Fact Verification via Natural Logic and Large Language Models"], "In-context Learning": ["Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices", "FLIRT: Feedback Loop In-context Red Teaming", "In-context Contrastive Learning for Event Causality Identification", "What's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs", "A Survey on In-context Learning", "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "STANDARDIZE: Aligning Language Models with Expert-Defined Standards for Content Generation", "Enhancing Advanced Visual Reasoning Ability of Large Language Models", "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting", "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning", "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning", "Interpretability-based Tailored Knowledge Editing in Transformers", "Interpretability-based Tailored Knowledge Editing in Transformers", "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing", "Aligning Translation-Specific Understanding to General Understanding in Large Language Models", "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems", "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems", "Focused Large Language Models are Stable Many-Shot Learners", "Focused Large Language Models are Stable Many-Shot Learners", "Revealing the Parallel Multilingual Learning within Large Language Models", "Does Large Language Model Contain Task-Specific Neurons?", "Learning to Retrieve Iteratively for In-Context Learning", "Learning to Retrieve Iteratively for In-Context Learning", "Position Engineering: Boosting Large Language Models through Positional Information Manipulation", "Position Engineering: Boosting Large Language Models through Positional Information Manipulation", "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning", "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "Language-to-Code Translation with a Single Labeled Example", "Retrieved In-Context Principles from Previous Mistakes", "Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias", "Efficient Sequential Decision Making with Large Language Models", "Ontologically Faithful Generation of Non-Player Character Dialogues", "Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives", "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation", "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation", "On the In-context Generation of Language Models", "When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Self-AMPLIFY : Improving Small Language Models with Self Post Hoc Explanations", "Self-AMPLIFY : Improving Small Language Models with Self Post Hoc Explanations", "ONE2SET + Large Language Model: Best Partners for Keyphrase Generation", "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMS", "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "Reasoning or a Semblance of it? A Diagnostic Study of Transitive Reasoning in LLMs", "What Are the Odds? Language Models Are Capable of Probabilistic Reasoning", "\"A good pun is its own reword\": Can Large Language Models Understand Puns?", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent", "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach", "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach", "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences", "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences", "Rethinking the Evaluation of In-Context Learning for LLMs", "Tools Fail: Detecting Silent Errors in Faulty Tools", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "SecCoder: Towards Generalizable and Robust Secure Code Generation", "LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law", "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models", "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?", "Exploring the Learning Capabilities of Language Models using LEVERWORLDS", "Generating Demonstrations for In-Context Compositional Generalization in Grounded Language Learning", "Generating Demonstrations for In-Context Compositional Generalization in Grounded Language Learning", "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!", "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!", "Representational Analysis of Binding in Language Models", "Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition", "In-Context Compositional Generalization for Large Vision-Language Models", "Tree of Problems: Improving structured problem solving with compositionality", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement", "Semformer: Transformer Language Models with Semantic Planning", "Semformer: Transformer Language Models with Semantic Planning", "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "Defending Jailbreak Prompts via In-Context Adversarial Game", "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Thoughts to Target: Enhance Planning for Target-driven Conversation", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "Measuring the Robustness of NLP Models to Domain Shifts", "Few shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering", "Categorial Grammar Supertagging via Large Language Models", "SPECIALEX: A Benchmark for In-Context Specialized Lexicon Learning", "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL", "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL", "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning", "Diversity, Rationalize, and Combine: Ensembling Multiple QA Strategies for Zero-shot Knowledge-based VQA", "Abstraction-of-Thought Makes Language Models Better Reasoners", "Are Large Language Models (LLMs) Good Social Predictors?", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "On the Empirical Complexity of Reasoning and Planning in LLMs", "Large Language Models are Limited in Out-of-Context Knowledge Reasoning", "Scaling Sentence Embeddings with Large Language Models", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming", "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning", "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning", "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning", "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents", "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models", "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning", "Can we teach language models to gloss endangered languages?", " FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking ", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "Improving Referring Ability for Biomedical Language Models", "Exploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions", "Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement", "Private prediction for large-scale synthetic text generation", "Demonstration Selection Strategies for Numerical Time Series Data-to-Text", "In-Context Learning with Iterative Demonstration Selection", "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues", "The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment", "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment", "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "TUTOR-ICL: Guiding Large Language Models for Improved In-Context Learning Performance", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "C-ICL: Contrastive In-context Learning for Information Extraction", "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs.", "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models", "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability", "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability", "MAVEN-FACT: A Large-scale Event Factuality Detection Dataset", "Revisiting the Impact of Pursuing Modularity for Code Generation", "Data-Centric AI in the Age of Large Language Models", "Augmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "What Matters in Memorizing and Recalling Facts? Multifaceted Benchmarks for Knowledge Probing in Language Models", "Skills-in-Context: Unlocking Compositionality in Large Language Models", "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers", "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks", "Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "Analyzing Context Contributions in LLM-based Machine Translation", "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models", "LAMBDA: Large Language Model-Based Data Augmentation for Multi-Modal Machine Translation", "Large Language Models Know What To Say But Not When to Speak", "SPINACH: SPARQL-Based Information Navigation for Challenging Real-World Questions", "Inference and Verbalization Functions During In-Context Learning", "Large Language Models are In-context Teachers for Knowledge Reasoning", "Large Language Models are In-context Teachers for Knowledge Reasoning", "NARRATIVE-OF-THOUGHT: Improving Temporal Reasoning of Large Language Models via Recounted Narratives", "Personalized Video Comment Generation"], "Fine-tuning": ["Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "Systematic Biases in LLM Simulations of Debates", "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?", "On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "Model Balancing Helps Low-data Training and Fine-tuning", "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "Dissecting Fine-Tuning Unlearning in Large Language Models", "Estimating Knowledge in Large Language Models Without Generating a Single Token", "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach", "Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge", "Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System", "Do Large Language Models Know How Much They Know?", "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?", "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "Fast Forwarding Low-Rank Training", "Evaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding", "Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension", "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions", "Breaking ReLU Barrier: Generalized MoEfication for Dense Pretrained Models", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "\u201cIn Dialogues We Learn\u201d: Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning", "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together", "Puzzle Solving using Reasoning of Large Language Models: A Survey", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent", "Domain adapted machine translation: What does catastrophic forgetting forget and why?", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models", "An Analysis and Mitigation of the Reversal Curse", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction", "Large Language Models Can Be Contextual Privacy Protection Learners", "Large Language Models Can Be Contextual Privacy Protection Learners", "MIXTURE-OF-SKILLS: Learning to Optimize Data Usage for Fine-Tuning Large Language Models", "MIXTURE-OF-SKILLS: Learning to Optimize Data Usage for Fine-Tuning Large Language Models", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models", "GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients", "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions", "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?", "Revisiting Supervised Contrastive Learning for Microblog Classification", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups", "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "User Inference Attacks on Large Language Models", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation", "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs", "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "I love pineapple on pizza != I hate pineapple on pizza: Stance-Aware Sentence Transformers for Opinion Mining", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets", "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation", "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering", "Measuring the Robustness of NLP Models to Domain Shifts", "TRANSLLAMA: LLM-based Simultaneous Translation System", "Categorial Grammar Supertagging via Large Language Models", "RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment", "Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "Visual Question Decomposition on Multimodal Large Language Models", "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Finance Communities in Long Form Question Answering", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "Enhancing Polyglot Voices by Leveraging Cross-Lingual Fine-Tuning in Any-to-One Voice Conversion", "On the Empirical Complexity of Reasoning and Planning in LLMs", "Scaling Sentence Embeddings with Large Language Models", "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning", "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies", "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach", "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning", "AdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning", "Private prediction for large-scale synthetic text generation", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Event-Keyed Summarization", "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "Step-level Value Preference Optimization for Mathematical Reasoning", "LaRA: Large Rank Adaptation for Speech and Text Cross-Modal Learning in Large Language Models", "DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models", "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "Can LLMs Reason in the Wild with Programs?", "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases", "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning", "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models", "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation", "ALANAVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding", "MAVEN-FACT: A Large-scale Event Factuality Detection Dataset", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "Revisiting the Impact of Pursuing Modularity for Code Generation", "SODA-EVAL: Open-Domain Dialogue Evaluation in the age of LLMs", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "Data-Centric AI in the Age of Large Language Models", "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning", "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "To Err Is Human, but Llamas Can Learn It Too", "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language Story Understanding", "QEFT: Quantization for Efficient Fine-Tuning of LLMs", "QEFT: Quantization for Efficient Fine-Tuning of LLMs", "Skills-in-Context: Unlocking Compositionality in Large Language Models", "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs", "LexMatcher: Dictionary-centric Data Curation for LLM-based Machine Translation", "Prospector: Improving LLM Agents with Self-Asking and Trajectory Ranking", "Remember This Event That Year? Assessing Temporal Information and Understanding in Large Language Models", "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "Reasoning Paths Optimization: Learning to Reason and Explore From Diverse Paths", "Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging", "Stochastic Fine-Tuning of Language Models Using Masked Gradients"], "Natural Language Processing": ["COCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds", "A Survey on In-context Learning", "A Survey on In-context Learning", "Model Balancing Helps Low-data Training and Fine-tuning", "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese", "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "Evaluating Large Language Models via Linguistic Profiling", "GOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory", "Fine-Grained Prediction of Reading Comprehension from Eye Movements", "A Generic Method for Fine-grained Category Discovery in Natural Language Texts", "A Generic Method for Fine-grained Category Discovery in Natural Language Texts", "Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method", "C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits", "Private Language Models via Truncated Laplacian Mechanism", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "Methods for Automatic Matrix Language Determination of Code-Switched Speech", "Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "A Survey of AMR Applications", "CareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation", "Knowledge-Centric Hallucination Detection", "Leveraging pre-trained language models for linguistic analysis: A case of argument structure constructions", "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples", "Perceptions of Linguistic Uncertainty by Language Models and Humans", "MEANT: Multimodal Encoder for Antecedent Information", "Statistical Uncertainty in Word Embeddings: GloVe-V", "Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models", "Toward Compositional Behavior in Neural Models: A Survey of Current Views", "Toward Compositional Behavior in Neural Models: A Survey of Current Views", "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text", "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic", "When Generative Adversarial Networks Meet Sequence Labeling Challenges", "Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "Red Teaming Language Models for Processing Contradictory Dialogues", "Pragmatic Norms Are All You Need \u2013 Why The Symbol Grounding Problem Does Not Apply to LLMs", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "ABSEval: An Agent-based Framework for Script Evaluation", "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models", "\"Global is Good, Local is Bad?\u201d: Understanding Brand Bias in LLMs", "Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models", "OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants", "ANALOBENCH: Benchmarking the Identification of Abstract and Long-context Analogies", "Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination", "Related Work and Citation Text Generation: A Survey", "Large Language Models Can Be Contextual Privacy Protection Learners", "Are LLMs Good Zero-Shot Fallacy Classifiers?", "Building Resources for Emakhuwa: Machine Translation and News Classification Benchmarks", "A Closer Look at Multidimensional Online Political Incivility", "HalluMeasure: Fine-grained Hallucination Measurement Using Chain-of-Thought Reasoning", "ADAPTIVE AXES: A Pipeline for In-domain Social Stereotype Analysis", "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "Worry Words: Norms of Anxiety Association for over 44k English Words", "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments", "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions", "Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "Calibrating Language Models with Adaptive Temperature Scaling", "Calibrating Language Models with Adaptive Temperature Scaling", "A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries", "Comparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media", "Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health", "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks", "MIPD: Exploring Manipulation and Intention in A Novel Corpus of Polish Disinformation", "Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs", "Which questions should I answer? Salience Prediction of Inquisitive Questions", "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions", "Let's discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference", "InsertGNN: A Hierarchical Graph Neural Network for the TOEFL Sentence Insertion Problem", "Unleashing Large Language Models' Proficiency in Zero-shot Essay Scoring", "Temporal Cognitive Tree: A Hierarchical Modeling Approach for Event Temporal Relation Extraction", "Understanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "Dealing with Controversy: An Emotion and Coping Strategy Corpus Based on Role Playing", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "Multi-dimensional Evaluation of Empathetic Dialogue Responses", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check", "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models", "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models", "RoQLlama: A Lightweight Romanian Adapted Language Model", "Rethinking Evaluation Methods for Machine Unlearning", "A Survey on Natural Language Counterfactual Generation", "Counter Turing Test (CT2): Investigating AI-Generated Text Detection for Hindi - Ranking LLMs based on Hindi AI Detectability Index (ADIhi)", "\"I Never Said That\": A dataset, taxonomy and baselines on response clarity classification", "Merely Judging Metaphor is Not Enough: Research on Reasonable Metaphor Detection", "Enhancing Byzantine-Resistant Aggregations with Client Embedding", "Nebula: A Discourse-Aware Minecraft Builder", "Crisis counselor language and perceived genuine concern in crisis conversations", "Crisis counselor language and perceived genuine concern in crisis conversations", "Large Language Models are Students at Various Levels: Zero-shot Question Difficulty Estimation", "Large Language Models are Students at Various Levels: Zero-shot Question Difficulty Estimation", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "Taking a turn for the better: Conversation redirection throughout the course of mental-health therapy", "A Survey on Open Information Extraction from Rule-based Model to Large Language Model", "DisGeM: Distractor Generation for Multiple Choice Questions with Span Masking", "Do they mean 'us'? Interpreting Referring Expression variation under Intergroup Bias", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "Towards Effective Counter-Responses: Aligning Human Preferences with Strategies to Combat Online Trolling", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Resilience of Large Language Models for Noisy Instructions", "An LLM-Enabled Knowledge Elicitation and Retrieval Framework for Zero-Shot Cross-Lingual Stance Identification", "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation", "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch", "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI", "Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition", "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation", "M3SCIQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation", "Locally Measuring Cross-lingual Lexical Alignment: A Domain and Word Level Perspective", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations"], "Zero-shot Learning": ["UNIGEN: Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation", "Table Question Answering for Low-resourced Indic Languages", "LLMs Are Zero-Shot Context-Aware Simultaneous Translators", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment", "Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement", "PsFuture: A Pseudo-Future-based Zero-Shot Adaptive Policy for Simultaneous Machine Translation", "TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control", "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "Evaluating Large Language Models via Linguistic Profiling", "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection", "Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages", "Incubating Text Classifiers Following User Instruction with Nothing but LLM", "Conditional and Modal Reasoning in Large Language Models", "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval", "Visual Prompting in LLMs for Enhancing Emotion Recognition", "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing", "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "ADELIE: Aligning Large Language Models on Information Extraction", "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "Zero-shot Cross-domain Dialogue State Tracking via Context-aware Auto-prompting and Instruction-following Contrastive Decoding", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "MixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity", "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering", "Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets", "Do LLMs suffer from Multi-Party Hangover? A Diagnostic Approach to Addressee Recognition and Response Selection in Conversations", "What Are the Odds? Language Models Are Capable of Probabilistic Reasoning", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness", "Style-Specific Neurons for Steering LLMs in Text Style Transfer", "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction", "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages", "Topic-Oriented Open Relation Extraction with A Priori Seed Generation", "Are LLMs Good Zero-Shot Fallacy Classifiers?", "Open-world Multi-label Text Classification with Extremely Weak Supervision", "LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law", "DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding", "Hate Personified: Investigating the role of LLMs in content moderation", "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "PALM: Few-Shot Prompt Learning for Audio Language Models", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization", "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "GRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization", "Altogether: Image Captioning via Re-aligning Alt-text", "Multilingual Topic Classification in X: Dataset and Analysis", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "Retrieval-enriched zero-shot image classification in low-resource domains", "Retrieval-enriched zero-shot image classification in low-resource domains", "Show and Guide: Instructional-Plan Grounded Vision and Language Model", "A Simple LLM Framework for Long-Range Video Question-Answering", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models", "Deciphering Cognitive Distortions in Patient-Doctor Mental Health Conversations: A Multimodal LLM-Based Detection and Reasoning Framework", "Transferability of Syntax-Aware Graph Neural Networks in Zero-Shot Cross-Lingual Semantic Role Labeling", "Text2Model: Text-based Model Induction for Zero-shot Image Classification", "Unleashing Large Language Models' Proficiency in Zero-shot Essay Scoring", "Can Large Language Models Identify Authorship?", "MalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language", "Diversity, Rationalize, and Combine: Ensembling Multiple QA Strategies for Zero-shot Knowledge-based VQA", "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models", "Exploring the Best Practices of Query Expansion with Large Language Models", "OpenGraph: Towards Open Graph Foundation Models", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "Language Models Still Struggle to Zero-shot Reason about Time Series", "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?", "Instance-Level Dynamic LoRAs Composition for Cross-Task Generalization", "Instance-Level Dynamic LoRAs Composition for Cross-Task Generalization", "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs", "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs", "LongForm: Effective Instruction Tuning with Reverse Instructions", "Large Language Models are Students at Various Levels: Zero-shot Question Difficulty Estimation", "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment", "Exploring Hint Generation Approaches in Open-Domain Question Answering", "Navigating Hallucinations for Reasoning of Unintentional Activities", "Navigating Hallucinations for Reasoning of Unintentional Activities", "Pruning Multilingual Large Language Models for Multilingual Inference", "VPL: Visual Proxy Learning Framework for Zero-Shot Medical Image Diagnosis", "Learning to Plan by Updating Natural Language", "Functionality learning through specification instructions", "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "Zero-shot Commonsense Reasoning over Machine Imagination", "Zero-shot Commonsense Reasoning over Machine Imagination", "SODA-EVAL: Open-Domain Dialogue Evaluation in the age of LLMs", "SAFARI: Cross-lingual Bias and Factuality Detection in News Media and News Articles", "An LLM-Enabled Knowledge Elicitation and Retrieval Framework for Zero-Shot Cross-Lingual Stance Identification", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "Lost in Translation: Chemical Language Models and the Misunderstanding of Molecule Structures", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "Large Language Models for Propaganda Span Annotation", "Identifying Factual Inconsistencies in Summaries: Grounding LLM Inference via Task Taxonomy", "Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition", "Multi-trait User Simulation with Adaptive Decoding for Conversational Task Assistants", "Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents", "Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering", "Zero-Shot Fact Verification via Natural Logic and Large Language Models"], "RAG": ["Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering", "Towards Low-Resource Harmful Meme Detection with LMM Agents", "Knowledge Verification to Nip Hallucination in the Bud", "SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation", "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering", "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering", "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering", "An Analysis of Multilingual FActScore", "DVD: Dynamic Contrastive Decoding for Knowledge Amplification in Multi-Document Question Answering", "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs", "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs", "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering", "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA", "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation", "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation", "Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation", "SATYRN: A Platform for Analytics Augmented Generation", "TimeR4: Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering", "Position Engineering: Boosting Large Language Models through Positional Information Manipulation", "Position Engineering: Boosting Large Language Models through Positional Information Manipulation", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "\u201cImage, Tell me your story!\" Predicting the original meta-context of visual misinformation", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "Attribute or Abstain: Large Language Models as Long Document Assistants", "AGRAME: Any-Granularity Ranking with Multi-Vector Embeddings", "AGRAME: Any-Granularity Ranking with Multi-Vector Embeddings", "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "From RAG to RICHES: Retrieval Interlaced with Sequence Generation", "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "Assessing \"Implicit\" Retrieval Robustness of Large Language Models", "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction", "MixGR: Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity", "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR", "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR", "Improve Dense Passage Retrieval with Entailment Tuning", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent", "Defending Against Social Engineering Attacks in the Age of LLMs", "Analysis of Plan-based Retrieval for Grounded Text Generation", "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "Free your mouse! Command Large Language Models to Generate Code to Format Word Documents", "Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs", "Multi-Level Information Retrieval Augmented Generation for Knowledge-based Visual Question Answering", "Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models", "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "LLOCO: Learning Long Contexts Offline", "Can Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction", "Searching for Best Practices in Retrieval-Augmented Generation", "Searching for Best Practices in Retrieval-Augmented Generation", "Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs", "Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs", "Improving Zero-shot LLM Re-Ranker with Risk Minimization", "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "QuBE: Question-based Belief Enhancement for Agentic LLM Reasoning", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "RAR: Retrieval-augmented retrieval for code generation in low-resource languages", "RAR: Retrieval-augmented retrieval for code generation in low-resource languages", "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation", "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering", "RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation", "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering", "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature", "Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework", "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts", "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts", "EvOR: Evolving Retrieval for Code Generation", "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs", "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models", "BOOKWORM: A Dataset for Character Description and Analysis", "LONG2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall", "Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models", "TRIAGEAGENT: Towards Better Multi-Agents Collaborations for Large Language Model-Based Clinical Triage", "LumberChunker: Long-Form Narrative Document Segmentation", "LumberChunker: Long-Form Narrative Document Segmentation", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs", "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice", "Query Routing for Homogeneous Tools: An Instantiation in the RAG Scenario", "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation", "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation", "Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA", "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft", "R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation", "R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation", "\"Knowing When You Don't Know\u201d: A Multilingual Relevance Assessment Dataset for Robust Retrieval-Augmented Generation", "Can We Instruct LLMs to Compensate for Position Bias?", "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP", "Can't Remember Details in Long Documents? You Need Some R&R", "Dense Passage Retrieval: Is it Retrieving?", "LLMs as Collaborator: Demands-Guided Collaborative Retrieval-Augmented Generation for Commonsense Knowledge-Grounded Open-Domain Dialogue Systems", "ConTReGen: Context-driven Tree-structured Retrieval for Open-domain Long-form Text Generation", "OPEN-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models", "Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision", "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models", "Mitigating Hallucination in Fictional Character Role-Play", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation", "Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA", "LLM generated responses to mitigate the impact of hate speech", "Unified Active Retrieval for Retrieval Augmented Generation"], "Reasoning": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification", "PhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study", "Teaching LLMs to Abstain across Languages via Multilingual Feedback", "When Reasoning Meets Information Aggregation: A Case Study with Sports Narratives", "Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization", "Multi-Granularity History and Entity Similarity Learning for Temporal Knowledge Graph Reasoning", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models", "Boosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?", "Revealing the Parallel Multilingual Learning within Large Language Models", "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts", "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons", "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs", "Retrieved In-Context Principles from Previous Mistakes", "Learning to Correct for QA Reasoning with Black-box LLMs", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Belief Revision: The Adaptability of Large Language Models Reasoning", "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA", "Self-AMPLIFY : Improving Small Language Models with Self Post Hoc Explanations", "DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting", "Puzzle Solving using Reasoning of Large Language Models: A Survey", "Subjective Topic meets LLMs: Unleashing Comprehensive, Reflective and Creative Thinking through the Negation of Negation", "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently", "Large Language Models Can Self-Correct with Key Condition Verification", "ANALOBENCH: Benchmarking the Identification of Abstract and Long-context Analogies", "FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition", "An Analysis and Mitigation of the Reversal Curse", "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning", "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "Re-Reading Improves Reasoning in Large Language Models", "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Working Memory Identifies Reasoning Limits in Language Models", "One Thousand and One Pairs: A \u201cnovel\u201d challenge for long-context language models", "One Thousand and One Pairs: A \u201cnovel\u201d challenge for long-context language models", "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering", "CAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity", "Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "Abstraction-of-Thought Makes Language Models Better Reasoners", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study", "Head-wise Shareable Attention for Large Language Models", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "On the Empirical Complexity of Reasoning and Planning in LLMs", "On the Empirical Complexity of Reasoning and Planning in LLMs", "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning?", "Language Models Still Struggle to Zero-shot Reason about Time Series", "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization", "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning", "Self-Contradictory Reasoning Evaluation and Detection", "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models", "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models", "Merely Judging Metaphor is Not Enough: Research on Reasonable Metaphor Detection", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate", "Improving Multi-Agent Debate with Sparse Communication Topology", "Improving Multi-Agent Debate with Sparse Communication Topology", "In-Context Learning with Iterative Demonstration Selection", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "Towards Benchmarking Situational Awareness of Large Language Models: Comprehensive Benchmark, Evaluation and Analysis", "Weak-to-Strong Reasoning", "Weak-to-Strong Reasoning", "Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping", "Natural Evolution-based Dual-Level Aggregation for Temporal Knowledge Graph Reasoning", "Navigating Hallucinations for Reasoning of Unintentional Activities", "Navigating Hallucinations for Reasoning of Unintentional Activities", "Can LLMs Reason in the Wild with Programs?", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "Learning to Plan by Updating Natural Language", "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication", "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models", "Enhancing Emotion-Cause Pair Extraction in Conversations via Center Event Detection and Reasoning", "Will LLMs Sink or Swim? Exploring Decision-Making Under Pressure", "ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering", "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner for Insightful Table Summarization", "Large Language Models Are Challenged by Habitat-Centered Reasoning", "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework", "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework", "Skills-in-Context: Unlocking Compositionality in Large Language Models", "OPEN-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models", "M3SCIQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction", "Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning", "Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning", "Explicit Inductive Inference using Large Language Models", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "SOCIALGAZE: Improving the Integration of Human Social Norms in Large Language Models", "Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness"], "Instruction Tuning": ["Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation", "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "LLMs Are Zero-Shot Context-Aware Simultaneous Translators", "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval", "QUDSELECT: Selective Decoding for Questions Under Discussion Parsing", "Towards Tool Use Alignment of Large Language Models", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "Knowledge Verification to Nip Hallucination in the Bud", "GOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory", "M\u00b2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning", "Incubating Text Classifiers Following User Instruction with Nothing but LLM", "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters", "VIMI: Grounding Video Generation through Multi-modal Instruction", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "Automatic Instruction Evolving for Large Language Models", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "ADELIE: Aligning Large Language Models on Information Extraction", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "Zero-shot Cross-domain Dialogue State Tracking via Context-aware Auto-prompting and Instruction-following Contrastive Decoding", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing", "From RAG to RICHES: Retrieval Interlaced with Sequence Generation", "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs", "Fast Forwarding Low-Rank Training", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions", "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions", "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG", "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback", "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "Learning Personalized Alignment in Evaluating Open-ended Text Generation", "Curriculum Consistency Learning for Conditional Sentence Generation", "Curriculum Consistency Learning for Conditional Sentence Generation", "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with Really Good Data", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Large Language Models Can Be Contextual Privacy Protection Learners", "DC-Instruct: An Effective Framework for Generative Multi-intent Spoken Language Understanding", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Control Large Language Models via Divide and Conquer", "Temporally Consistent Factuality Probing for Large Language Models", "COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities", "How Susceptible are Large Language Models to Ideological Manipulation?", "How Susceptible are Large Language Models to Ideological Manipulation?", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "Which questions should I answer? Salience Prediction of Inquisitive Questions", "DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing", "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?", "Can LLMs Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner", "Adversarial Text Generation using Large Language Models for Dementia Detection", "XRec: Large Language Models for Explainable Recommendation", "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization", "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search", "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search", "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Data Diversity Matters for Robust Instruction Tuning", "Data Diversity Matters for Robust Instruction Tuning", "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models", "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs", "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models", "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context", "Improving Argument Effectiveness Across Ideologies using Instruction-tuned Large Language Models", "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach", "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach", "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning", "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning", "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization", "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations", "LongForm: Effective Instruction Tuning with Reverse Instructions", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "An Instruction Tuning-Based Contrastive Learning Framework for Aspect Sentiment Quad Prediction with Implicit Aspects and Opinions", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "Achieving Stronger Generation via Simple Contrastive Tuning", "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "Enhancing Discourse Dependency Parsing with Sentence Dependency Parsing: A Unified Generative Method Based on Code Representation", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "Better Alignment with Instruction Back-and-Forth Translation", "Better Alignment with Instruction Back-and-Forth Translation", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "TextLap: Customizing Language Models for Text-to-Layout Planning", "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks", "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging", "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging", "The Overlooked Repetitive Lengthening Form in Sentiment Analysis", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "BiMediX: Bilingual Medical Mixture of Experts LLM", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning"], "Text Classification": ["On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices", "On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices", "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification", "A Generic Method for Fine-grained Category Discovery in Natural Language Texts", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification", "Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification", "Incubating Text Classifiers Following User Instruction with Nothing but LLM", "Incubating Text Classifiers Following User Instruction with Nothing but LLM", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse", "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates", "PERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts", "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction", "Towards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs", "Towards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs", "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "A Bayesian Approach to Harnessing the Power of LLMs in Authorship Attribution", "Rethinking the Evaluation of In-Context Learning for LLMs", "Rethinking the Evaluation of In-Context Learning for LLMs", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Quantum Recurrent Architectures for Text Classification", "Quantum Recurrent Architectures for Text Classification", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "Detecting Online Community Practices with Large Language Models: A Case Study of Pro-Ukrainian Publics on Twitter", "AMPO: Automatic Multi-Branched Prompt Optimization", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Linear Layer Extrapolation for Fine-Grained Emotion Classification", "A Morphology-Based Investigation of Positional Encodings", "Bias Wipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability", "ADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers", "ADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers", "GottBERT: a pure German Language Model", "GottBERT: a pure German Language Model", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "Adversarial Text Generation using Large Language Models for Dementia Detection", "On the Fragility of Active Learners for Text Classification", "On the Fragility of Active Learners for Text Classification", " 'Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated Peer Reviews ", "Improve Meta-learning for Few-Shot Text Classification with All You Can Acquire from the Tasks", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "Transfer Learning for Text Classification via Model Risk Analysis", "Transfer Learning for Text Classification via Model Risk Analysis", "LoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "Rethinking Evaluation Methods for Machine Unlearning", "Rethinking Evaluation Methods for Machine Unlearning", "A Survey on Natural Language Counterfactual Generation", "Enhancing Byzantine-Resistant Aggregations with Client Embedding", "Enhancing Byzantine-Resistant Aggregations with Client Embedding", "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models", "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference", "Multi-Loss Fusion: Angular and Contrastive Integration for Machine-Generated Text Detection", "On the Generalization of Training-based ChatGPT Detection Methods", "PE: A Poincare Explanation Method for Fast Text Hierarchy Generation", "MedINST: Meta Dataset of Biomedical Instructions", "Unlocking the Potential of Model Merging for Low-Resource Languages", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "CERT-ED: Certifiably Robust Text Classification for Edit Distance", "CERT-ED: Certifiably Robust Text Classification for Edit Distance", "Mental Disorder Classification via Temporal Representation of Text", "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation", "Logits Reranking via Semantic Labels for Hard Samples in Text Classification", "Logits Reranking via Semantic Labels for Hard Samples in Text Classification", "Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment", "Distance-aware Calibration for Pre-trained Language Models", "Distance-aware Calibration for Pre-trained Language Models", "Textual Dataset Distillation via Language Model Embedding", "Textual Dataset Distillation via Language Model Embedding", "Robust Text Classification: Analyzing Prototype-Based Networks", "Robust Text Classification: Analyzing Prototype-Based Networks", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI", "Random Label Forests: An Ensemble Method with Label Subsampling For Extreme Multi-Label Problems", "Efficient Active Learning with Adapters", "CED: Comparing Embedding Differences for Detecting Out-of-Distribution and Hallucinated Text", "CED: Comparing Embedding Differences for Detecting Out-of-Distribution and Hallucinated Text", "Characterizing Text Datasets with Psycholinguistic Features", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information"], "Generalization": ["Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS", "Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization", "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval", "MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use", "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "Cross-Domain Audio Deepfake Detection: Dataset and Analysis", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models", "Learning from Natural Language Explanations for Generalizable Entity Matching", "Personas as a Way to Model Truthfulness in Language Models", "Can Large Language Models Learn Independent Causal Mechanisms?", "Learning to Retrieve Iteratively for In-Context Learning", "Morpheus: Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "CommonIT: Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions", "Breaking ReLU Barrier: Generalized MoEfication for Dense Pretrained Models", "Demystifying Verbatim Memorization in Large Language Models", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "Subword Segmentation in LLMs: Looking at Inflection and Consistency", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", ">Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "Preserving Generalization of Language Models in Few-shot Continual Relation Extraction", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "Data Contamination Can Cross Language Barriers", "Data Contamination Can Cross Language Barriers", "On the Universal Truthfulness Hyperplane Inside LLMS", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks", "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "Can Language Models Induce Grammatical Knowledge from Indirect Evidence?", "PREDICT: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection", "PREDICT: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection", "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing", "Knowledge Graph Enhanced Large Language Model Editing", "Knowledge Graph Enhanced Large Language Model Editing", "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology", "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?", "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?", "OpenGraph: Towards Open Graph Foundation Models", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "Rethinking Evaluation Methods for Machine Unlearning", "TOOLVERIFIER: Generalization to New Tools via Self-Verification", "Immunization against harmful fine-tuning attacks", "Merely Judging Metaphor is Not Enough: Research on Reasonable Metaphor Detection", "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models", "Learning Semantic Structure through First-Order-Logic Translation", "A Training Data Recipe to Accelerate A* Search with Large Language Models\\", "On the Generalization of Training-based ChatGPT Detection Methods", "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness", "Achieving Stronger Generation via Simple Contrastive Tuning", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "Monotonic Paraphrasing Improves Generalization of Language Model Prompting", "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "Functionality learning through specification instructions", "Scaling Laws for Fact Memorization of Large Language Models", "Zero-shot Commonsense Reasoning over Machine Imagination", "Predicting generalization performance with correctness discriminators", "Cross-Lingual Multi-Hop Knowledge Editing", "Towards Robust Evaluation of Unlearning in LLMs via Data Transformations", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion", "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains", "Towards One-to-Many Visual Question Answering", "Stochastic Fine-Tuning of Language Models Using Masked Gradients"], "Machine Translation": ["Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?", "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?", "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models", "LLMs Are Zero-Shot Context-Aware Simultaneous Translators", "Word Alignment as Preference for Machine Translation", "Word Alignment as Preference for Machine Translation", "MiTTenS: A Dataset for Evaluating Gender Mistranslation", "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects", "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects", "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing", "Aligning Translation-Specific Understanding to General Understanding in Large Language Models", "Aligning Translation-Specific Understanding to General Understanding in Large Language Models", "Beyond Reference: Evaluating High Quality Translations Better than Human References", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Pretraining Language Models Using Translationese", "Reconsidering Sentence-Level Sign Language Translation", "An Audit on the Perspectives and Challenges of Hallucinations in NLP", "A Survey of AMR Applications", "A Survey of AMR Applications", "Revealing the Parallel Multilingual Learning within Large Language Models", "Revealing the Parallel Multilingual Learning within Large Language Models", "Lexically Grounded Subword Segmentation", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation", "SCOI: Syntax-augmented Coverage-based In-context Example Selection for Machine Translation", "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance", "Distributional Properties of Subword Regularization", "Distributional Properties of Subword Regularization", "MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language", "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "Curriculum Consistency Learning for Conditional Sentence Generation", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Can Automatic Metrics Assess High-Quality Translations?", "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation", "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation", "Building Resources for Emakhuwa: Machine Translation and News Classification Benchmarks", "Building Resources for Emakhuwa: Machine Translation and News Classification Benchmarks", "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation", "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation", "Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level", "Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level", "Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing", "Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs", "BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training", "BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training", "What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study", "What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study", "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation", "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "Back to School: Translation Using Grammar Books", "Back to School: Translation Using Grammar Books", "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators", "Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics", "Improving Minimum Bayes Risk Decoding with Multi-Prompt", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "Evaluating Automatic Metrics with Incremental Machine Translation Systems", "Evaluating Automatic Metrics with Incremental Machine Translation Systems", "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation", "Leveraging Grammar Induction for Language Understanding and Generation", "Leveraging Grammar Induction for Language Understanding and Generation", "Dual-teacher Knowledge Distillation for Low-frequency Word Translation", "Visual Pivoting Unsupervised Multimodal Machine Translation in Low-Resource Distant Language Pairs", "On Creating an English-Thai Code-switched Machine Translation in Medical Domain", "On Creating an English-Thai Code-switched Machine Translation in Medical Domain", "A Deep Analysis of the Impact of Multiword Expressions and Named Entities on Chinese-English Machine Translations", "Cross-lingual Contextualized Phrase Retrieval", "Cross-lingual Contextualized Phrase Retrieval", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "Low-Resource Machine Translation through the Lens of Personalized Federated Learning", "Datasets for Multilingual Answer Sentence Selection", "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models", "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization", "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization", "Exploring Design Choices for Building Language-Specific LLMs", "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "Multi-Target Cross-Lingual Summarization: a novel task and a language-neutral approach", "Benchmarking Machine Translation with Cultural Awareness", "Benchmarking Machine Translation with Cultural Awareness", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Representational Isomorphism and Alignment of Multilingual Large Language Models", "LexMatcher: Dictionary-centric Data Curation for LLM-based Machine Translation", "LexMatcher: Dictionary-centric Data Curation for LLM-based Machine Translation", "Analyzing Context Contributions in LLM-based Machine Translation", "Analyzing Context Contributions in LLM-based Machine Translation", "XTOWER: A Multilingual LLM for Explaining and Correcting Translation Errors", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS", "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation"], "Few-shot Learning": ["A Survey on In-context Learning", "A Survey on In-context Learning", "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Towards Low-Resource Harmful Meme Detection with LMM Agents", "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "Evaluating Large Language Models via Linguistic Profiling", "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL", "Cross-Domain Audio Deepfake Detection: Dataset and Analysis", "INDUCT-LEARN: Short Phrase Prompting with Instruction Induction", "INDUCT-LEARN: Short Phrase Prompting with Instruction Induction", "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "Learning to Retrieve Iteratively for In-Context Learning", "ADELIE: Aligning Large Language Models on Information Extraction", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "Language-to-Code Translation with a Single Labeled Example", "Emoknob: Enhance Voice Cloning with Fine-Grained Emotion Control", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs", "Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "Major Entity Identification: A Generalizable Alternative to Coreference Resolution", "What Are the Odds? Language Models Are Capable of Probabilistic Reasoning", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction", "OpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation", "OpenSep: Leveraging Large Language Models with Textual Inversion for Open World Audio Separation", "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction", "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "Preserving Generalization of Language Models in Few-shot Continual Relation Extraction", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "Optimizing Chinese Lexical Simplification Across Word Types: A Hybrid Approach", "Generating Demonstrations for In-Context Compositional Generalization in Grounded Language Learning", "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs", "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups", "Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "PALM: Few-Shot Prompt Learning for Audio Language Models", "Multilingual Topic Classification in X: Dataset and Analysis", "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters", "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "Measuring the Robustness of NLP Models to Domain Shifts", "Improve Meta-learning for Few-Shot Text Classification with All You Can Acquire from the Tasks", "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning", "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models", "AnyTrans: Translate AnyText in the Image with Large Scale Models", "A Coarse-to-Fine Prototype Learning Approach for Multi-Label Few-Shot Intent Detection", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation", "Sanitizing Large Language Models in Bug Detection with Data-Flow", "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "TOOLVERIFIER: Generalization to New Tools via Self-Verification", "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models", "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting", "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting", "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning", "In-Context Learning with Iterative Demonstration Selection", "Multilingual Fine-Grained News Headline Hallucination Detection", "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models", "Scalable and Domain-General Abstractive Proposition Segmentation", "Scalable and Domain-General Abstractive Proposition Segmentation", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "Exploring Hint Generation Approaches in Open-Domain Question Answering", "Learning to Plan by Updating Natural Language", "C-ICL: Contrastive In-context Learning for Information Extraction", "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft", "Augmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering", "Dual-Phase Accelerated Prompt Optimization", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "TINYSTYLER: Efficient Few-Shot Text Style Transfer with Authorship Embeddings", "Representational Isomorphism and Alignment of Multilingual Large Language Models", "POSIX: A Prompt Sensitivity Index For Large Language Models", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "Analyzing Context Contributions in LLM-based Machine Translation", "Talking the Talk Does Not Entail Walking the Walk: On the Limits of Large Language Models in Lexical Entailment Recognition", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "Large Language Models are In-context Teachers for Knowledge Reasoning", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", "Personalized Video Comment Generation"], "Data Augmentation": ["ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "Deciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning", "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "RWKV-CLIP: A Robust Vision-Language Representation Learner", "Teaching Small Language Models Reasoning through Counterfactual Distillation", "Story Embeddings \u2013 Narrative-Focused Representations of Fictional Stories", "An Audit on the Perspectives and Challenges of Hallucinations in NLP", "CareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation", "CareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation", "Enhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs", "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free-Word-Ordered and Morphologically-Rich Low-Resource Languages", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference", "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing", "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?", "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model", "VerifyMatch: A Semi-Supervised Learning Paradigm for Natural Language Inference with Confidence-Aware MixUp", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "Task Oriented In-Domain Data Augmentation", "ADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers", "Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4", "Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4", "Retrieval-enriched zero-shot image classification in low-resource domains", "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support", "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support", "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents", "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL", "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "OpenGraph: Towards Open Graph Foundation Models", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "GE2PE: Persian End-to-End Grapheme-to-Phoneme Conversion", "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals", "When and Where Did It Happen? An Encoder-Decoder Model to Identify Scenario Context", "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "CodeFort: Robust Training for Code Generation Models", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "Evolutionary Contrastive Distillation for Language Model Alignment", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "On Creating an English-Thai Code-switched Machine Translation in Medical Domain", "Authorship Obfuscation in Multilingual Machine-Generated Text Detection", "Cross-lingual Contextualized Phrase Retrieval", "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations", "LongForm: Effective Instruction Tuning with Reverse Instructions", "A Study of Implicit Ranking Unfairness in Large Language Models", "A Study of Implicit Ranking Unfairness in Large Language Models", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "ICON: Improving Inter-Report Consistency in Radiology Report Generation via Lesion-aware Mixup Augmentation", "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages", "DICTDIS: Dictionary Constrained Disambiguation for Improved NMT", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "STARK: Social Long-Term Multi-Modal Conversation with Persona Commonsense Knowledge", "Self-training Language Models for Arithmetic Reasoning", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP", "HUMVI: A Multilingual Dataset for Detecting Violent Incidents Impacting Humanitarian Aid", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "\u201cWhat is the value of {templates}?\u201d Rethinking Document Information Extraction Datasets for LLMs", "Better Alignment with Instruction Back-and-Forth Translation", "LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification", "SARCAT: Generative Span-Act Guided Response Generation Using Copy-Enhanced Target Augmentation", "Does Context Help Mitigate Gender Bias in Neural Machine Translation?", "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study", "LAMBDA: Large Language Model-Based Data Augmentation for Multi-Modal Machine Translation", "Controlled Transformation of Text-Attributed Graphs", "Controlled Transformation of Text-Attributed Graphs", "Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing", "Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper"], "Evaluation": ["EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "CUTE: Measuring LLMs' Understanding of Their Tokens", "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models", "MiTTenS: A Dataset for Evaluating Gender Mistranslation", "PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models", "Academics Can Contribute to Domain-Specialized Language Models", "MIRRORSTORIES: Reflecting Diversity through Personalized Narrative Generation with Large Language Models", "PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "LawBench: Benchmarking Legal Knowledge of Large Language Models", "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios", "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "ANALOBENCH: Benchmarking the Identification of Abstract and Long-context Analogies", "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "AKEW: Assessing Knowledge Editing in the Wild", "AKEW: Assessing Knowledge Editing in the Wild", "COPYBENCH: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "Towards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey", "ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP", "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning", "Factuality of Large Language Models: A Survey", "Evaluating Diversity in Automatic Poetry Generation", "Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "Do LLMs Plan Like Human Writers? Comparing Journalist Coverage of Press Releases with LLMs", "Assessing and Verifying Task Utility in LLM-Powered Applications", "CELLO: Causal Evaluation of Large Vision-Language Models", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "An Open-Source Data Contamination Report for Large Language Models", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "SPECIALEX: A Benchmark for In-Context Specialized Lexicon Learning", "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification", "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs", "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Finance Communities in Long Form Question Answering", "Multi-dimensional Evaluation of Empathetic Dialogue Responses", "Privacy Evaluation Benchmarks for NLP Models", "Self-Contradictory Reasoning Evaluation and Detection", "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs", "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "Rethinking Evaluation Methods for Machine Unlearning", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "ALIGN-SIM: A Task-Free Test Bed for Evaluating and Interpreting Sentence Embeddings through Semantic Similarity Alignment", "Towards Benchmarking Situational Awareness of Large Language Models: Comprehensive Benchmark, Evaluation and Analysis", "Towards Benchmarking Situational Awareness of Large Language Models: Comprehensive Benchmark, Evaluation and Analysis", "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer", "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence", "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation", "A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "Can LLM be a Personalized Judge?", "Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis", "In2Core: Leveraging Influence Functions for Coreset Selection in Instruction Finetuning of Large Language Models", "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition", "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion", "MVP-Bench: Can Large Vision\u2013Language Models Conduct Multi-level Visual Perception Like Humans?", "TOWER: Tree Organized Weighting for Evaluating Complex Instructions", "BLADE: Benchmarking Language Model Agents for Data-Driven Science", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "POSIX: A Prompt Sensitivity Index For Large Language Models", "CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models", "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion", "One-to-many testing for code generation from (just) natural language", "Knowledge-Centric Templatic Views of Documents", "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets", "Evaluating Gender Bias of LLMs in Making Morality Judgements", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs"], "Benchmarks": ["RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning", "EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models", "Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval", "Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners", "VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values", "Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "CUTE: Measuring LLMs' Understanding of Their Tokens", "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models", "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation", "I Could've Asked That: Reformulating Unanswerable Questions", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "Knowledge-Centric Hallucination Detection", "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "LawBench: Benchmarking Legal Knowledge of Large Language Models", "Attribute or Abstain: Large Language Models as Long Document Assistants", "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs", "RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs", "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading", "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "Subjective Topic meets LLMs: Unleashing Comprehensive, Reflective and Creative Thinking through the Negation of Negation", "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories", "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "TROTR: A Framework for Evaluating the Recontextualization of Text", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles", "AKEW: Assessing Knowledge Editing in the Wild", "AKEW: Assessing Knowledge Editing in the Wild", "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments", "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning", "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models", "GuardBench: A Large-Scale Benchmark for Guardrail Models", "CAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans", "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages", "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "Mitigating Open-Vocabulary Caption Hallucinations", "Measuring the Robustness of NLP Models to Domain Shifts", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction", "MalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language", "LONGGENBENCH: Long-context Generation Benchmark", "SPECIALEX: A Benchmark for In-Context Specialized Lexicon Learning", "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models", "INTENTIONQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce", "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning", "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs", "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models", "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "LONG2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models", "SecureSQL: Evaluating Data Leakage of Large Language Models as Natural Language Interfaces to Databases", "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models", "CLEAR: Can Language Models Really Understand Causal Graphs?", "M2QA: Multi-domain Multilingual Question Answering", "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays", "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "Towards Benchmarking Situational Awareness of Large Language Models: Comprehensive Benchmark, Evaluation and Analysis", "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation", "Enhancing Tool Retrieval with Iterative Feedback from Large Language Models", "Can LLMs Reason in the Wild with Programs?", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion", "MVP-Bench: Can Large Vision\u2013Language Models Conduct Multi-level Visual Perception Like Humans?", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "LEGOBENCH: Scientific Leaderboard Generation Benchmark", "BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla", "EU DisinfoTest: a Benchmark for Evaluating Language Models' Ability to Detect Disinformation Narratives", "One-to-many testing for code generation from (just) natural language", "M3SCIQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "Probing the Capacity of Language Model Agents to Operationalize Disparate Experiential Context Despite Distraction", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?", "SedarEval: Automated Evaluation using Self-Adaptive Rubrics"], "Summarization": ["MULTI-NEWS+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation", "FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document", "LONGEMBED: Extending Embedding Models for Long Context Retrieval", "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment", "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Story Embeddings \u2013 Narrative-Focused Representations of Fictional Stories", "Optimized Speculative Sampling for GPU Hardware Accelerators", "A Survey of AMR Applications", "A Survey of AMR Applications", "Aligning Large Language Models with Diverse Political Viewpoints", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works", "Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works", "Attribute or Abstain: Large Language Models as Long Document Assistants", "A Thorough Examination of Decoding Methods in the Era of LLMs", "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "Paraphrase Types Elicit Prompt Engineering Capabilities", "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability", "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!", "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "Searching for Best Practices in Retrieval-Augmented Generation", "Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization", "Semformer: Transformer Language Models with Semantic Planning", "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics", "Re-Evaluating Evaluation for Multilingual Summarization", "Which questions should I answer? Salience Prediction of Inquisitive Questions", "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization", "Towards Aligning Language Models with Textual Feedback", "Towards Aligning Language Models with Textual Feedback", "Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion", "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Reformatted Alignment", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning", "Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning", "LoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation", "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization", "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs", "Inference-Time Language Model Alignment via Integrated Value Guidance", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism", "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Event-Keyed Summarization", "Event-Keyed Summarization", "MedINST: Meta Dataset of Biomedical Instructions", "TUTOR-ICL: Guiding Large Language Models for Improved In-Context Learning Performance", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA", "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity", "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts", "Calibrating Long-form Generations from Large Language Models", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "ConTReGen: Context-driven Tree-structured Retrieval for Open-domain Long-form Text Generation", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "Identifying Factual Inconsistencies in Summaries: Grounding LLM Inference via Task Taxonomy", "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models", "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "A Critical Look at Meta-evaluating Summarisation Evaluation Metrics", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS"], "Text Generation": ["Prompts have evil twins", "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation", "ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings", "Evaluating Large Language Models via Linguistic Profiling", "StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements", "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval", "KidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions", "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models", "PERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts", "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs", "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "Reverse-Engineering the Reader", "Precise Model Benchmarking with Only a Few Observations", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood", "Enhancing High-order Interaction Awareness in LLM-based Recommender Model", "Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments", "Atomic Self-Consistency for Better Long Form Generations", "Analysis of Plan-based Retrieval for Grounded Text Generation", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "Style-Specific Neurons for Steering LLMs in Text Style Transfer", "Evaluating n-Gram Novelty of Language Models Using RUSTY-DAWG", "Evaluating n-Gram Novelty of Language Models Using RUSTY-DAWG", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "Control Large Language Models via Divide and Conquer", "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "Scaling Laws for Linear Complexity Language Models", "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards", "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions", "Transformers are Multi-State RNNS", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "An Empirical Analysis of the Writing Styles of Persona-Assigned LLMs", "An Empirical Analysis of the Writing Styles of Persona-Assigned LLMs", "Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?", "DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing", "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules", "Preference-Guided Reflective Sampling for Aligning Language Models", "Adversarial Text Generation using Large Language Models for Dementia Detection", "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation", "Improving Minimum Bayes Risk Decoding with Multi-Prompt", "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models", "Filtered Direct Preference Optimization", "SPECIALEX: A Benchmark for In-Context Specialized Lexicon Learning", "Breaking the Boundaries: A Unified Framework for Chinese Named Entity Recognition Across Text and Speech", "Are LLMs Aware that Some Questions are not Open-ended?", "Are LLMs Aware that Some Questions are not Open-ended?", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "LongForm: Effective Instruction Tuning with Reverse Instructions", "Contextualized Graph Representations for Generating Counter-Narratives against Hate Speech", "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer", "Enable Fast Sampling for Seq2Seq Text Diffusion", "Achieving Stronger Generation via Simple Contrastive Tuning", "Achieving Stronger Generation via Simple Contrastive Tuning", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models", "TARA: Token-level Attribute Relation Adaptation for Multi-Attribute Controllable Text Generation", "TAB2TEXT - A framework for deep learning with tabular data", "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation", "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "Empirical Prior for Text Autoencoders", "Empirical Prior for Text Autoencoders", "Downstream Trade-offs of a Family of Text Watermarks", "Local and Global Decoding in Text Generation", "Local and Global Decoding in Text Generation", "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation", "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains", "Extrinsic Evaluation of Cultural Competence in Large Language Models", "Extrinsic Evaluation of Cultural Competence in Large Language Models"], "Visual QA": ["Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering", "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering", "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "Tag-grounded Visual Instruction Tuning with Retrieval Augmentation", "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering", "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering", "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "Benchmarking Vision Language Models for Cultural Understanding", "Benchmarking Vision Language Models for Cultural Understanding", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model", "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model", "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "Attribute Diversity Determines the Systematicity Gap in VQA", "Attribute Diversity Determines the Systematicity Gap in VQA", "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA", "Large Language Models Know What is Key Visual Entity: An LLM-assisted Multimodal Retrieval for VQA", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "CommVQA: Situating Visual Question Answering in Communicative Contexts", "CommVQA: Situating Visual Question Answering in Communicative Contexts", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments", "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments", "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "Multi-Level Information Retrieval Augmented Generation for Knowledge-based Visual Question Answering", "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models", "In-Context Compositional Generalization for Large Vision-Language Models", "In-Context Compositional Generalization for Large Vision-Language Models", "Game on Tree: Visual Hallucination Mitigation via Coarse-to-Fine View Tree and Game Theory", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems", "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models", "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models", "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "Nearest Neighbor Normalization Improves Multimodal Retrieval", "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM", "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP", "VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "Diversity, Rationalize, and Combine: Ensembling Multiple QA Strategies for Zero-shot Knowledge-based VQA", "A Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect", "A Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect", "M5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "PropTest: Automatic Property Testing for Improved Visual Programming", "PropTest: Automatic Property Testing for Improved Visual Programming", "Learning to Ask Denotative and Connotative Questions for Knowledge-based VQA", "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models", "Zero-shot Commonsense Reasoning over Machine Imagination", "\u201cWhat is the value of {templates}?\u201d Rethinking Document Information Extraction Datasets for LLMs", "\u201cWhat is the value of {templates}?\u201d Rethinking Document Information Extraction Datasets for LLMs", "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "Why do LLaVA Vision-Language Models Reply to Images in English?", "Query-based Cross-Modal Projector Bolstering Mamba Multimodal LLM", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "Towards One-to-Many Visual Question Answering"], "Sentiment Analysis": ["\u201cWe Demand Justice!\u201d: Towards Social Context Grounding of Political Texts", "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue", "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification", "CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading", "DA\u00b3: A Distribution-Aware Adversarial Attack against Language Models", "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates", "Locating Information Gaps and Narrative Inconsistencies Across Languages: A Case Study of LGBT People Portrayals on Wikipedia", "Does Large Language Model Contain Task-Specific Neurons?", "Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners", "MOSEL: Inference Serving Using Dynamic Modality Selection", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Latent Concept-based Explanation of NLP Models", "Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research", "Rethinking the Evaluation of In-Context Learning for LLMs", "Rethinking the Evaluation of In-Context Learning for LLMs", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "RepMatch: Quantifying Cross-Instance Similarities in Representation Space", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Revisiting Supervised Contrastive Learning for Microblog Classification", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "Worry Words: Norms of Anxiety Association for over 44k English Words", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training", "Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training", "Quantum Recurrent Architectures for Text Classification", "Semantics and Sentiment: Cross-lingual Variations in Emoji Use", "Semantics and Sentiment: Cross-lingual Variations in Emoji Use", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "ALVIN: Active Learning Via INterpolation", "Measuring the Robustness of NLP Models to Domain Shifts", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "ASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "Transfer Learning for Text Classification via Model Risk Analysis", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Modeling News Interactions and Influence for Financial Market Prediction", "A Survey on Natural Language Counterfactual Generation", "DADEE: Unsupervised Domain Adaptation in Early Exit PLMS", "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists", "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists", "Crisis counselor language and perceived genuine concern in crisis conversations", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "An Instruction Tuning-Based Contrastive Learning Framework for Aspect Sentiment Quad Prediction with Implicit Aspects and Opinions", "LoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis", "Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis", "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models", "Exploring Design Choices for Building Language-Specific LLMs", "Functionality learning through specification instructions", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Regression-aware Inference with LLMs", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla", "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study", "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization", "The Overlooked Repetitive Lengthening Form in Sentiment Analysis", "The Overlooked Repetitive Lengthening Form in Sentiment Analysis", "Inference and Verbalization Functions During In-Context Learning", "CROWD: Certified Robustness via Weight Distribution for Smoothed Classifiers against Backdoor Attack"], "Robustness": ["Prompts have evil twins", "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning", "Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation", "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection", "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making", "Advancing Large Language Model Attribution through Self-Improving", "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning", "IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding", "FOOL ME IF YOU CAN! An Adversarial Dataset to Investigate the Robustness of LMs in Word Sense Disambiguation", "An LLM Feature-based Framework for Dialogue Constructiveness Assessment", "ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples", "Knowledge Conflicts for LLMs: A Survey", "A Thorough Examination of Decoding Methods in the Era of LLMs", "Towards Robust Speech Representation Learning for Thousands of Languages", "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR", "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented GENERATOR", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently", "Exploring the Practicality of Generative Retrieval on Dynamic Corpora", "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction", "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations", "SecCoder: Towards Generalizable and Robust Secure Code Generation", "Nash CoT: Multi-Path Inference with Preference Equilibrium", "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "RAFT: Realistic Attacks to Fool Text Detectors", "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking", "Revisiting the Robustness of Watermarking to Paraphrasing Attacks", "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics", "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation", "Waterfall: Scalable Framework for Robust Text Watermarking and Provenance for LLMs", "ADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers", "Assessing and Verifying Task Utility in LLM-Powered Applications", "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets", "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models", "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs", "Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology", "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations", "Data Diversity Matters for Robust Instruction Tuning", "Revisiting Query Variation Robustness of Transformer Models", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "Rethinking Evaluation Methods for Machine Unlearning", "A Survey on Natural Language Counterfactual Generation", "CodeFort: Robust Training for Code Generation Models", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "CLEAR: Can Language Models Really Understand Causal Graphs?", "Authorship Obfuscation in Multilingual Machine-Generated Text Detection", "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models", "Enhancing Learning-Based Binary Code Similarity Detection Model through Adversarial Training with Multiple Function Variants", "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks", "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion", "Distance-aware Calibration for Pre-trained Language Models", "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks", "Robust Text Classification: Analyzing Prototype-Based Networks", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions", "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers", "POSIX: A Prompt Sensitivity Index For Large Language Models", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", "Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness", "Robust AI-Generated Text Detection by Restricted Embeddings"], "Natural Language Inference": ["FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document", "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "QUDSELECT: Selective Decoding for Questions Under Discussion Parsing", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search", "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving", "How Hard is this Test Set?\nNLI Characterization by Exploiting Training Dynamics", "How Hard is this Test Set?\nNLI Characterization by Exploiting Training Dynamics", "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "How Do Humans Write Code? Large Models Do It the Same Way Too", "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars", "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars", "Revealing the Parallel Multilingual Learning within Large Language Models", "ECON: On the Detection and Resolution of Evidence Conflicts", "Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners", "Attribute or Abstain: Large Language Models as Long Document Assistants", "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic", "Atomic Inference for NLI with Generated Facts as Atoms", "Atomic Inference for NLI with Generated Facts as Atoms", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "Improve Dense Passage Retrieval with Entailment Tuning", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "Latent Concept-based Explanation of NLP Models", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "VerifyMatch: A Semi-Supervised Learning Paradigm for Natural Language Inference with Confidence-Aware MixUp", "VerifyMatch: A Semi-Supervised Learning Paradigm for Natural Language Inference with Confidence-Aware MixUp", "A Morphology-Based Investigation of Positional Encodings", "GottBERT: a pure German Language Model", "GottBERT: a pure German Language Model", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference", "Inference Helps PLMs' Conceptual Understanding: Improving the Abstract Inference Ability with Hierarchical Conceptual Entailment Graphs", "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets", "Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text", "Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text", "ALVIN: Active Learning Via INterpolation", "Measuring the Robustness of NLP Models to Domain Shifts", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals", "Self-Contradictory Reasoning Evaluation and Detection", "A Survey on Natural Language Counterfactual Generation", "DADEE: Unsupervised Domain Adaptation in Early Exit PLMS", "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference", "Event-Keyed Summarization", "Multilingual Fine-Grained News Headline Hallucination Detection", "CONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules", "Scalable and Domain-General Abstractive Proposition Segmentation", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "How Entangled is Factuality and Deception in German?", "How Entangled is Factuality and Deception in German?", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "\u201cSeeing the Big through the Small\u201d: Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?", "\u201cSeeing the Big through the Small\u201d: Can LLMs Approximate Human Judgment Distributions on NLI from a Few Explanations?", "Identifying Factual Inconsistencies in Summaries: Grounding LLM Inference via Task Taxonomy", "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study", "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization", "CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models", "CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "Inference and Verbalization Functions During In-Context Learning"], "Language Modeling": ["Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS", "Rethinking Token Reduction for State Space Models", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "Leading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Probabilities", "When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages", "Teaching LLMs to Abstain across Languages via Multilingual Feedback", "KidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions", "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models", "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling", "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing", "Can Large Language Models Learn Independent Causal Mechanisms?", "Extending Context Window of Large Language Models from a Distributional Perspective", "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models", "Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives", "Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives", "Can Transformers Learn n-gram Language Models?", "PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "Demystifying Verbatim Memorization in Large Language Models", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Stable Language Model Pre-training by Reducing Embedding Variability", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "Lifelong Event Detection via Optimal Transport", "Target-Aware Language Modeling via Granular Data Sampling", "Chain and Causal Attention for Efficient Entity Tracking", "Turn Waste into Worth: Rectifying Top-k Router of MoE", "Evaluating n-Gram Novelty of Language Models Using RUSTY-DAWG", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training", "Scaling Laws for Linear Complexity Language Models", "Scaling Laws for Linear Complexity Language Models", "Do LLMs learn a true syntactic universal?", "Calibrating Language Models with Adaptive Temperature Scaling", "How to Compute the Probability of a Word", "Language models and brains align due to more than next-word prediction and word-level information", "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models", "Semformer: Transformer Language Models with Semantic Planning", "Transformers are Multi-State RNNS", "Generation with Dynamic Vocabulary", "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection", "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging", "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings", "Is Child-Directed Speech Effective Training Data for Language Models?", "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling", "Tending Towards Stability: Convergence Challenges in Small Language Models", "Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens", "Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens", "On the token distance modeling ability of higher RoPE attention dimension", "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models", "LONGHEADS: Multi-Head Attention is Secretly a Long Context Processor", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "Can Textual Unlearning Solve Cross-Modality Safety Alignment?", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "Generalized Measures of Anticipation and Responsivity in Online Language Processing", "Securing Multi-turn Conversational Language Models From Distributed Backdoor Triggers", "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation", "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "Empirical Prior for Text Autoencoders", "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning", "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models", "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper", "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models"], "Commonsense Reasoning": ["Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "Teaching Small Language Models Reasoning through Counterfactual Distillation", "Investigating Mysteries of CoT-Augmented Distillation", "Investigating Mysteries of CoT-Augmented Distillation", "Focused Large Language Models are Stable Many-Shot Learners", "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering", "Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via LLM-based Theory Resolution", "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs", "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding", "Mixture-of-Subspaces in Low-Rank Adaptation", "Retrieved In-Context Principles from Previous Mistakes", "A Thorough Examination of Decoding Methods in the Era of LLMs", "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "ABSEval: An Agent-based Framework for Script Evaluation", "Large Language Models Can Self-Correct with Key Condition Verification", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "Nash CoT: Multi-Path Inference with Preference Equilibrium", "Commonsense Knowledge Editing Based on Free-Text in LLMs", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Re-Reading Improves Reasoning in Large Language Models", "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training", "Scaling Laws for Linear Complexity Language Models", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game", "Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game", "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "Learning to Paraphrase for Alignment with LLM Preference", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning", "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information", "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning", "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning", "MUSCLE: A Model Update Strategy for Compatible LLM Evolution", "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization", "The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning", "The Mystery of Compositional Generalization in Graph-based Generative Commonsense Reasoning", "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning", "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "Exploring Design Choices for Building Language-Specific LLMs", "Zero-shot Commonsense Reasoning over Machine Imagination", "Zero-shot Commonsense Reasoning over Machine Imagination", "PizzaCommonSense: Learning to Model Commonsense Reasoning about Intermediate Steps in Cooking Recipes", "PizzaCommonSense: Learning to Model Commonsense Reasoning about Intermediate Steps in Cooking Recipes", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "Large Language Models are In-context Teachers for Knowledge Reasoning", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning"], "Contrastive Learning": ["Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "In-context Contrastive Learning for Event Causality Identification", "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval", "Watch Every Step! LLM Agent Learning via Iterative Step-Level Process Refinement", "Decoding the Echoes of Vision from fMRI: Memory Disentangling for Past Semantic Information", "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese", "MiniConGTS: A Near Ultimate Minimalist Contrastive Grid Tagging Scheme for Aspect Sentiment Triplet Extraction", "A Generic Method for Fine-grained Category Discovery in Natural Language Texts", "Deciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System", "Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction", "Story Embeddings \u2013 Narrative-Focused Representations of Fictional Stories", "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP", "Contrastive Entity Coreference and Disambiguation for Historical Texts", "Divide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning", "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free-Word-Ordered and Morphologically-Rich Low-Resource Languages", "Learning to Correct for QA Reasoning with Black-box LLMs", "SignCLIP: Connecting Text and Sign Language by Contrastive Learning", "PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss", "Applying Contrastive Learning to Code Vulnerability Type Classification", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "Improving Knowledge Graph Completion with Structure-Aware Supervised Contrastive Learning", "Cluster-Norm for Unsupervised Probing of Knowledge", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles", "DC-Instruct: An Effective Framework for Generative Multi-intent Spoken Language Understanding", "Interpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding", "Shortcuts Arising from Contrast: Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models", "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality", "Video-Text Prompting for Weakly Supervised Spatio-Temporal Video Grounding", "Updating CLIP to Prefer Descriptions Over Captions", "CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research", "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion", "Local Contrastive Editing of Gender Stereotypes", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "Nearest Neighbor Normalization Improves Multimodal Retrieval", "Document Hashing with Multi-Grained Prototype-Induced Hierarchical Generative Model", "NCPrompt: NSP-Based Prompt Learning and Contrastive Learning for Implicit Discourse Relation Recognition", "BOOLQUESTIONS: Does Dense Retrieval Understand Boolean Logic in Language?", "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation", "QUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware", "TOOLVERIFIER: Generalization to New Tools via Self-Verification", "CodeFort: Robust Training for Code Generation Models", "Evolutionary Contrastive Distillation for Language Model Alignment", "A Simple Angle-based Approach for Contrastive Learning of Unsupervised Sentence Representation", "Cross-lingual Contextualized Phrase Retrieval", "Multi-Loss Fusion: Angular and Contrastive Integration for Machine-Generated Text Detection", "Unsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance", "Pseudo-Label Enhanced Prototypical Contrastive Learning for Uniformed Intent Discovery", "Pseudo-Label Enhanced Prototypical Contrastive Learning for Uniformed Intent Discovery", "An Instruction Tuning-Based Contrastive Learning Framework for Aspect Sentiment Quad Prediction with Implicit Aspects and Opinions", "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation", "C-ICL: Contrastive In-context Learning for Information Extraction", "Improving Factual Consistency of News Summarization by Contrastive Preference Optimization", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "Vanessa : Visual Connotation and Aesthetic Attributes Understanding Network for Multimodal Aspect-based Sentiment Analysis", "Cross-Lingual Multi-Hop Knowledge Editing", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "Representational Isomorphism and Alignment of Multilingual Large Language Models", "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs", "Multi-label Sequential Sentence Classification via Large Language Model"], "Interpretability": ["FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document", "Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation", "Backward Lens: Projecting Language Model Gradients into the Vocabulary Space", "Backward Lens: Projecting Language Model Gradients into the Vocabulary Space", "From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP", "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning", "Interpretability-based Tailored Knowledge Editing in Transformers", "StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements", "An LLM Feature-based Framework for Dialogue Constructiveness Assessment", "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models", "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model", "Does Large Language Model Contain Task-Specific Neurons?", "Taxonomy-guided Semantic Indexing for Academic Paper Search", "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models", "Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning", "Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning", "DIVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions", "Toward Compositional Behavior in Neural Models: A Survey of Current Views", "Atomic Inference for NLI with Generated Facts as Atoms", "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective", "Unveiling the Role of Pretraining in Direct Speech Translation", "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "PANDA: Persona Attributes Navigation for Detecting and Alleviating Overuse Problem in Large Language Models", "Latent Concept-based Explanation of NLP Models", "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models", "Learning Personalized Alignment in Evaluating Open-ended Text Generation", "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries", "Interpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding", "Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability", "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions", "MedCoT: Medical Chain of Thought via Hierarchical Expert", "Information Flow Routes: Automatically Interpreting Language Models at Scale", "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning", "Exploring Intra and Inter-language Consistency in Embeddings with ICA", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "IntCoOp: Interpretability-Aware Vision-Language Prompt Tuning", "Updating CLIP to Prefer Descriptions Over Captions", "Rethinking the Role of Proxy Rewards in Language Model Alignment", "Beyond Correlation: Interpretable Evaluation of Machine Translation Metrics", "Bias Wipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability", "Metrics for What, Metrics for Whom: Assessing Actionability of Bias Evaluation Metrics in NLP\\", "Adversarial Text Generation using Large Language Models for Dementia Detection", "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation", "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation", "GREEN: Generative Radiology Report Evaluation and Error Notation", "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings", "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings", "MGCL: Multi-Granularity Clue Learning for Emotion-Cause Pair Extraction via Cross-Grained Knowledge Distillation", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "Towards Explainable Computerized Adaptive Testing with Large Language Model", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context", "Learning to Generate Rules for Realistic Few-Shot Relation Classification: An Encoder-Decoder Approach", "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists", "Knowledge Mechanisms in Large Language Models: A Survey and Perspective", "Activation Scaling for Steering and Interpreting Language Models", "Variational Language Concepts for Interpreting Foundation Language Models", "Variational Language Concepts for Interpreting Foundation Language Models", "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models", "Low-Resource Machine Translation through the Lens of Personalized Federated Learning", "Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals", "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence", "Improving LLM Attributions with Randomized Path-Integration", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes", "Robust Text Classification: Analyzing Prototype-Based Networks", "NALA: an Effective and Interpretable Entity Alignment Method", "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP", "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions", "Analyzing Context Contributions in LLM-based Machine Translation", "XTOWER: A Multilingual LLM for Explaining and Correcting Translation Errors", "Dual Process Masking for Dialogue Act Recognition", "Efficient and Interpretable Grammatical Error Correction with Mixture of Experts"], "Reinforcement Learning": ["Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation", "Mitigating the Alignment Tax of RLHF", "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment", "Be Helpful but Don't Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support", "VIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation", "VIDEOSCORE: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation", "Direct Multi-Turn Preference Optimization for Language Agents", "Improving Multi-party Dialogue Generation via Topic and Rhetorical Coherence", "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use", "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use", "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback", "F2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation", "Outcome-Constrained Large Language Models for Countering Hate Speech", "Bootstrapped Policy Learning for Task-oriented Dialogue through Goal Shaping", "How Do Humans Write Code? Large Models Do It the Same Way Too", "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs", "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales", "Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation", "Learning to Retrieve Iteratively for In-Context Learning", "METAREFLECTION: Learning Instructions for Language Agents using Past Reflections", "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic", "Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "ORPO: Monolithic Preference Optimization without Reference Model", "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback", "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction", "MIXTURE-OF-SKILLS: Learning to Optimize Data Usage for Fine-Tuning Large Language Models", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents", "Temporally Consistent Factuality Probing for Large Language Models", "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards", "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback", "Grounding Language in Multi-Perspective Referential Communication", "Towards Aligning Language Models with Textual Feedback", "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion", "ABLE: Personalized Disability Support with Politeness and Empathy Integration", "COFFEE-GYM: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code", "COFFEE-GYM: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code", "Mitigating Open-Vocabulary Caption Hallucinations", "RaFe: Ranking Feedback Improves Query Rewriting for RAG", "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation", "Large Language Model-based Human-Agent Collaboration for Complex Task Solving", "Reward Difference Optimization For Sample Reweighting In Offline RLHF", "Enhancing Agent Learning through World Dynamics Modeling", "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension", "Learning Autonomous Driving Tasks via Human Feedbacks with Large Language Models", "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain", "A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies", "Using RL to Identify Divisive Perspectives Improves LLMs Abilities to Identify Communities on Social Media", "Exploiting Careful Design of SVM Solution for Aspect-term Sentiment Analysis", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "E2CL: Exploration-based Error Correction Learning for Embodied Agents", "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "Step-level Value Preference Optimization for Mathematical Reasoning", "Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue", "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models without Preference Data", "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models", "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models", "TRIP NEGOTIATOR: A Travel Persona-aware Reinforced Dialogue Generation Model for Personalized Integrative Negotiation in Tourism"], "Instruction Following": ["Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection", "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation", "Direct Multi-Turn Preference Optimization for Language Agents", "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course", "Instruction Pre-Training: Language Models are Supervised Multitask Learners", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "Automatic Instruction Evolving for Large Language Models", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs", "WPO: Enhancing RLHF with Weighted Preference Optimization", "A Thorough Examination of Decoding Methods in the Era of LLMs", "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG", "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG", "Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models", "FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding", "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models", "Dual-Space Knowledge Distillation for Large Language Models", "ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback", "BLSP-Emo: Towards Empathetic Large Speech-Language Models", "BLSP-Emo: Towards Empathetic Large Speech-Language Models", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks", "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?", "Preference-Guided Reflective Sampling for Aligning Language Models", "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "The SIFo Benchmark: Investigating the Sequential Instruction Following Ability of Large Language Models", "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation", "Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "RULER: A Model-Agnostic Method to Control Generated Length for Large Language Models", "Inference-Time Language Model Alignment via Integrated Value Guidance", "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation\\", "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation\\", "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "Nebula: A Discourse-Aware Minecraft Builder", "LongForm: Effective Instruction Tuning with Reverse Instructions", "LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints", "LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models", "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "Achieving Stronger Generation via Simple Contrastive Tuning", "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors", "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "Resilience of Large Language Models for Noisy Instructions", "How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression"], "Code Generation": ["GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation", "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation", "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "Automatic Instruction Evolving for Large Language Models", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models", "How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with Really Good Data", "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "Free your mouse! Command Large Language Models to Generate Code to Format Word Documents", "Free your mouse! Command Large Language Models to Generate Code to Format Word Documents", "DocCGen: Document-based Controlled Code Generation", "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing", "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing", "Sequential API Function Calling Using GraphQL Schema", "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "CODEJUDGE: Evaluating Code Generation with Large Language Models", "CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research", "RAR: Retrieval-augmented retrieval for code generation in low-resource languages", "RAR: Retrieval-augmented retrieval for code generation in low-resource languages", "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "Improving Minimum Bayes Risk Decoding with Multi-Prompt", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "Large Language Model-based Human-Agent Collaboration for Complex Task Solving", "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation", "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation", "EvOR: Evolving Retrieval for Code Generation", "EvOR: Evolving Retrieval for Code Generation", "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization", "Learning Autonomous Driving Tasks via Human Feedbacks with Large Language Models", "CodeFort: Robust Training for Code Generation Models", "CodeFort: Robust Training for Code Generation Models", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision", "PropTest: Automatic Property Testing for Improved Visual Programming", "Is GPT-4V (ision) All You Need for Automating Academic Data Visualization? Exploring Vision-Language Models' Capability in Reproducing Academic Charts", "CODEIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code", "CODEIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "Fine-tuning Smaller Language Models for Question Answering over Financial Documents", "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft", "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft", "Revisiting the Impact of Pursuing Modularity for Code Generation", "LLM-TOPLA: Efficient LLM Ensemble by Maximising Diversity", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "On Leakage of Code Generation Evaluation Datasets", "On Leakage of Code Generation Evaluation Datasets", "SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement", "One-to-many testing for code generation from (just) natural language", "One-to-many testing for code generation from (just) natural language", "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation", "Socratic Human Feedback (SoHF): Expert Steering Strategies for LLM Code Generation", "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs", "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs"], "Information Retrieval": ["Bridging Cultures in the Kitchen: A Framework and Benchmark for Cross-Cultural Recipe Retrieval", "Evaluating D-MERIT of Partial-annotation on Information Retrieval", "Do Large Language Models Know How Much They Know?", "Unifying Multimodal Retrieval via Document Screenshot Embedding", "GENRA: Enhancing Zero-shot Retrieval with Rank Aggregation", "GENRA: Enhancing Zero-shot Retrieval with Rank Aggregation", "FIRST: Faster Improved Listwise Reranking with Single Token Decoding", "FIRST: Faster Improved Listwise Reranking with Single Token Decoding", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions", "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions", "Exploring the Practicality of Generative Retrieval on Dynamic Corpora", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models", "Bridging Local Details and Global Context in Text-Attributed Graphs", "LitSearch: A Retrieval Benchmark for Scientific Literature Search", "LitSearch: A Retrieval Benchmark for Scientific Literature Search", "Dense X Retrieval: What Retrieval Granularity Should We Use?", "Scaling Laws for Linear Complexity Language Models", "Scaling Laws for Linear Complexity Language Models", "One Thousand and One Pairs: A \u201cnovel\u201d challenge for long-context language models", "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement", "DocCGen: Document-based Controlled Code Generation", "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation", "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "You Make me Feel like a Natural Question: Training QA Systems on Transformed Trivia Questions", "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "MedAdapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "Document Hashing with Multi-Grained Prototype-Induced Hierarchical Generative Model", "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "Exploring the Best Practices of Query Expansion with Large Language Models", "Exploring the Best Practices of Query Expansion with Large Language Models", "Revisiting Query Variation Robustness of Transformer Models", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "Disentangling Questions from Query Generation for Task-Adaptive Retrieval", "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism", "Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell", "MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "STARD: A Chinese Statute Retrieval Dataset Derived from Real-life Queries by Non-professionals", "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval", "Cross-Lingual Multi-Hop Knowledge Editing", "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems", "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model", "Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations", "Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations", "HyQE: Ranking Contexts with Hypothetical Query Embeddings", "HyQE: Ranking Contexts with Hypothetical Query Embeddings", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "M3SCIQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models", "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction"], "Vision-language Models": ["ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement", "TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners", "Enhancing Advanced Visual Reasoning Ability of Large Language Models", "Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "Towards Low-Resource Harmful Meme Detection with LMM Agents", "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification", "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?", "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison", "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison", "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering", "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Benchmarking Vision Language Models for Cultural Understanding", "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "MMOE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts", "ModSCAN: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities", "CommVQA: Situating Visual Question Answering in Communicative Contexts", "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "Interpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding", "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective", "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning", "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "GRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization", "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality", "FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture", "IntCoOp: Interpretability-Aware Vision-Language Prompt Tuning", "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "Retrieval-enriched zero-shot image classification in low-resource domains", "CELLO: Causal Evaluation of Large Vision-Language Models", "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP", "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models", "VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "FAITHSCORE: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models", "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts", "Unveiling the Invisible: Captioning Videos with Metaphors", "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "MACAROON: Training Vision-Language Models To Be Your Engaged Partners", "Is GPT-4V (ision) All You Need for Automating Academic Data Visualization? Exploring Vision-Language Models' Capability in Reproducing Academic Charts", "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models", "Can Textual Unlearning Solve Cross-Modality Safety Alignment?", "Can Textual Unlearning Solve Cross-Modality Safety Alignment?", "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models", "Light-weight Fine-tuning Method for Defending Adversarial Noise in Pre-trained Medical Vision-Language Models", "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "Why do LLaVA Vision-Language Models Reply to Images in English?", "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables", "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP", "Improving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design"], "Dataset": ["ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "COCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds", "DocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing", "TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners", "C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits", "MiTTenS: A Dataset for Evaluating Gender Mistranslation", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "Leveraging Conflicts in Social Media Posts: Unintended Offense Dataset", "Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges", "STORYSUMM: Evaluating Faithfulness in Story Summarization", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Towards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos", "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles", "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments", "MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations", "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "MEDREADME: A Systematic Study for Fine-grained Sentence Readability in Medical Domain", "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures", "Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs", "Multilingual Topic Classification in X: Dataset and Analysis", "CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "ArMeme: Propagandistic Content in Arabic Memes", "FOLIO: Natural Language Reasoning with First-Order Logic", "CELLO: Causal Evaluation of Large Vision-Language Models", "Ukrainian Resilience: A Dataset for Detection of Help-Seeking Signals Amidst the Chaos of War", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems", "Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS", "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization", "Emosical: An Emotion-Annotated Musical Theatre Dataset", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "OEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary", "BOOKWORM: A Dataset for Character Description and Analysis", "\"I Never Said That\": A dataset, taxonomy and baselines on response clarity classification", "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models", "Unveiling the Invisible: Captioning Videos with Metaphors", "ESG-Kor: A Korean Dataset for ESG-related Information Extraction and Practical Use Cases", "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays", "Exploring the Capability of Multimodal LLMs with Yonkoma Manga: The YManga dataset and Its Challenging Tasks", "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence", "ALANAVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding", "Designing Logic Pattern Templates for Counter-Argument Logical Structure Analysis", "MDCR: A Dataset for Multi-Document Conditional Reasoning", "Android in the Zoo: Chain-of-Action-Thought for GUI Agents", "Problem-Oriented Segmentation and Retrieval: Case Study on Tutoring Conversations", "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language Story Understanding", "TextLap: Customizing Language Models for Text-to-Layout Planning", "BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla", "CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models", "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion", "Shoes-ACOSI: A Dataset for Aspect-Based Sentiment Analysis with Implicit Opinion Extraction", "Towards Explainable Chinese Native Learner Essay Fluency Assessment: Dataset, Tasks, and Method", "CoCoHD: Congress Committee Hearing Dataset", "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets", "Evaluating Gender Bias of LLMs in Making Morality Judgements", "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains"], "Benchmarksing": ["Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection", "DocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing", "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification", "An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "Benchmarking Vision Language Models for Cultural Understanding", "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models", "FROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models", "ASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?", "Precise Model Benchmarking with Only a Few Observations", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets", "What Are the Odds? Language Models Are Capable of Probabilistic Reasoning", "ANALOBENCH: Benchmarking the Identification of Abstract and Long-context Analogies", "Large Language Models Can Be Contextual Privacy Protection Learners", "Social Bias Probing: Fairness Benchmarking for Language Models", "Revisiting Automated Evaluation for Long-form Table Question Answering", "ActPlan-1K: Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities", "AppBench: Planning of Multiple APIs from Various APPs for Complex User Instruction", "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?", "SCIAGENT: Tool-augmented Language Models for Scientific Reasoning", "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models", "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model", "Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs", "FOLIO: Natural Language Reasoning with First-Order Logic", "On the Fragility of Active Learners for Text Classification", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents", "MINERS: Multilingual Language Models as Semantic Retrievers", "A Notion of Complexity for Theory of Mind via Discrete World Models", "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models", "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning", "Authorship Obfuscation in Multilingual Machine-Generated Text Detection", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation", "CONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules", "METAKP: On-Demand Keyphrase Generation", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents", "FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks", "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks", "Towards Robust Evaluation of Unlearning in LLMs via Data Transformations", "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems", "TuringQ: Benchmarking AI Comprehension in Theory of Computation", "On Leakage of Code Generation Evaluation Datasets", "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language Story Understanding", "BLADE: Benchmarking Language Model Agents for Data-Driven Science", "BLADE: Benchmarking Language Model Agents for Data-Driven Science", "Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation", "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs"], "Mathematical Reasoning": ["Table Question Answering for Low-resourced Indic Languages", "Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "How Do Humans Write Code? Large Models Do It the Same Way Too", "How Do Humans Write Code? Large Models Do It the Same Way Too", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "MuMath-Code: Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning", "Revealing the Parallel Multilingual Learning within Large Language Models", "Automatic Instruction Evolving for Large Language Models", "FROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models", "FROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "Retrieved In-Context Principles from Previous Mistakes", "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "ControlMath: Controllable Data Generation Promotes Math Generalist Models", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "LM\u00b2: A Simple Society of Language Models Solves Complex Reasoning", "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models", "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models", "Reformatted Alignment", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "Mixed Distillation Helps Smaller Language Models Reason Better", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations", "Enhancing Arguments Recognition for Financial Mathematical Reasoning over Hybrid Data", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts", "Self-Consistency Boosts Calibration for Math Reasoning", "Exploring the Limits of Fine-grained LLM-based Physics Inference via Premise Removal Interventions", "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information", "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information", "AUTODETECT: Towards a Unified Framework for Automated Weakness Detection in Large Language Models", "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning", "Step-level Value Preference Optimization for Mathematical Reasoning", "Step-level Value Preference Optimization for Mathematical Reasoning", "Weak-to-Strong Reasoning", "Learning to Plan by Updating Natural Language", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS", "How Does Quantization Affect Multilingual LLMs?", "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation"], "Chain-of-thought": ["MULTI-NEWS+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation", "Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering", "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback", "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback", "How Do Humans Write Code? Large Models Do It the Same Way Too", "How Do Humans Write Code? Large Models Do It the Same Way Too", "INDUCT-LEARN: Short Phrase Prompting with Instruction Induction", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Teaching Small Language Models Reasoning through Counterfactual Distillation", "Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons", "An Empirical Study of Multilingual Reasoning Distillation for Question Answering", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "Reasoning Robustness of LLMs to Adversarial Typographical Errors", "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMS", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences", "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "HalluMeasure: Fine-grained Hallucination Measurement Using Chain-of-Thought Reasoning", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Tree of Problems: Improving structured problem solving with compositionality", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Sequential API Function Calling Using GraphQL Schema", "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought", "Is This a Bad Table? A Closer Look at the Evaluation of Table Generation from Text", "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity", "Few shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "Guided Knowledge Generation with Language Models for Commonsense Reasoning", "Understanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "Mixed Distillation Helps Smaller Language Models Reason Better", "Abstraction-of-Thought Makes Language Models Better Reasoners", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning", "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "Wrong-of-Thought: An Integrated Reasoning Framework with Multi-Perspective Verification and Wrong Information", "Is GPT-4V (ision) All You Need for Automating Academic Data Visualization? Exploring Vision-Language Models' Capability in Reproducing Academic Charts", "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models", "Auto-Evolve: Enhancing Large Language Model's Performance via Self-Reasoning Framework", "Reference-based Metrics Disprove Themselves in Question Generation", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "XTOWER: A Multilingual LLM for Explaining and Correcting Translation Errors", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations"], "Low-resource Languages": ["Table Question Answering for Low-resourced Indic Languages", "Table Question Answering for Low-resourced Indic Languages", "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models", "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection", "When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages", "Teaching LLMs to Abstain across Languages via Multilingual Feedback", "An Analysis of Multilingual FActScore", "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing", "Using Language Models to Disambiguate Lexical Choices in Translation", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Pretraining Language Models Using Translationese", "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free-Word-Ordered and Morphologically-Rich Low-Resource Languages", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models", "GlossLM: A Massively Multilingual Corpus and Pretrained Model for Interlinear Glossed Text", "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?", "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages", "The Zeno's Paradox of \u2018Low-Resource' Languages", "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "Back to School: Translation Using Grammar Books", "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese", "RAR: Retrieval-augmented retrieval for code generation in low-resource languages", "RAR: Retrieval-augmented retrieval for code generation in low-resource languages", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "MINERS: Multilingual Language Models as Semantic Retrievers", "Are Modern Neural ASR Architectures Robust for Polysynthetic Languages?", "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization", "M5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks", "IndoCL: Benchmarking Indonesian Language Development Assessment", "Can we teach language models to gloss endangered languages?", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "Representation Alignment and Adversarial Networks for Cross-lingual Dependency Parsing", "Unlocking the Potential of Model Merging for Low-Resource Languages", "Low-Resource Machine Translation through the Lens of Personalized Federated Learning", "Datasets for Multilingual Answer Sentence Selection", "Exploring Design Choices for Building Language-Specific LLMs", "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages", "DICTDIS: Dictionary Constrained Disambiguation for Improved NMT", "To Err Is Human, but Llamas Can Learn It Too", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models", "Targeted Multilingual Adaptation for Low-resource Language Families", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS", "How Does Quantization Affect Multilingual LLMs?"], "Transfer Learning": ["Table Question Answering for Low-resourced Indic Languages", "Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNS", "A Survey on In-context Learning", "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation", "Reusing Transferable Weight Increments for Low-resource Style Generation", "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases", "EXPLORA: Efficient Exemplar Subset Selection for Complex Reasoning", "Concept Space Alignment in Multilingual LLMs", "Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models", "Boosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?", "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning", "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "RepMatch: Quantifying Cross-Instance Similarities in Representation Space", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Data Contamination Can Cross Language Barriers", "SEG2ACT: Global Context-aware Action Generation for Document Logical Structuring", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "GottBERT: a pure German Language Model", "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "Transfer Learning for Text Classification via Model Risk Analysis", "Scaling Sentence Embeddings with Large Language Models", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning", "Dual-teacher Knowledge Distillation for Low-frequency Word Translation", "M2QA: Multi-domain Multilingual Question Answering", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "Representation Alignment and Adversarial Networks for Cross-lingual Dependency Parsing", "Causal Discovery Inspired Unsupervised Domain Adaptation for Emotion-Cause Pair Extraction", "LaRA: Large Rank Adaptation for Speech and Text Cross-Modal Learning in Large Language Models", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Datasets for Multilingual Answer Sentence Selection", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation", "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning", "Aligners: Decoupling LLMs and Alignment", "Audio-Based Linguistic Feature Extraction for Enhancing Multi-lingual and Low-Resource Text-to-Speech", "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging", "Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents", "Efficient and Interpretable Grammatical Error Correction with Mixture of Experts", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging"], "Alignment": ["Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence", "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment", "Towards Tool Use Alignment of Large Language Models", "Aligning Language Models to Explicitly Handle Ambiguity", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "Alignment-Enhanced Decoding: Defending Jailbreaks via Token-Level Adaptive Refining of Probability Distributions", "Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems", "ADELIE: Aligning Large Language Models on Information Extraction", "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "MDPO: Conditional Preference Optimization for Multimodal Large Language Models", "WPO: Enhancing RLHF with Weighted Preference Optimization", "On the Relationship between Truth and Political Bias in Language Models", "ARM: An Alignment-and-Replacement Module for Chinese Spelling Check Based on LLMs", "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "ORPO: Monolithic Preference Optimization without Reference Model", "Let Me Teach You: Pedagogical Foundations of Feedback for Language Models", "Learning Personalized Alignment in Evaluating Open-ended Text Generation", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "A SMART Mnemonic Sounds like \u201cGlue Tonic\": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick", "Leveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training", "Towards Aligning Language Models with Textual Feedback", "Do LLMs Know to Respect Copyright Notice?", "Enhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing", "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations", "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "Reformatted Alignment", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "Learning to Paraphrase for Alignment with LLM Preference", "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States", "Scaling Sentence Embeddings with Large Language Models", "Reward Modeling Requires Automatic Adjustment Based on Data Quality", "Self-Evolution Fine-Tuning for Policy Optimization", "A Simple Angle-based Approach for Contrastive Learning of Unsupervised Sentence Representation", "Visual Pivoting Unsupervised Multimodal Machine Translation in Low-Resource Distant Language Pairs", "MathFish: Evaluating Language Model Math Reasoning via Grounding in Educational Curricula", "Semi-Supervised Reward Modeling via Iterative Self-Training", "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness", "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline", "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors", "HealthAlignSumm: Utilizing Alignment for Multimodal Summarization of Code-Mixed Healthcare Dialogues", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "Better Alignment with Instruction Back-and-Forth Translation", "Better Alignment with Instruction Back-and-Forth Translation", "Aligners: Decoupling LLMs and Alignment", "LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "FACTALIGN: Long-form Factuality Alignment of Large Language Models"], "Knowledge Distillation": ["AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation", "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation", "DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models", "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "AlignCap: Aligning Speech Emotion Captioning to Human Preferences", "MTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval", "MTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval", "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server", "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server", "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding", "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision", "Optimizing Chinese Lexical Simplification Across Word Types: A Hybrid Approach", "Optimizing Chinese Lexical Simplification Across Word Types: A Hybrid Approach", "Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation", "LM\u00b2: A Simple Society of Language Models Solves Complex Reasoning", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Dual-Space Knowledge Distillation for Large Language Models", "Dual-Space Knowledge Distillation for Large Language Models", "PAIRDISTILL: Pairwise Relevance Distillation for Dense Retrieval", "PAIRDISTILL: Pairwise Relevance Distillation for Dense Retrieval", "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "Mixed Distillation Helps Smaller Language Models Reason Better", "MGCL: Multi-Granularity Clue Learning for Emotion-Cause Pair Extraction via Cross-Grained Knowledge Distillation", "A Coarse-to-Fine Prototype Learning Approach for Multi-Label Few-Shot Intent Detection", "Privacy Evaluation Benchmarks for NLP Models", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Dual-teacher Knowledge Distillation for Low-frequency Word Translation", "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning", "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "DADEE: Unsupervised Domain Adaptation in Early Exit PLMS", "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL", "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models", "MUSCLE: A Model Update Strategy for Compatible LLM Evolution", "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "LoRAExit: Empowering Dynamic Modulation of LLMs in Resource-limited Settings using Low-rank Adapters", "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models", "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation", "Temperature-Centric Investigation of Speculative Decoding with Knowledge Distillation", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper"], "Hallucination": ["Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models", "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?", "Word Alignment as Preference for Machine Translation", "CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering", "When Context Leads but Parametric Memory Follows in Large Language Models", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales", "Do Large Language Models Know How Much They Know?", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "An Audit on the Perspectives and Challenges of Hallucinations in NLP", "Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via LLM-based Theory Resolution", "Knowledge-Centric Hallucination Detection", "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?", "MDPO: Conditional Preference Optimization for Multimodal Large Language Models", "Attribute or Abstain: Large Language Models as Long Document Assistants", "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models", "Analysis of Plan-based Retrieval for Grounded Text Generation", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting", "Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models", "FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "On the Universal Truthfulness Hyperplane Inside LLMS", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Factuality of Large Language Models: A Survey", "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity", "Are LLMs Aware that Some Questions are not Open-ended?", "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation", "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "FAITHSCORE: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models", "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models", "Learning Semantic Structure through First-Order-Logic Translation", "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Navigating Hallucinations for Reasoning of Unintentional Activities", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation", "Improving Factual Consistency of News Summarization by Contrastive Preference Optimization", "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection", "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts", "Unveiling Narrative Reasoning Limits of Large Language Models with Trope in Movie Synopses", "Self-training Large Language Models through Knowledge Detection", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations", "Unified Active Retrieval for Retrieval Augmented Generation"], "Parameter-efficient Fine-tuning": ["Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models", "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models", "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "PERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts", "Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning", "Mixture-of-Subspaces in Low-Rank Adaptation", "Fast Forwarding Low-Rank Training", "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models", "TL-CL: Task And Language Incremental Continual Learning", "Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation", "ADAPTERS MIXUP: Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers", "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "LoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis", "RoQLlama: A Lightweight Romanian Adapted Language Model", "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models", "BIPEFT: Budget-Guided Iterative Search for Parameter Efficient Fine-Tuning of Large Pretrained Language Models", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Achieving Stronger Generation via Simple Contrastive Tuning", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "OPEN-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models", "Efficient Active Learning with Adapters", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning"], "Named Entity Recognition": ["MTLS: Making Texts into Linguistic Symbols", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "ADELIE: Aligning Large Language Models on Information Extraction", "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints", "Paraphrase Types Elicit Prompt Engineering Capabilities", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents", "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages", "Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Embedded Named Entity Recognition using Probing Classifiers", "Embedded Named Entity Recognition using Probing Classifiers", "NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?", "Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation?", "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries", "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries", "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR", "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR", "A Morphology-Based Investigation of Positional Encodings", "Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4", "GottBERT: a pure German Language Model", "GottBERT: a pure German Language Model", "General Collaborative Framework between Large Language Model and Experts for Universal Information Extraction", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Breaking the Boundaries: A Unified Framework for Chinese Named Entity Recognition Across Text and Speech", "Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting", "Few-shot clinical entity recognition in English, French and Spanish: masked language models outperform generative model prompting", "MedINST: Meta Dataset of Biomedical Instructions", "C-ICL: Contrastive In-context Learning for Information Extraction", "C-ICL: Contrastive In-context Learning for Information Extraction", "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases", "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Efficient Active Learning with Adapters", "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models"], "Image Captioning": ["ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation", "HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding", "Tag-grounded Visual Instruction Tuning with Retrieval Augmentation", "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?", "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "Benchmarking Vision Language Models for Cultural Understanding", "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "A Survey of AMR Applications", "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "Divide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning", "Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes", "Resampled Datasets Are Not Enough: Mitigating Societal Bias Beyond Single Attributes", "Precise Model Benchmarking with Only a Few Observations", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Altogether: Image Captioning via Re-aligning Alt-text", "Altogether: Image Captioning via Re-aligning Alt-text", "Updating CLIP to Prefer Descriptions Over Captions", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "Nearest Neighbor Normalization Improves Multimodal Retrieval", "Mitigating Open-Vocabulary Caption Hallucinations", "Mitigating Open-Vocabulary Caption Hallucinations", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "M5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks", "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "LAMBDA: Large Language Model-Based Data Augmentation for Multi-Modal Machine Translation", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "Improving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design"], "Explainability": ["Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records", "An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records", "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales", "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation", "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "PANDA: Persona Attributes Navigation for Detecting and Alleviating Overuse Problem in Large Language Models", "Latent Concept-based Explanation of NLP Models", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models", "CURE: Context- and Uncertainty-Aware Mental Disorder Detection", "PepRec: Progressive Enhancement of Prompting for Recommendation", "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues", "Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification?", "Local Contrastive Editing of Gender Stereotypes", "Can Large Language Models Identify Authorship?", "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States", "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States", "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration", "Towards Explainable Computerized Adaptive Testing with Large Language Model", "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization", "A Survey on Natural Language Counterfactual Generation", "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring", "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective for Molecular Property Prediction", "On Evaluating Explanation Utility for Human-AI Decision Making in NLP", "PE: A Poincare Explanation Method for Fast Text Hierarchy Generation", "Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals", "Improving LLM Attributions with Randomized Path-Integration", "LLM Explainability via Attributive Masking Learning", "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors", "Denoising Rationalization for Multi-hop Fact Verification via Multi-granular Explainer", "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "LLMs for Generating and Evaluating Counterfactuals: A Comprehensive Study", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "Towards Explainable Chinese Native Learner Essay Fluency Assessment: Dataset, Tasks, and Method", "The Overlooked Repetitive Lengthening Form in Sentiment Analysis", "Zero-Shot Fact Verification via Natural Logic and Large Language Models"], "Bias": ["Systematic Biases in LLM Simulations of Debates", "AGENTREVIEW: Exploring Peer Review Dynamics with LLM Agents", "MiTTenS: A Dataset for Evaluating Gender Mistranslation", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Voices in a Crowd: Searching for Clusters of Unique Perspectives", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Understanding Slang with LLMs: Modelling Cross-Cultural Nuances through Paraphrasing", "Towards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis", "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "How Susceptible are Large Language Models to Ideological Manipulation?", "Moral Foundations of Large Language Models", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "Please note that I'm just an AI: Analysis of Behavior Patterns of LLMs in (Non-)offensive Speech Identification", "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse", "Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection", "\u201cThey are uncultured\u201d: Unveiling Covert Harms and Social Threats in LLM Generated Conversations", "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps", "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "SAFETY-J: Evaluating Safety with Critique", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models", "Evaluating Biases in Context-Dependent Sexual and Reproductive Health Questions", "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric", "BadFair: Backdoored Fairness Attacks with Group-conditioned Triggers", "The Potential and Challenges of Evaluating Attitudes, Opinions, and Values in Large Language Models", "Gender Identity in Pretrained Language Models: An Inclusive Approach to Data Creation and Probing", "Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "SOCIALGAZE: Improving the Integration of Human Social Norms in Large Language Models", "Look Who's Talking Now: Covert Channels From Biased LLMs"], "Synthetic Data": ["Scaling Properties of Speech Language Models", "Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice?", "Advancing Large Language Model Attribution through Self-Improving", "Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs", "Pretraining Language Models Using Translationese", "Personas as a Way to Model Truthfulness in Language Models", "CareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation", "Enhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs", "Emoknob: Enhance Voice Cloning with Fine-Grained Emotion Control", "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "On the In-context Generation of Language Models", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "LLMs Are Prone to Fallacies in Causal Inference", "Effective Synthetic Data and Test-Time Adaptation for OCR Correction", "Fill In The Gaps: Model Calibration and Generalization with Synthetic Data", "Altogether: Image Captioning via Re-aligning Alt-text", "Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection", "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents", "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP", "Is Child-Directed Speech Effective Training Data for Language Models?", "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts", "RAG-Studio: Towards In-Domain Adaptation of Retrieval Augmented Generation Through Self-Alignment", "LONGGENBENCH: Long-context Generation Benchmark", "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?", "Precision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model", "TOOLVERIFIER: Generalization to New Tools via Self-Verification", "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "Private prediction for large-scale synthetic text generation", "Scalable and Domain-General Abstractive Proposition Segmentation", "Multimodal Misinformation Detection by Learning from Synthetic Data with Multimodal LLMs", "Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models", "To Err Is Human, but Llamas Can Learn It Too", "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch", "Better Alignment with Instruction Back-and-Forth Translation", "Aligners: Decoupling LLMs and Alignment", "CACTUS: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory", "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models"], "Classification": ["Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "CareCorpus+: Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "Attribute or Abstain: Large Language Models as Long Document Assistants", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "Major Entity Identification: A Generalizable Alternative to Coreference Resolution", "Applying Contrastive Learning to Code Vulnerability Type Classification", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "A Closer Look at Multidimensional Online Political Incivility", "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Comparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "MIPD: Exploring Manipulation and Intention in A Novel Corpus of Polish Disinformation", "Computational Meme Understanding: A Survey", "Adversarial Text Generation using Large Language Models for Dementia Detection", "Reformatted Alignment", "Categorial Grammar Supertagging via Large Language Models", "Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification", "\"I Never Said That\": A dataset, taxonomy and baselines on response clarity classification", "ESG-Kor: A Korean Dataset for ESG-related Information Extraction and Practical Use Cases", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "TUTOR-ICL: Guiding Large Language Models for Improved In-Context Learning Performance", "Rethinking Code Refinement: Learning to Judge Code Efficiency", "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability", "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models", "Downstream Trade-offs of a Family of Text Watermarks", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness", "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models", "Re-examining Sexism and Misogyny Classification with Annotator Attitudes", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains", "Unified Active Retrieval for Retrieval Augmented Generation", "Unified Active Retrieval for Retrieval Augmented Generation"], "Multi-task Learning": ["Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment", "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment", "MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic", "MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "MODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "Deciphering Rumors: A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning", "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Incomplete Utterance Rewriting with Editing Operation Guidance and Utterance Augmentation", "CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models", "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction", "Language Concept Erasure for Language-invariant Dense Retrieval", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "FAME: Towards Factual Multi-Task Model Editing", "Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards", "Argument Relation Classification through Discourse Markers and Adversarial Training", "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection", "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction", "Temporal Cognitive Tree: A Hierarchical Modeling Approach for Event Temporal Relation Extraction", "Mixed Distillation Helps Smaller Language Models Reason Better", "Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "STTATTS: Unified Speech-To-Text And Text-To-Speech Model", "METAKP: On-Demand Keyphrase Generation", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "Enhancing Emotion-Cause Pair Extraction in Conversations via Center Event Detection and Reasoning", "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing", "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing", "Efficient Pointwise-Pairwise Learning-to-Rank for News Recommendation", "Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification", "Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification"], "Direct Preference Optimization": ["Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence", "Evaluating Psychological Safety of Large Language Models", "Direct Multi-Turn Preference Optimization for Language Agents", "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "Word Alignment as Preference for Machine Translation", "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "ADELIE: Aligning Large Language Models on Information Extraction", "MDPO: Conditional Preference Optimization for Multimodal Large Language Models", "LIONS: An Empirically Optimized Approach to Align Language Models", "\u201cIn Dialogues We Learn\u201d: Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment", "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment", "Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers", "A SMART Mnemonic Sounds like \u201cGlue Tonic\": Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick", "Closing the Loop: Learning to Generate Writing Feedback via Language Model Simulated Student Revisions", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Self-Training Large Language and Vision Assistant for Medical Question-Answering", "Enhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing", "Filtered Direct Preference Optimization", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain", "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning", "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization", "Step-level Value Preference Optimization for Mathematical Reasoning", "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models without Preference Data", "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline", "HealthAlignSumm: Utilizing Alignment for Multimodal Summarization of Code-Mixed Healthcare Dialogues", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback", "Pedagogical Alignment of Large Language Models", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "Self-training Large Language Models through Knowledge Detection", "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization"], "Evaluation Metrics": ["Evaluating Readability and Faithfulness of Concept-based Explanations", "I Could've Asked That: Reformulating Unanswerable Questions", "Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-context Models", "Beyond Reference: Evaluating High Quality Translations Better than Human References", "Understanding and Mitigating Language Confusion in LLMs", "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "Emoknob: Enhance Voice Cloning with Fine-Grained Emotion Control", "APPLS: Evaluating Evaluation Metrics for Plain Language Summarization", "STORYSUMM: Evaluating Faithfulness in Story Summarization", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language", "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models", "PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation", "Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges in Large Language Models", "ABSEval: An Agent-based Framework for Script Evaluation", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "Can Automatic Metrics Assess High-Quality Translations?", "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?", "Measuring Psychological Depth in Language Models", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "Automated Essay Scoring: A Reflection on the State of the Art", "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics", "Re-Evaluating Evaluation for Multilingual Summarization", "Do LLMs Know to Respect Copyright Notice?", "Editing Conceptual Knowledge for Large Language Models", "Recent Trends in Linear Text Segmentation: A Survey", "BOOKWORM: A Dataset for Character Description and Analysis", "A Survey on Natural Language Counterfactual Generation", "A Deep Analysis of the Impact of Multiword Expressions and Named Entities on Chinese-English Machine Translations", "Efficiently Computing Susceptibility to Context in Language Models", "What's under the hood: Investigating Automatic Metrics on Meeting Summarization", "LINKAGE: Listwise Ranking among Varied-Quality References for Non-Factoid QA Evaluation via LLMs", "Information Parity: Measuring and Predicting the Multilingual Capabilities of Language Models", "Scalable and Domain-General Abstractive Proposition Segmentation", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "Improving LLM Attributions with Randomized Path-Integration", "A LLM-Based Ranking Method for the Evaluation of Automatic Counter-Narrative Generation", "Benchmarking Machine Translation with Cultural Awareness", "Regression-aware Inference with LLMs", "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions"], "Multimodal Learning": ["When LLMs Meet Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection", "When LLMs Meet Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection", "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "TOPVIEWRS: Vision-Language Models as Top-View Spatial Reasoners", "Autoregressive Pre-Training on Pixels and Texts", "Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting", "VIMI: Grounding Video Generation through Multi-modal Instruction", "Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts", "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding", "MEANT: Multimodal Encoder for Antecedent Information", "MEANT: Multimodal Encoder for Antecedent Information", "MMOE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts", "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions", "TroL: Traversal of Layers for Large Language and Vision Models", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents", "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking", "Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks", "Unsupervised Discrete Representations of American Sign Language", "VIEWS: Entity-Aware News Video Captioning", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Holistic Evaluation for Interleaved Text-and-Image Generation", "Deciphering Cognitive Distortions in Patient-Doctor Mental Health Conversations: A Multimodal LLM-Based Detection and Reasoning Framework", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "Breaking the Boundaries: A Unified Framework for Chinese Named Entity Recognition Across Text and Speech", "Learning Musical Representations for Music Performance Question Answering", "Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models", "Geneverse: A Collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research", "MMUTF: Multimodal Multimedia Event Argument Extraction with Unified Template Filling", "Hope 'The Paragraph Guy' explains the rest : Introducing MeSum, the Meme Summarizer", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "Large Language Models Are Challenged by Habitat-Centered Reasoning", "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation", "MiRAGeNews: Multimodal Realistic AI-Generated News Detection"], "Efficiency": ["Rethinking Token Reduction for State Space Models", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model", "Learning to Extract Structured Entities Using Language Models", "Multi-pass Decoding for Grammatical Error Correction", "Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "Chain and Causal Attention for Efficient Entity Tracking", "Exploring the Practicality of Generative Retrieval on Dynamic Corpora", "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "Bridging Local Details and Global Context in Text-Attributed Graphs", "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning", "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval", "MedAdapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning", "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning", "EAVE: Efficient Product Attribute Value Extraction via Lightweight Sparse-layer Interaction", "In-Context Former: Lightning-fast Compressing Context for Large Language Model", "QUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware", " FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking ", "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "Improving Multi-Agent Debate with Sparse Communication Topology", "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts", "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness", "PEDANTS: Cheap but Effective and Interpretable Answer Equivalence", "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy", "Exploring Design Choices for Building Language-Specific LLMs", "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning"], "Fairness": ["\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "\"You Gotta be a Doctor, Lin\": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations", "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization", "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment", "\"Global is Good, Local is Bad?\u201d: Understanding Brand Bias in LLMs", "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?", "Social Bias Probing: Fairness Benchmarking for Language Models", "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "How Susceptible are Large Language Models to Ideological Manipulation?", "Annotator-Centric Active Learning for Subjective NLP Tasks", "Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse", "The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models", "Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection", "Bias Wipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability", "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps", "Local Contrastive Editing of Gender Stereotypes", "Reconfidencing LLMs from the Grouping Loss Perspective", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models", "A Survey on Natural Language Counterfactual Generation", "A Fairness-Driven Method for Learning Human-Compatible Negotiation Strategies", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric", "A Study of Implicit Ranking Unfairness in Large Language Models", "TWBias: A Benchmark for Assessing Social Bias in Traditional Chinese Large Language Models through a Taiwan Cultural Lens", "Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models", "Modeling Gender and Dialect Bias in Automatic Speech Recognition", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains", "SOCIALGAZE: Improving the Integration of Human Social Norms in Large Language Models"], "Transformer": ["Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification", "Forgetting Curve: A Reliable Method for Evaluating Memorization Capability for Long-context Models", "Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "Unveiling the Role of Pretraining in Direct Speech Translation", "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Varying Sentence Representations via Condition-Specified Routers", "Semformer: Transformer Language Models with Semantic Planning", "Generation with Dynamic Vocabulary", "TempoFormer: A Transformer for Temporally-aware Representations in Change Detection", "RECANTFormer: Referring Expression Comprehension with Varying Numbers of Targets", "RevMUX: Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference", "Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes", "TRANSLLAMA: LLM-based Simultaneous Translation System", "ASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction", "Exploring Question Guidance and Answer Calibration for Visually Grounded Video Question Answering", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "Graph-tree Fusion Model with Bidirectional Information Propagation for Long Document Classification", "Leveraging Grammar Induction for Language Understanding and Generation", "Stanceformer: Target-Aware Transformer for Stance Detection", "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction", "LOCR: Location-Guided Transformer for Optical Character Recognition", "Automatic Reconstruction of Ancient Chinese Pronunciations", "On the token distance modeling ability of higher RoPE attention dimension", "LaCo: Large Language Model Pruning via Layer Collapse", "Insights into LLM Long-Context Failures: When Transformers Know but Don't Tell", "Financial Forecasting from Textual and Tabular Time Series", "LLM Explainability via Attributive Masking Learning", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "DisGeM: Distractor Generation for Multiple Choice Questions with Span Masking", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models"], "Hallucination Detection": ["FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document", "Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "Embedding and Gradient Say Wrong: A White-Box Method for Hallucination Detection", "Embedding and Gradient Say Wrong: A White-Box Method for Hallucination Detection", "When Context Leads but Parametric Memory Follows in Large Language Models", "Knowledge-Centric Hallucination Detection", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector", "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector", "HalluMeasure: Fine-grained Hallucination Measurement Using Chain-of-Thought Reasoning", "Can LLMs replace Neil deGrasse Tyson? Evaluating the Reliability of LLMs as Science Communicators", "On the Universal Truthfulness Hyperplane Inside LLMS", "Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation", "Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation", "Factuality of Large Language Models: A Survey", "Reference-free Hallucination Detection for Large Vision-Language Models", "Reference-free Hallucination Detection for Large Vision-Language Models", "FASTTRACK: Reliable Fact Tracing via Clustering and LLM-Powered Evidence Validation", "Multilingual Fine-Grained News Headline Hallucination Detection", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models", "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "CED: Comparing Embedding Differences for Detecting Out-of-Distribution and Hallucinated Text", "CED: Comparing Embedding Differences for Detecting Out-of-Distribution and Hallucinated Text", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning", "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation", "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation"], "Calibration": ["Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing", "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "Calibrating the Confidence of Large Language Models by Eliciting Fidelity", "Teaching LLMs to Abstain across Languages via Multilingual Feedback", "Bayesian Calibration of Win Rate Estimation with LLM Evaluators", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models", "CONTESTS: a Framework for Consistency Testing of Span Probabilities in Language Models", "Calibrating Language Models with Adaptive Temperature Scaling", "Calibrating Language Models with Adaptive Temperature Scaling", "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality", "Factuality of Large Language Models: A Survey", "Reconfidencing LLMs from the Grouping Loss Perspective", "Reconfidencing LLMs from the Grouping Loss Perspective", "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration", "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration", "The Craft of Selective Prediction: Towards Reliable Case Outcome Classification - An Empirical Study on European Court of Human Rights Cases", "Self-Consistency Boosts Calibration for Math Reasoning", "Self-Consistency Boosts Calibration for Math Reasoning", "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees", "ExpertEase: A Multi-Agent Framework for Grade-Specific Document Simplification with Large Language Models", "Code Membership Inference for Detecting Unauthorized Data Use in Code Pre-trained Language Models", "Distance-aware Calibration for Pre-trained Language Models", "Distance-aware Calibration for Pre-trained Language Models", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States", "Calibrating Long-form Generations from Large Language Models", "Calibrating Long-form Generations from Large Language Models", "Uncertainty Calibration for Tool-Using Language Agents"], "Cross-lingual Transfer": ["Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection", "Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages", "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm", "TL-CL: Task And Language Incremental Continual Learning", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", ">Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models", "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?", "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages", "A Morphology-Based Investigation of Positional Encodings", "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "MMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos", "Exploring Multilingual Concepts of Human Values in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "MINERS: Multilingual Language Models as Semantic Retrievers", "Large Language Models are Limited in Out-of-Context Knowledge Reasoning", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "Pruning Multilingual Large Language Models for Multilingual Inference", "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models", "Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment", "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?", "Why do LLaVA Vision-Language Models Reply to Images in English?", "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language Story Understanding", "Representational Isomorphism and Alignment of Multilingual Large Language Models", "SSP: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models"], "Multilingual": ["Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment", "MTLS: Making Texts into Linguistic Symbols", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts", "PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "SignCLIP: Connecting Text and Sign Language by Contrastive Learning", "Towards Robust Speech Representation Learning for Thousands of Languages", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering", "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness", "Hate Personified: Investigating the role of LLMs in content moderation", "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups", "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures", "MASIVE: Open-Ended Affective State Identification in English and Spanish", "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese", "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "MMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos", "MMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos", "Bahasa Harmony: A Comprehensive Dataset for Bahasa Text-to-Speech Synthesis with Discrete Codec Modeling of EnGen-TTS", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "M2QA: Multi-domain Multilingual Question Answering", "Authorship Obfuscation in Multilingual Machine-Generated Text Detection", "Multilingual Fine-Grained News Headline Hallucination Detection", "Information Parity: Measuring and Predicting the Multilingual Capabilities of Language Models", "Multilingual Contrastive Decoding via Language-Agnostic Layers Skipping", "Datasets for Multilingual Answer Sentence Selection", "Multilingual Synopses of Movie Narratives: A Dataset for Vision-Language Story Understanding", "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation"], "Relation Extraction": ["Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "ADELIE: Aligning Large Language Models on Information Extraction", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints", "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents", "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents", "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction", "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "Preserving Generalization of Language Models in Few-shot Continual Relation Extraction", "Topic-Oriented Open Relation Extraction with A Priori Seed Generation", "Topic-Oriented Open Relation Extraction with A Priori Seed Generation", "SRF: Enhancing Document-Level Relation Extraction with a Novel Secondary Reasoning Framework", "ATAP: Automatic Template-Augmented Commonsense Knowledge Graph Completion via Pre-Trained Language Models", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "A Survey on Natural Language Counterfactual Generation", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "Entity or Relation Embeddings? An Analysis of Encoding Strategies for Relation Extraction", "Entity or Relation Embeddings? An Analysis of Encoding Strategies for Relation Extraction", "MedINST: Meta Dataset of Biomedical Instructions", "A Survey on Open Information Extraction from Rule-based Model to Large Language Model", "C-ICL: Contrastive In-context Learning for Information Extraction", "C-ICL: Contrastive In-context Learning for Information Extraction", "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "Consistent Document-Level Relation Extraction via Counterfactuals", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "AliGATr: Graph-based layout generation for form understanding", "AliGATr: Graph-based layout generation for form understanding", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models"], "Information Extraction": ["Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Learning to Extract Structured Entities Using Language Models", "A Survey of AMR Applications", "A Survey of AMR Applications", "ADELIE: Aligning Large Language Models on Information Extraction", "Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards", "Exploring Nested Named Entity Recognition with Large Language Models: Methods, Challenges, and Insights", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "Paraphrase Types Elicit Prompt Engineering Capabilities", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "TKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs", "TKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs", "Embedded Named Entity Recognition using Probing Classifiers", "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries", "De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP", "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering", "General Collaborative Framework between Large Language Model and Experts for Universal Information Extraction", "When and Where Did It Happen? An Encoder-Decoder Model to Identify Scenario Context", "When and Where Did It Happen? An Encoder-Decoder Model to Identify Scenario Context", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "ESG-Kor: A Korean Dataset for ESG-related Information Extraction and Practical Use Cases", "Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities", "C-ICL: Contrastive In-context Learning for Information Extraction", "Schema-Driven Information Extraction from Heterogeneous Tables", "Schema-Driven Information Extraction from Heterogeneous Tables", "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases", "\u201cWhat is the value of {templates}?\u201d Rethinking Document Information Extraction Datasets for LLMs", "SciDoc2Diagrammer-MAF: Towards Generation of Scientific Diagrams from Documents guided by Multi-Aspect Feedback Refinement", "Updating Large Language Models' Memories with Time Constraints", "LEGOBENCH: Scientific Leaderboard Generation Benchmark"], "Pre-training": ["NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning", "Cross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective", "Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Autoregressive Pre-Training on Pixels and Texts", "MTLS: Making Texts into Linguistic Symbols", "When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages", "Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System", "Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction", "Pretraining Language Models Using Translationese", "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery", "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers", "PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data", "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models", "Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "Can Language Models Induce Grammatical Knowledge from Indirect Evidence?", "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging", "GottBERT: a pure German Language Model", "ASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding", "Scaling Laws for Fact Memorization of Large Language Models", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "BanglaTLit: A Benchmark Dataset for Back-Transliteration of Romanized Bangla", "CASE: Efficient Curricular Data Pre-training for Building Assistive Psychology Expert Models", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling"], "Domain Adaptation": ["Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "Cross-domain NER with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective", "Advancing Test-Time Adaptation in Wild Acoustic Test Settings", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "Domain adapted machine translation: What does catastrophic forgetting forget and why?", "Domain adapted machine translation: What does catastrophic forgetting forget and why?", "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models", "Generation with Dynamic Vocabulary", "Generation with Dynamic Vocabulary", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts", "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "Transfer Learning for Text Classification via Model Risk Analysis", "Double-Checker: Large Language Model as a Checker for Few-shot Named Entity Recognition", "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis", "A Survey on Natural Language Counterfactual Generation", "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning", "Improving Referring Ability for Biomedical Language Models", "Financial Forecasting from Textual and Tabular Time Series", "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs.", "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science", "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages", "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "Adaptive BPE Tokenization for Enhanced Vocabulary Adaptation in Finetuning Pretrained Language Models", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "Domain Adaptation via Prompt Learning for Alzheimer's Detection", "Robust AI-Generated Text Detection by Restricted Embeddings"], "Knowledge Editing": ["RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "Interpretability-based Tailored Knowledge Editing in Transformers", "Interpretability-based Tailored Knowledge Editing in Transformers", "EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation", "EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation", ">Why Does New Knowledge Create Messy Ripple Effects in LLMs?", ">Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "Commonsense Knowledge Editing Based on Free-Text in LLMs", "Commonsense Knowledge Editing Based on Free-Text in LLMs", "AKEW: Assessing Knowledge Editing in the Wild", "AKEW: Assessing Knowledge Editing in the Wild", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing", "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing", "Editing Conceptual Knowledge for Large Language Models", "Editing Conceptual Knowledge for Large Language Models", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models", "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities", "Adaptive Token Biaser: Knowledge Editing via Biasing Key Entities", "Cross-Lingual Multi-Hop Knowledge Editing", "Cross-Lingual Multi-Hop Knowledge Editing", "Updating Large Language Models' Memories with Time Constraints", "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments", "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments", "Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion"], "Reinforcement Learning From Human Feedback": ["Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence", "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence", "WPO: Enhancing RLHF with Weighted Preference Optimization", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents", "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization", "Calibrating Language Models with Adaptive Temperature Scaling", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Rethinking the Role of Proxy Rewards in Language Model Alignment", "Filtered Direct Preference Optimization", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "Reward Modeling Requires Automatic Adjustment Based on Data Quality", "Self-Evolution Fine-Tuning for Policy Optimization", "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization", "Semi-Supervised Reward Modeling via Iterative Self-Training", "Semi-Supervised Reward Modeling via Iterative Self-Training", "Step-level Value Preference Optimization for Mathematical Reasoning", "Inverse-Q*: Token Level Reinforcement Learning for Aligning Large Language Models without Preference Data", "Towards Pareto-Efficient RLHF: Paying Attention to a Few High-Reward Samples with Reward Dropout", "On Diversified Preferences of Large Language Model Alignment", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback", "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback", "DEFT: Distribution-guided Efficient Fine-Tuning for Human Alignment", "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models", "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization"], "Faithfulness": ["Evaluating Readability and Faithfulness of Concept-based Explanations", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models", "F2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation", "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "APPLS: Evaluating Evaluation Metrics for Plain Language Summarization", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "STORYSUMM: Evaluating Faithfulness in Story Summarization", "Atomic Inference for NLI with Generated Facts as Atoms", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "Revisiting Automated Evaluation for Long-form Table Question Answering", "Learning to Rank Salient Content for Query-focused Summarization", "BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems", "Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach", "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs", "FAITHSCORE: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models", " FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking ", "Activation Scaling for Steering and Interpreting Language Models", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "LLM Explainability via Attributive Masking Learning", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "Unveiling Implicit Table Knowledge with Question-Then-Pinpoint Reasoner for Insightful Table Summarization", "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts", "A Critical Look at Meta-evaluating Summarisation Evaluation Metrics", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning", "Zero-Shot Fact Verification via Natural Logic and Large Language Models", "Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning"], "Human Evaluation": ["Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering", "Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration", "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval-Augmented Question Answering", "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts", "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts", "Aligning Large Language Models with Diverse Political Viewpoints", "PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "STORYSUMM: Evaluating Faithfulness in Story Summarization", "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance", "MMTE: Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language", "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation", "Enhancing AI Assisted Writing with One-Shot Implicit Negative Feedback", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation", "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "DATANARRATIVE: Automated Data-Driven Storytelling with Visualizations and Texts", "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics", "Re-Evaluating Evaluation for Multilingual Summarization", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?", "Rethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning", "Suri: Multi-constraint Instruction Following for Long-form Text Generation", "On Creating an English-Thai Code-switched Machine Translation in Medical Domain", "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?", "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition", "SODA-EVAL: Open-Domain Dialogue Evaluation in the age of LLMs", "Reference-based Metrics Disprove Themselves in Question Generation", "I'm sure you're a real scholar yourself: Exploring Ironic Content Generation by Large Language Models", "I'm sure you're a real scholar yourself: Exploring Ironic Content Generation by Large Language Models", "How Does Quantization Affect Multilingual LLMs?"], "Bias Mitigation": ["\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "Studying and Mitigating Biases in Sign Language Understanding Models", "RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework", "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning", "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction", "Cluster-Norm for Unsupervised Probing of Knowledge", "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation", "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective", "Bias Wipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability", "Local Contrastive Editing of Gender Stereotypes", "Nearest Neighbor Normalization Improves Multimodal Retrieval", "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "Giving Control Back to Models: Enabling Offensive Language Detection Models to Autonomously Identify and Mitigate Biases", "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "Cognitive Bias in Decision-Making with LLMs", "TAB2TEXT - A framework for deep learning with tabular data", "Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models", "Does Context Help Mitigate Gender Bias in Neural Machine Translation?", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information"], "Bias Detection": ["A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "Locating Information Gaps and Narrative Inconsistencies Across Languages: A Case Study of LGBT People Portrayals on Wikipedia", "Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights", "Voices in a Crowd: Searching for Clusters of Unique Perspectives", "\"Global is Good, Local is Bad?\u201d: Understanding Brand Bias in LLMs", "BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs", "ADAPTIVE AXES: A Pipeline for In-domain Social Stereotype Analysis", "Towards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey", "Moral Foundations of Large Language Models", "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation", "Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation", "The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models", "Divine LLaMAs: Bias, Stereotypes, Stigmatization, and Emotion Representation of Religion in Large Language Models", "Quantifying Generative Media Bias with a Corpus of Real-world and Generated News Articles", "Evaluating Biases in Context-Dependent Sexual and Reproductive Health Questions", "PclGPT: A Large Language Model for Patronizing and Condescending Language Detection", "PclGPT: A Large Language Model for Patronizing and Condescending Language Detection", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models", "Gender Identity in Pretrained Language Models: An Inclusive Approach to Data Creation and Probing", "Cognitive Bias in Decision-Making with LLMs", "Can LLMs Replace Clinical Doctors? Exploring Bias in Disease Diagnosis by Large Language Models", "LLM Tropes: Revealing Fine-Grained Values and Opinions in Large Language Models"], "Reading Comprehension": ["Mitigating the Alignment Tax of RLHF", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "Fine-Grained Prediction of Reading Comprehension from Eye Movements", "Revealing the Parallel Multilingual Learning within Large Language Models", "LawBench: Benchmarking Legal Knowledge of Large Language Models", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks", "A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks", "AMPO: Automatic Multi-Branched Prompt Optimization", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "SCIDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals", "SCA: Selective Compression Attention for Efficiently Extending the Context Window of Large Language Models", "Functionality learning through specification instructions", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "InternalInspector I\u00b2: Robust Confidence Estimation in LLMs through Internal States"], "Natural Language Understanding": ["RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "Advancing Event Causality Identification via Heuristic Semantic Dependency Inquiry Network", "On Training Data Influence of GPT Models", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Revealing the Parallel Multilingual Learning within Large Language Models", "FROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models", "FROG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large Language Models", "Perceptions of Linguistic Uncertainty by Language Models and Humans", "Pragmatic Norms Are All You Need \u2013 Why The Symbol Grounding Problem Does Not Apply to LLMs", "MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations", "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "Head-wise Shareable Attention for Large Language Models", "BOOLQUESTIONS: Does Dense Retrieval Understand Boolean Logic in Language?", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "Leveraging Grammar Induction for Language Understanding and Generation", "Self-Distillation for Model Stacking Unlocks Cross-Lingual NLU in 200+ Languages", "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays", "Traffic Light or Light Traffic? Investigating Phrasal Semantics in Large Language Models", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "Zero-shot Commonsense Reasoning over Machine Imagination", "Large Language Models Can Not Perform Well in Understanding and Manipulating Natural Language at Both Character and Word Levels?", "Dual-Phase Accelerated Prompt Optimization", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning", "A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents", "HyperLoRA: Efficient Cross-task Generalization via Constrained Low-Rank Adapters Generation"], "LLM Evaluation": ["Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models", "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts", "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context", "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "PARIKSHA: Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations", "Towards Measuring and Modeling \u201cCulture\u201d in LLMs: A Survey", "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs", "Assessing and Verifying Task Utility in LLM-Powered Applications", "An Open-Source Data Contamination Report for Large Language Models", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning", "Adversarial Math Word Problem Generation", "Self-Evaluation of Large Language Model based on Glass-box Features", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation", "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation", "Knowledge-based Consistency Testing of Large Language Models", "Knowledge-based Consistency Testing of Large Language Models", "SODA-EVAL: Open-Domain Dialogue Evaluation in the age of LLMs", "Cognitive Bias in Decision-Making with LLMs", "Compare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY", "Compare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY", "TOWER: Tree Organized Weighting for Evaluating Complex Instructions", "Are Large Language Models Consistent over Value-laden Questions?", "SedarEval: Automated Evaluation using Self-Adaptive Rubrics"], "Preference Optimization": ["Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment", "Word Alignment as Preference for Machine Translation", "Advancing Large Language Model Attribution through Self-Improving", "AlignCap: Aligning Speech Emotion Captioning to Human Preferences", "EPO: Hierarchical LLM Agents with Environment Preference Optimization", "Aligning Large Language Models with Diverse Political Viewpoints", "Aligning Large Language Models with Diverse Political Viewpoints", "MDPO: Conditional Preference Optimization for Multimodal Large Language Models", "WPO: Enhancing RLHF with Weighted Preference Optimization", "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment", "ORPO: Monolithic Preference Optimization without Reference Model", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "Adaptive Query Rewriting: Aligning Rewriters through Marginal Probability of Conversational Answers", "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Rethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning", "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain", "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring", "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization", "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization", "Weak-to-Strong Reasoning", "Improving Factual Consistency of News Summarization by Contrastive Preference Optimization", "Self-training Language Models for Arithmetic Reasoning", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback", "Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness"], "Model Compression": ["Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "MTA4DPR: Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval", "Structured Optimal Brain Pruning for Large Language Models", "Structured Optimal Brain Pruning for Large Language Models", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging", "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Dual-Space Knowledge Distillation for Large Language Models", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "Head-wise Shareable Attention for Large Language Models", "Adaptive Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization", "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "LaCo: Large Language Model Pruning via Layer Collapse", "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL", "Intermediate Layer Distillation with the Reused Teacher Classifier: A Study on the Importance of the Classifier of Attention-based Models", "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy", "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling", "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models", "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models"], "Logical Reasoning": ["Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "LogicAsker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models", "Conditional and Modal Reasoning in Large Language Models", "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars", "Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars", "Retrieved In-Context Principles from Previous Mistakes", "METAREFLECTION: Learning Instructions for Language Agents using Past Reflections", "Puzzle Solving using Reasoning of Large Language Models: A Survey", "Boosting Logical Fallacy Reasoning in LLMs via Logical Structure Tree", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "Symbolic Working Memory Enhances Language Models for Complex Rule Application", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?", "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?", "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models", "FOLIO: Natural Language Reasoning with First-Order Logic", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "Abstraction-of-Thought Makes Language Models Better Reasoners", "EconLogicQA: A Question-Answering Benchmark for Evaluating Large Language Models in Economic Sequential Reasoning", "SALMON: A Structure-Aware Language Model with logicality and densification strategy for Temporal Knowledge Graph Reasoning", "Learning to Plan by Updating Natural Language", "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains", "P-FOLIO: Evaluating and Improving Logical Reasoning with Abundant Human-Written Reasoning Chains", "Chain of Condition: Construct, Verify and Solve Conditions for Conditional Question Answering"], "Speech Recognition": ["Scaling Properties of Speech Language Models", "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects", "EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning", "Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges", "Multi-Dialect Vietnamese: Task, Dataset, Baseline Models and Challenges", "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models", "MOSEL: Inference Serving Using Dynamic Modality Selection", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages", "950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages", "VHASR: A Multimodal Speech Recognition System With Vision Hotwords", "AudioVSR: Enhancing Video Speech Recognition with Audio Data", "Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps", "Twists, Humps, and Pebbles: Multilingual Speech Recognition Models Exhibit Gender Performance Gaps", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "SpeechQE: Estimating the Quality of Direct Speech Translation", "Are Modern Neural ASR Architectures Robust for Polysynthetic Languages?", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "STTATTS: Unified Speech-To-Text And Text-To-Speech Model", "LaRA: Large Rank Adaptation for Speech and Text Cross-Modal Learning in Large Language Models", "Modeling Gender and Dialect Bias in Automatic Speech Recognition"], "Transformers": ["DocHieNet: A Large and Diverse Dataset for Document Hierarchy Parsing", "Model Balancing Helps Low-data Training and Fine-tuning", "Autoregressive Pre-Training on Pixels and Texts", "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning", "Interpretability-based Tailored Knowledge Editing in Transformers", "Semantic Training Signals Promote Hierarchical Syntactic Generalization in Transformers", "Investigating Mysteries of CoT-Augmented Distillation", "MOSEL: Inference Serving Using Dynamic Modality Selection", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "Birdie: Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives", "Can Transformers Learn n-gram Language Models?", "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "Chain and Causal Attention for Efficient Entity Tracking", "MolTRES: Improving Chemical Language Representation Learning for Molecular Property Prediction", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos", "Commonsense Knowledge Editing Based on Free-Text in LLMs", "Transformers are Multi-State RNNS", "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection", "Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules", "ArMeme: Propagandistic Content in Arabic Memes", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "Recent Trends in Linear Text Segmentation: A Survey", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding", "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation", "A Unified Framework for Model Editing"], "Multimodal Llms": ["EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models", "MAR: Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering", "TinyChart: Efficient Chart Understanding with Program-of-Thoughts Learning and Visual Token Merging", "Tag-grounded Visual Instruction Tuning with Retrieval Augmentation", "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting", "M\u00b2PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models", "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model", "Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale", "MDPO: Conditional Preference Optimization for Multimodal Large Language Models", "Layout-aware GUI Screen Reading with Tree-of-Lens Grounding", "Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge", "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance", "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance", "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "Visual Question Decomposition on Multimodal Large Language Models", "LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "Exploring the Capability of Multimodal LLMs with Yonkoma Manga: The YManga dataset and Its Challenging Tasks", "PRESTO: Progressive Pretraining Enhances Synthetic Chemistry Outcomes", "Multimodal Misinformation Detection by Learning from Synthetic Data with Multimodal LLMs", "ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering", "Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models", "Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective", "Personalized Video Comment Generation"], "Hallucination Mitigation": ["EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models", "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps", "HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding", "Knowledge Verification to Nip Hallucination in the Bud", "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models", "Word Alignment as Preference for Machine Translation", "Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "Atomic Self-Consistency for Better Long Form Generations", "Analysis of Plan-based Retrieval for Grounded Text Generation", "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?", "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models", "LONGAGENT: Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration", "Mitigating Open-Vocabulary Caption Hallucinations", "Dynamic Planning for LLM-based Graphical User Interface Automation", "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension", "Sanitizing Large Language Models in Bug Detection with Data-Flow", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective for Molecular Property Prediction", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models", "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models", "MAVEN-FACT: A Large-scale Event Factuality Detection Dataset", "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "Mitigating Hallucination in Fictional Character Role-Play", "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations", "Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning", "Dial BEINFO for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning"], "Quantization": ["Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization", "QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models", "QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models", "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "QUEST: Efficient Extreme Multi-Label Text Classification with Large Language Models on Commodity Hardware", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization", "MobileQuant: Mobile-friendly Quantization for On-device Language Models", "MobileQuant: Mobile-friendly Quantization for On-device Language Models", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "Fast Matrix Multiplications for Lookup Table-Quantized LLMs", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "Exploring Quantization for Efficient Pre-Training of Transformer Language Models", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "QEFT: Quantization for Efficient Fine-Tuning of LLMs", "QEFT: Quantization for Efficient Fine-Tuning of LLMs", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "How Does Quantization Affect Multilingual LLMs?", "How Does Quantization Affect Multilingual LLMs?", "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models", "ATQ: Activation Transformation for Weight-Activation Quantization of Large Language Models", "ATQ: Activation Transformation for Weight-Activation Quantization of Large Language Models"], "Model Editing": ["LEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models", "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning", "Consecutive Batch Model Editing with HooK Layers", "Consecutive Batch Model Editing with HooK Layers", "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries", "Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing", "FAME: Towards Factual Multi-Task Model Editing", "On the Robustness of Editing Large Language Models", "On the Robustness of Editing Large Language Models", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "DEM: Distribution Edited Model for Training with Mixed Data Distributions", "Local Contrastive Editing of Gender Stereotypes", "Local Contrastive Editing of Gender Stereotypes", "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing", "Rebuilding ROME : Resolving Model Collapse during Sequential Model Editing", "Knowledge Graph Enhanced Large Language Model Editing", "Knowledge Graph Enhanced Large Language Model Editing", "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "The Fall of ROME: Understanding the Collapse of LLMs in Model Editing", "The Fall of ROME: Understanding the Collapse of LLMs in Model Editing", "Better Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization", "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models", "Dense Passage Retrieval: Is it Retrieving?", "A Unified Framework for Model Editing", "A Unified Framework for Model Editing"], "Text Summarization": ["Aligning Large Language Models with Diverse Political Viewpoints", "Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?", "Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization?", "FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping", "Semformer: Transformer Language Models with Semantic Planning", "Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts", "Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts", "Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and Generative Datasets", "Exploring the Relationship between In-Context Learning and Instruction Tuning", "Divide and Conquer: Legal Concept-guided Criminal Court View Generation", "RoQLlama: A Lightweight Romanian Adapted Language Model", "ALIGNSUM: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference", "ALIGNSUM: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "HealthAlignSumm: Utilizing Alignment for Multimodal Summarization of Code-Mixed Healthcare Dialogues", "A Decoding Algorithm for Length-Control Summarization Based on Directed Acyclic Transformers", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions", "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions", "Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles", "From Reading to Compressing: Exploring the Multi-document Reader for Prompt Compression", "Presentations are not always linear! GNN meets LLM for Text Document-to-Presentation Transformation with Attribution"], "Hate Speech Detection": ["Hateful Word in Context Classification", "Hateful Word in Context Classification", "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze", "Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze", "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization", "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Hate Personified: Investigating the role of LLMs in content moderation", "MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification", "Annotator-Centric Active Learning for Subjective NLP Tasks", "Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection", "Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection", "PREDICT: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection", "PREDICT: Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection", "Computational Meme Understanding: A Survey", "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation", "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation", "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models", "Functionality learning through specification instructions", "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation", "LLM generated responses to mitigate the impact of hate speech"], "Diversity": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making", "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "How Far Can We Extract Diverse Perspectives from Large Language Models?", "MIRRORSTORIES: Reflecting Diversity through Personalized Narrative Generation with Large Language Models", "Automatic Instruction Evolving for Large Language Models", "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts", "Aligning Large Language Models with Diverse Political Viewpoints", "Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM", "Nash CoT: Multi-Path Inference with Preference Equilibrium", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "Inductive-Deductive Strategy Reuse for Multi-Turn Instructional Dialogues", "Annotator-Centric Active Learning for Subjective NLP Tasks", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "Evaluating Diversity in Automatic Poetry Generation", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "In-Context Learning with Iterative Demonstration Selection", "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLMs"], "Consistency": ["Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification", "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison", "Formality is Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge", "Consistent Bidirectional Language Modelling: Expressive Power and Representational Conciseness", "On the Reliability of Psychological Scales on Large Language Models", "Split and Merge: Aligning Position Biases in LLM-based Evaluators", "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation", "Subword Segmentation in LLMs: Looking at Inflection and Consistency", "Temporally Consistent Factuality Probing for Large Language Models", "Moral Foundations of Large Language Models", "CAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans", "Virtual Personas for Language Models via an Anthology of Backstories", "LONGGENBENCH: Long-context Generation Benchmark", "DEVIL'S ADVOCATE: Anticipatory Reflection for LLM Agents", "Evaluating Language Model Character Traits", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering", "Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "Better Call SAUL: Fluent and Consistent Language Model Editing with Generation Regularization", "Consistent Document-Level Relation Extraction via Counterfactuals", "Denoising Rationalization for Multi-hop Fact Verification via Multi-granular Explainer", "Compare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY", "What Matters in Memorizing and Recalling Facts? Multifaceted Benchmarks for Knowledge Probing in Language Models", "AXCEL: Automated eXplainable Consistency Evaluation using LLMS", "Unraveling the Truth: Do VLMs really Understand Charts? A Deep Dive into Consistency and Robustness"], "Arithmetic Reasoning": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models", "Large Language Models Can Self-Correct with Key Condition Verification", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination", "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding", "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning", "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning", "Re-Reading Improves Reasoning in Large Language Models", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "Is C4 Dataset Optimal for Pruning? An Investigation of Calibration Data for LLM Pruning", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "Evaluating Large Language Models on Time Series Feature Understanding: A Comprehensive Taxonomy and Benchmark", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "Self-training Language Models for Arithmetic Reasoning", "Self-training Language Models for Arithmetic Reasoning"], "Factuality": ["RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "Estimating Knowledge in Large Language Models Without Generating a Single Token", "An Analysis of Multilingual FActScore", "F2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation", "LUQ: Long-text Uncertainty Quantification for LLMs", "Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM", "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "Analysis of Plan-based Retrieval for Grounded Text Generation", "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models", "Temporally Consistent Factuality Probing for Large Language Models", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Factuality of Large Language Models: A Survey", "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models", "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models", "Reformatted Alignment", "BOOKWORM: A Dataset for Character Description and Analysis", "Improving Multi-Agent Debate with Sparse Communication Topology", "Improving Multi-Agent Debate with Sparse Communication Topology", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation", "How Entangled is Factuality and Deception in German?", "Zero-shot Persuasive Chatbots with LLM-Generated Strategies and Information Retrieval", "FACTALIGN: Long-form Factuality Alignment of Large Language Models"], "Visual Reasoning": ["Enhancing Advanced Visual Reasoning Ability of Large Language Models", "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis", "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models", "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "The Instinctive Bias: Spurious Images lead to Illusion in MLLMs", "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model", "Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model", "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "MMCode: Benchmarking Multimodal Large Language Models in Code Generation with Visually Rich Programming Problems", "MM-ChatAlign: A Novel Multimodal Reasoning Framework based on Large Language Models for Entity Alignment", "CONSTRUCTURE: Benchmarking CONcept STRUCTUre REasoning for Multimodal Large Language Models", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "Plot Twist: Multimodal Models Don't Comprehend Simple Chart Details", "PropTest: Automatic Property Testing for Improved Visual Programming", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "Large Language Models Are Challenged by Habitat-Centered Reasoning", "Knowledge-Aware Reasoning over Multimodal Semi-structured Tables", "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks", "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks"], "Benchmarkss": ["Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs", "Academics Can Contribute to Domain-Specialized Language Models", "To Preserve or To Compress: An In-Depth Study of Connector Selection in Multimodal Large Language Models", "Do Text-to-Vis Benchmarks Test Real Use of Visualisations?", "Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding", "Puzzle Solving using Reasoning of Large Language Models: A Survey", "FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition", "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "Building Resources for Emakhuwa: Machine Translation and News Classification Benchmarks", "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning", "LLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks", "Data Contamination Can Cross Language Barriers", "Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models", "An Open-Source Data Contamination Report for Large Language Models", "Privacy Evaluation Benchmarks for NLP Models", "Comparing Edge-based and Node-based Methods on a Citation Prediction Task", "The Effect of Sampling Temperature on Problem Solving in Large Language Models", "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models", "A Survey on Detection of LLMs-Generated Content", "Data-Centric AI in the Age of Large Language Models", "Compare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY", "What Matters in Memorizing and Recalling Facts? Multifaceted Benchmarks for Knowledge Probing in Language Models", "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models", "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization"], "Social Media": ["\u201cWe Demand Justice!\u201d: Towards Social Context Grounding of Political Texts", "F2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation", "Leveraging Conflicts in Social Media Posts: Unintended Offense Dataset", "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "MisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"", "Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models", "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness", "A Closer Look at Multidimensional Online Political Incivility", "Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach", "Understanding Slang with LLMs: Modelling Cross-Cultural Nuances through Paraphrasing", "Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis", "CURE: Context- and Uncertainty-Aware Mental Disorder Detection", "Please note that I'm just an AI: Analysis of Behavior Patterns of LLMs in (Non-)offensive Speech Identification", "Comparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media", "Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health", "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models", "The Empirical Variability of Narrative Perceptions of Social Media Texts", "Multilingual Topic Classification in X: Dataset and Analysis", "Ukrainian Resilience: A Dataset for Detection of Help-Seeking Signals Amidst the Chaos of War", "LLMs Cannot (Yet) Match the Specificity and Simplicity of Online Finance Communities in Long Form Question Answering", "Revealing COVID-19's Social Dynamics: Diachronic Semantic Analysis of Vaccine and Symptom Discourse on Twitter", "Mental Disorder Classification via Temporal Representation of Text", "LLM generated responses to mitigate the impact of hate speech"], "Lora": ["DA\u00b3: A Distribution-Aware Adversarial Attack against Language Models", "Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering", "Reusing Transferable Weight Increments for Low-resource Style Generation", "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration", "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "LLOCO: Learning Long Contexts Offline", "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "Updating CLIP to Prefer Descriptions Over Captions", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "Categorial Grammar Supertagging via Large Language Models", "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly", "Instance-Level Dynamic LoRAs Composition for Cross-Task Generalization", "AdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization", "Fine-Tuning Language Models with Differential Privacy through Adaptive Noise Allocation", "Promoting Data and Model Privacy in Federated Learning through Quantized LoRA", "MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "AFRIINSTRUCT: Instruction Tuning of African Languages for Diverse Tasks", "Towards Efficient Visual-Language Alignment of the Q-Former for Visual Reasoning Tasks", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune", "Towards One-to-Many Visual Question Answering"], "LLM Alignment": ["Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment", "BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment", "Global Reward to Local Rewards: Multimodal-Guided Decomposition for Improving Dialogue Agents", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "Contrastive Policy Gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion", "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization", "Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "Reward Difference Optimization For Sample Reweighting In Offline RLHF", "Reward Difference Optimization For Sample Reweighting In Offline RLHF", "Self-Evolution Fine-Tuning for Policy Optimization", "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "On Diversified Preferences of Large Language Model Alignment", "On Diversified Preferences of Large Language Model Alignment", "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch", "Aligners: Decoupling LLMs and Alignment", "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks", "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization", "On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization"], "Continual Learning": ["SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models", "SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models", "Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting", "Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting", "TL-CL: Task And Language Incremental Continual Learning", "Lifelong Event Detection via Optimal Transport", "COGEN: Learning from Feedback with Coupled Comprehension and Generation", "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models", "Preserving Generalization of Language Models in Few-shot Continual Relation Extraction", "Explicit Memory Learning with Expectation Maximization", "Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech", "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis", "Unlocking Continual Learning Abilities in Language Models", "Unlocking Continual Learning Abilities in Language Models", "ICL: Iterative Continual Learning for Multi-domain Neural Machine Translation", "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging", "Gradient Localization Improves Lifelong Pretraining of Language Models", "Gradient Localization Improves Lifelong Pretraining of Language Models", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging"], "Bert": ["Pixology: Probing the Linguistic and Visual Capabilities of Pixel-based Language Models", "Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature", "The effects of distance on NPI illusive effects in BERT", "When Generative Adversarial Networks Meet Sequence Labeling Challenges", "Revisiting Supertagging for Faster HPSG Parsing", "KAR\u00b3L: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students", "Leveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training", "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers", "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries", "Comparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media", "TempoFormer: A Transformer for Temporally-aware Representations in Change Detection", "MIPD: Exploring Manipulation and Intention in A Novel Corpus of Polish Disinformation", "Automatic sentence segmentation of clinical record narratives in real-world data", "Bias Wipe: Mitigating Unintended Bias in Text Classifiers through Model Interpretability", "GottBERT: a pure German Language Model", "OEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary", "Class Name Guided Out-of-Scope Intent Classification", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction", "Dense Passage Retrieval: Is it Retrieving?", "Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision", "CASE: Efficient Curricular Data Pre-training for Building Assistive Psychology Expert Models"], "Gsm8k": ["Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "MODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning", "From Bottom to Top: Extending the Potential of Parameter Efficient Fine-Tuning", "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "Reasoning Robustness of LLMs to Adversarial Typographical Errors", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "LLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks", "Mentor-KD: Making Small Language Models Better Multi-step Reasoners", "AlphaLoRA: Assigning LoRA Experts Based on Layer Training Quality", "Task Oriented In-Domain Data Augmentation", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "Self-Consistency Boosts Calibration for Math Reasoning", "Weak-to-Strong Reasoning", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning", "Skills-in-Context: Unlocking Compositionality in Large Language Models", "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning"], "Deep Learning": ["Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification", "A Survey on In-context Learning", "Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients", "Unifying Multimodal Retrieval via Document Screenshot Embedding", "The Best Defense is Attack: Repairing Semantics in Textual Adversarial Examples", "MEANT: Multimodal Encoder for Antecedent Information", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "Towards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs", "Applying Contrastive Learning to Code Vulnerability Type Classification", "Latent Concept-based Explanation of NLP Models", "RAt: Injecting Implicit Bias for Text-To-Image Prompt Refinement Models", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "ASTE-Transformer: Modelling Dependencies in Aspect-Sentiment Triplet Extraction", "Tending Towards Stability: Convergence Challenges in Small Language Models", "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check", "Enhancing Multi-Label Text Classification under Label-Dependent Noise: A Label-Specific Denoising Framework", "A Deep Analysis of the Impact of Multiword Expressions and Named Entities on Chinese-English Machine Translations", "Financial Forecasting from Textual and Tabular Time Series", "ROBERT2VecTM: A Novel Approach for Topic Extraction in Islamic Studies", "Improving LLM Attributions with Randomized Path-Integration", "Mental Disorder Classification via Temporal Representation of Text", "TAB2TEXT - A framework for deep learning with tabular data"], "Preference Learning": ["Towards Tool Use Alignment of Large Language Models", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Unsupervised Human Preference Learning", "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "EPO: Hierarchical LLM Agents with Environment Preference Optimization", "Speechworthy Instruction-tuned Language Models", "What are the Generator Preferences for End-to-end Task-Oriented Dialog System?", "Modeling User Preferences with Automatic Metrics: Creating a High-Quality Preference Dataset for Machine Translation", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "Enhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "Learning from Relevant Subgoals in Successful Dialogs using Iterative Training for Task-oriented Dialog Systems", "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning", "Step-level Value Preference Optimization for Mathematical Reasoning", "Step-level Value Preference Optimization for Mathematical Reasoning", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization", "SWAG: Storytelling With Action Guidance", "DEFT: Distribution-guided Efficient Fine-Tuning for Human Alignment"], "Safety": ["CMD: a framework for Context-aware Model self-Detoxification", "CMD: a framework for Context-aware Model self-Detoxification", "VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values", "KidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions", "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective", "The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "Please note that I'm just an AI: Analysis of Behavior Patterns of LLMs in (Non-)offensive Speech Identification", "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models", "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations", "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation", "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming", "PURE: Aligning LLM via Pluggable Query Reformulation for Enhanced Helpfulness", "TrustAgent: Towards Safe and Trustworthy LLM-based Agents", "Ask the experts: Sourcing a high-quality nutrition counseling dataset through Human-AI collaboration", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "Preference Tuning For Toxicity Mitigation Generalizes Across Languages", "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging", "Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging"], "Error Correction": ["Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving", "Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving", "C-LLM: Learn to Check Chinese Spelling Errors Character by Character", "Retrieved In-Context Principles from Previous Mistakes", "ARM: An Alignment-and-Replacement Module for Chinese Spelling Check Based on LLMs", "Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models", "Learn Beyond The Answer: Training Language Models with Reflection for Mathematical Reasoning", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records", "COFFEE-GYM: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code", "Correct after Answer: Enhancing Multi-Span Question Answering with Post-Processing Method", "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check", "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check", "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models", "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models", "E2CL: Exploration-based Error Correction Learning for Embodied Agents", "E2CL: Exploration-based Error Correction Learning for Embodied Agents", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "C-ICL: Contrastive In-context Learning for Information Extraction", "Resilience of Large Language Models for Noisy Instructions", "Resilience of Large Language Models for Noisy Instructions", "XTOWER: A Multilingual LLM for Explaining and Correcting Translation Errors"], "Supervised Fine-tuning": ["ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback", "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales", "LIONS: An Empirically Optimized Approach to Align Language Models", "LIONS: An Empirically Optimized Approach to Align Language Models", "KNN-INSTRUCT: Automatic Instruction Construction with K Nearest Neighbor Deduction", "ORPO: Monolithic Preference Optimization without Reference Model", "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences", "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences", "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging", "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "Instruction Fine-Tuning: Does Prompt Loss Matter?", "Are LLMs Good Annotators for Discourse-level Event Relation Extraction?", "Self-Evolution Fine-Tuning for Policy Optimization", " FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking ", "Learning from Relevant Subgoals in Successful Dialogs using Iterative Training for Task-oriented Dialog Systems", "Unlocking the Potential of Model Merging for Low-Resource Languages", "From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models", "Leveraging Web-Crawled Data for High-Quality Fine-Tuning", "Self-training Language Models for Arithmetic Reasoning", "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks"], "Error Analysis": ["Liar, Liar, Logical Mire: A Benchmark for Suppositional Reasoning in Large Language Models", "ASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?", "Revisiting Automated Evaluation for Long-form Table Question Answering", "NOISEBENCH: Benchmarking the Impact of Real Label Noise on Named Entity Recognition", "A linguistically-motivated evaluation methodology for unraveling model's abilities in reading comprehension tasks", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?", "Step-by-Step Reasoning to Solve Grid Puzzles: Where do LLMs Falter?", "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "A Deep Analysis of the Impact of Multiword Expressions and Named Entities on Chinese-English Machine Translations", "What's under the hood: Investigating Automatic Metrics on Meeting Summarization", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "Difficult Task Yes but Simple Task No: Unveiling the Laziness in Multimodal LLMs", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion", "To Err Is Human, but Llamas Can Learn It Too", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language"], "Knowledge Graphs": ["XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs", "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text", "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints", "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering", "DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction", "1+1>2: Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators?", "Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs", "MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs", "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering", "Knowledge Graph Enhanced Large Language Model Editing", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "What Would Happen Next? Predicting Consequences from An Event Causality Graph", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "Evidence Retrieval for Fact Verification using Multi-stage Reranking", "Retrieval and Reasoning on KGs: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts", "SALMON: A Structure-Aware Language Model with logicality and densification strategy for Temporal Knowledge Graph Reasoning", "A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments", "Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA"], "Tokenization": ["NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning", "Tokenization Is More Than Compression", "CUTE: Measuring LLMs' Understanding of Their Tokens", "Where is the signal in tokenization space?", "Lexically Grounded Subword Segmentation", "Lexically Grounded Subword Segmentation", "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs", "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs", "Distributional Properties of Subword Regularization", "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models", "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation", "BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training", "BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training", "On the Proper Treatment of Tokenization in Psycholinguistics", "Unsupervised Discrete Representations of American Sign Language", "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR", "Tokenization Falling Short: On Subword Robustness in Large Language Models", "Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia", "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference", "Exploring Design Choices for Building Language-Specific LLMs", "Finding the Optimal Byte-Pair Encoding Merge Operations for Neural Machine Translation in a Low-Resource Setting"], "Knowledge Base": ["Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities", "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering", "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases", "Do Large Language Models Know How Much They Know?", "Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation", "Contrastive Entity Coreference and Disambiguation for Historical Texts", "What are the Generator Preferences for End-to-end Task-Oriented Dialog System?", "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting", "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "ROCEL: Advancing Table Entity Linking through Distinctive Row and Column Contexts", "Efficient Overshadowed Entity Disambiguation by Mitigating Shortcut Learning", "Temporally Consistent Factuality Probing for Large Language Models", "ZEBRA: Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "EvOR: Evolving Retrieval for Code Generation", "HoneyComb: A Flexible LLM-Based Agent System for Materials Science", "Context-Driven Index Trimming: A Data Quality Perspective to Enhancing Precision of RALMs", "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation", "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science", "Extractive Medical Entity Disambiguation with Memory Mechanism and Memorized Entity Information"], "Multimodal": ["RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models", "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "Bridging Modalities: Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning", "SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading", "ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos", "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models", "MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification", "In-Context Compositional Generalization for Large Vision-Language Models", "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning", "I-AM-G: Interest Augmented Multimodal Generator for Item Personalization", "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought", "M\u00b3D: MultiModal MultiDocument Fine-Grained Inconsistency Detection", "Rethinking Pragmatics in Large Language Models: Towards Open-Ended Evaluation and Preference Tuning", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "MMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos", "MMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos", "UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause", "Financial Forecasting from Textual and Tabular Time Series", "Seeing Through VisualBERT: A Causal Adventure on Memetic Landscapes"], "Graph Neural Networks": ["Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "Message Passing on Semantic-Anchor-Graphs for Fine-grained Emotion Representation Learning and Classification", "DGLF: A Dual Graph-based Learning Framework for Multi-modal Sarcasm Detection", "Enhancing High-order Interaction Awareness in LLM-based Recommender Model", "Bridging Local Details and Global Context in Text-Attributed Graphs", "Can Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction", "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "Knowledge Graph Enhanced Large Language Model Editing", "Transferability of Syntax-Aware Graph Neural Networks in Zero-Shot Cross-Lingual Semantic Role Labeling", "InsertGNN: A Hierarchical Graph Neural Network for the TOEFL Sentence Insertion Problem", "XRec: Large Language Models for Explainable Recommendation", "What Would Happen Next? Predicting Consequences from An Event Causality Graph", "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning", "OpenGraph: Towards Open Graph Foundation Models", "Comparing Edge-based and Node-based Methods on a Citation Prediction Task", "Explaining Graph Neural Networks with Large Language Models: A Counterfactual Perspective for Molecular Property Prediction", "CONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules", "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs", "Enhancing Emotion-Cause Pair Extraction in Conversations via Center Event Detection and Reasoning", "LEGOBENCH: Scientific Leaderboard Generation Benchmark", "Presentations are not always linear! GNN meets LLM for Text Document-to-Presentation Transformation with Attribution"], "Dialogue Generation": ["Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems", "Ontologically Faithful Generation of Non-Player Character Dialogues", "Ontologically Faithful Generation of Non-Player Character Dialogues", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "Paraphrase Types Elicit Prompt Engineering Capabilities", "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models", "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities", "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective", "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "Towards Aligning Language Models with Textual Feedback", "ABLE: Personalized Disability Support with Politeness and Empathy Integration", "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support", "HOLLMWOOD: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Mixed-Session Conversation with Egocentric Memory", "Securing Multi-turn Conversational Language Models From Distributed Backdoor Triggers", "CACTUS: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory", "TRIP NEGOTIATOR: A Travel Persona-aware Reinforced Dialogue Generation Model for Personalized Integrative Negotiation in Tourism", "TRIP NEGOTIATOR: A Travel Persona-aware Reinforced Dialogue Generation Model for Personalized Integrative Negotiation in Tourism"], "Active Learning": ["ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale", "An L* Algorithm for Deterministic Weighted Regular Languages", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "Annotator-Centric Active Learning for Subjective NLP Tasks", "Annotator-Centric Active Learning for Subjective NLP Tasks", "DISCERN: Decoding Systematic Errors in Natural Language for Text Classifiers", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "On the Fragility of Active Learners for Text Classification", "On the Fragility of Active Learners for Text Classification", "ALVIN: Active Learning Via INterpolation", "Selective Annotation via Data Allocation: These Data Should Be Triaged to Experts for Annotation Rather Than the Model", "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation", "Active Learning for Abstractive Text Summarization via LLM-Determined Curriculum and Certainty Gain Maximization", "Active Learning for Abstractive Text Summarization via LLM-Determined Curriculum and Certainty Gain Maximization", "Efficient Active Learning with Adapters"], "Self-supervised Learning": ["EmphAssess : a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models", "Mitigating Matthew Effect: Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation", "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach", "EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning", "CSSL: Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free-Word-Ordered and Morphologically-Rich Low-Resource Languages", "Towards Robust Speech Representation Learning for Thousands of Languages", "MolTRES: Improving Chemical Language Representation Learning for Molecular Property Prediction", "AudioVSR: Enhancing Video Speech Recognition with Audio Data", "Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training", "NCPrompt: NSP-Based Prompt Learning and Contrastive Learning for Implicit Discourse Relation Recognition", "Enhancing Polyglot Voices by Leveraging Cross-Lingual Fine-Tuning in Any-to-One Voice Conversion", "Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training", "Exploring Question Guidance and Answer Calibration for Visually Grounded Video Question Answering", "Generative Deduplication For Socia Media Data Selection", "Entity or Relation Embeddings? An Analysis of Encoding Strategies for Relation Extraction", "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs", "LLM Explainability via Attributive Masking Learning", "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability", "Phonetic and Lexical Discovery of Canine Vocalization", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness"], "Topic Modeling": ["On Fake News Detection with LLM Enhanced Semantics Mining", "PhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study", "Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature", "Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature", "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments", "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement", "LLMEdgeRefine: Enhancing Text Clustering with LLM-Based Boundary Point Refinement", "The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?", "Reap the Wild Wind: Detecting Media Storms in Large-Scale News Corpora", "Unsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance", "NeuroMax: Enhancing Neural Topic Modeling via Maximizing Mutual Information and Group Topic Regularization", "Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature", "ROBERT2VecTM: A Novel Approach for Topic Extraction in Islamic Studies", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "Platform-Invariant Topic Modeling via Contrastive Learning to Mitigate Platform-Induced Bias", "Topic Modeling: Contextual Token Embeddings Are All You Need", "Topic Modeling: Contextual Token Embeddings Are All You Need", "Characterizing Text Datasets with Psycholinguistic Features", "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs", "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs"], "Synthetic Data Generation": ["Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection", "GOLDCOIN: Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory", "A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners", "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "LLM-based Code-Switched Text Generation for Grammatical Error Correction", "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic CheckLists", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "Synthetic Multimodal Question Generation", "Synthetic Multimodal Question Generation", "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting", "Better Alignment with Instruction Back-and-Forth Translation", "Pedagogical Alignment of Large Language Models", "Aligners: Decoupling LLMs and Alignment", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information", "ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information"], "Open-domain QA": ["BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering", "BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering", "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering", "From RAG to RICHES: Retrieval Interlaced with Sequence Generation", "Large Language Models Can Self-Correct with Key Condition Verification", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Improving Zero-shot LLM Re-Ranker with Risk Minimization", "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation", "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation", "RaFe: Ranking Feedback Improves Query Rewriting for RAG", "Chain-of-Rewrite: Aligning Question and Documents for Open-Domain Question Answering", "Chain-of-Rewrite: Aligning Question and Documents for Open-Domain Question Answering", "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts", "Evidence Retrieval for Fact Verification using Multi-stage Reranking", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs", "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs", "Exploring Hint Generation Approaches in Open-Domain Question Answering"], "Catastrophic Forgetting": ["Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation", "LEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models", "SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models", "Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting", "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs", "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs", "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models", "Lifelong Event Detection via Optimal Transport", "Domain adapted machine translation: What does catastrophic forgetting forget and why?", "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance", "Preserving Pre-trained Representation Space: On Effectiveness of Prefix-tuning for Large Multi-modal Models", "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration", "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models", "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "Large Language Model for Multi-Domain Translation: Benchmarking and Domain CoT Fine-tuning", "Unlocking Continual Learning Abilities in Language Models", "ICL: Iterative Continual Learning for Multi-domain Neural Machine Translation", "Self-training Large Language Models through Knowledge Detection", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging"], "Rag": ["Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models", "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "Investigating LLMs as Voting Assistants via Contextual Augmentation: A Case Study on the European Parliament Elections 2024", "Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation", "Do You Know What You Are Talking About? Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation", "SATYRN: A Platform for Analytics Augmented Generation", "Ranking Manipulation for Conversational Search Engines", "Improve Dense Passage Retrieval with Entailment Tuning", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "QuBE: Question-based Belief Enhancement for Agentic LLM Reasoning", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "EvOR: Evolving Retrieval for Code Generation", "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation", "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs", "Learning Autonomous Driving Tasks via Human Feedbacks with Large Language Models", "FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models", "More Bang for your Context: Virtual Documents for Question Answering over Long Documents", "Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA", "Unified Active Retrieval for Retrieval Augmented Generation"], "Adversarial Attack": ["Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models", "DA\u00b3: A Distribution-Aware Adversarial Attack against Language Models", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks", "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment", "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models", "Ranking Manipulation for Conversational Search Engines", "Reasoning Robustness of LLMs to Adversarial Typographical Errors", "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks", "BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting", "RAFT: Realistic Attacks to Fool Text Detectors", "RAFT: Realistic Attacks to Fool Text Detectors", "RAt: Injecting Implicit Bias for Text-To-Image Prompt Refinement Models", "RAt: Injecting Implicit Bias for Text-To-Image Prompt Refinement Models", "Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology", "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations", "Enhancing Learning-Based Binary Code Similarity Detection Model through Adversarial Training with Multiple Function Variants", "Robust Text Classification: Analyzing Prototype-Based Networks", "Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions"], "Video Understanding": ["MatchTime: Towards Automatic Soccer Game Commentary Generation", "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection", "Encoding and Controlling Global Semantics for Long-form Video Question Answering", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer", "ECIS-VQG: Generation of Entity-centric Information-seeking Questions from Videos", "VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models", "GRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization", "Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties", "A Simple LLM Framework for Long-Range Video Question-Answering", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP", "Enhancing Temporal Modeling of Video LLMs via Time Gating", "Navigating Hallucinations for Reasoning of Unintentional Activities", "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation", "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation", "Grounding Partially-Defined Events in Multimodal Data", "Personalized Video Comment Generation"], "Prompt Optimization": ["Prompts have evil twins", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling", "METAREFLECTION: Learning Instructions for Language Agents using Past Reflections", "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs", "Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs", "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "AMPO: Automatic Multi-Branched Prompt Optimization", "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "StraGo: Harnessing Strategic Guidance for Prompt Optimization", "Repairing Catastrophic-Neglect in Text-to-Image Diffusion Models via Attention-Guided Feature Enhancement", "Dual-Phase Accelerated Prompt Optimization", "LLM as a metric critic for low resource relation identification"], "Adversarial Attacks": ["Prompts have evil twins", "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection", "Thinking Outside of the Differential Privacy Box: A Case Study in Text Privatization with Language Model Prompting", "Reconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with GPT Models", "Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis", "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking", "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming", "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming", "Adversarial Math Word Problem Generation", "Authorship Obfuscation in Multilingual Machine-Generated Text Detection", "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate", "Detecting Machine-Generated Long-Form Content with Latent-Space Variables", "Robust Text Classification: Analyzing Prototype-Based Networks", "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation", "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation", "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers", "CROWD: Certified Robustness via Weight Distribution for Smoothed Classifiers against Backdoor Attack"], "Rlhf": ["Mitigating the Alignment Tax of RLHF", "Calibrating the Confidence of Large Language Models by Eliciting Fidelity", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "WPO: Enhancing RLHF with Weighted Preference Optimization", "Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "Rethinking the Role of Proxy Rewards in Language Model Alignment", "Filtered Direct Preference Optimization", "Reward Modeling Requires Automatic Adjustment Based on Data Quality", "Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring", "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models"], "Annotation": ["What's Mine becomes Yours: Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs", "Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature", "The Computational Anatomy of Humility: Modeling Intellectual Humility in Online Public Discourse", "Leveraging pre-trained language models for linguistic analysis: A case of argument structure constructions", "The Empirical Variability of Narrative Perceptions of Social Media Texts", "Flee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling", "Let's discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment", "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues", "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays", "CEAMC: Corpus and Empirical Study of Argument Analysis in Education via LLMs", "Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas", "Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Understanding Faithfulness and Reasoning of Large Language Models on Plain Biomedical Summaries", "Designing Logic Pattern Templates for Counter-Argument Logical Structure Analysis", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "Large Language Models for Propaganda Span Annotation", "Grounding Partially-Defined Events in Multimodal Data"], "Generation": ["UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation", "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation", "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model", "Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism", "COGEN: Learning from Feedback with Coupled Comprehension and Generation", "COGEN: Learning from Feedback with Coupled Comprehension and Generation", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models", "Reformatted Alignment", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items", "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues", "Modeling Human Subjectivity in LLMs Using Explicit and Implicit Human Factors in Personas", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation", "MATHWELL: Generating Educational Math Word Problems Using Teacher Annotations"], "Privacy": ["An Inversion Attack Against Obfuscated Embedding Matrix in Language Model Inference", "Order of Magnitude Speedups for LLM Membership Inference", "Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients", "Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients", "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model", "PERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts", "RECALL: Membership Inference via Relative Conditional Log-Likelihoods", "Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal Intervention Perspective", "Granular Privacy Control for Geolocation with Vision Language Models", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4", "MedAdapter: Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning", "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models", "Privacy Evaluation Benchmarks for NLP Models", "Rethinking Evaluation Methods for Machine Unlearning", "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models", "CSLM: A Framework for Question Answering Dataset Generation through Collaborative Small Language Models", "Towards Robust Evaluation of Unlearning in LLMs via Data Transformations", "Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains"], "Unsupervised Learning": ["Unsupervised Human Preference Learning", "An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records", "Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of the Noisy Channel", "Matryoshka-Adaptor: Unsupervised and Supervised Tuning for Smaller Embedding Dimensions", "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization", "README++: Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment", "Voices in a Crowd: Searching for Clusters of Unique Perspectives", "Cluster-Norm for Unsupervised Probing of Knowledge", "Unsupervised Named Entity Disambiguation for Low Resource Domains", "COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities", "Unsupervised Extraction of Dialogue Policies from Conversations", "BMRETRIEVER: Tuning Large Language Models as Better Biomedical Text Retrievers", "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts", "Revealing COVID-19's Social Dynamics: Diachronic Semantic Analysis of Vaccine and Symptom Discourse on Twitter", "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning", "Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval", "A Simple Angle-based Approach for Contrastive Learning of Unsupervised Sentence Representation", "Toeing the Party Line: Election Manifestos as a Key to Understand Political Discourse on Twitter", "Class Name Guided Out-of-Scope Intent Classification"], "Safety Alignment": ["Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "DATA ADVISOR: Dynamic Data Curation for Safety Alignment of Large Language Models", "DATA ADVISOR: Dynamic Data Curation for Safety Alignment of Large Language Models", "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models", "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks", "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance", "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking", "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations", "Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations", "How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States", "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing", "Can Textual Unlearning Solve Cross-Modality Safety Alignment?", "Can Textual Unlearning Solve Cross-Modality Safety Alignment?", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch", "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch", "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers"], "Language Model Alignment": ["LIONS: An Empirically Optimized Approach to Align Language Models", "LIONS: An Empirically Optimized Approach to Align Language Models", "Reverse-Engineering the Reader", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities", "GDPO: Learning to Directly Align Language Models with Diversity Using GFlowNets", "Rethinking the Role of Proxy Rewards in Language Model Alignment", "Rethinking the Role of Proxy Rewards in Language Model Alignment", "Enhancing Language Model Alignment: A Confidence-Based Approach to Label Smoothing", "Preference-Guided Reflective Sampling for Aligning Language Models", "Filtered Direct Preference Optimization", "Filtered Direct Preference Optimization", "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search", "Inference-Time Language Model Alignment via Integrated Value Guidance", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "Evolutionary Contrastive Distillation for Language Model Alignment", "FACTALIGN: Long-form Factuality Alignment of Large Language Models"], "Language Understanding": ["NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning", "Autoregressive Pre-Training on Pixels and Texts", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark", "LLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks", "Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game", "MalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language", "MalayMMLU: A Multitask Benchmark for the Low-Resource Malay Language", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance", "Enhancing Agent Learning through World Dynamics Modeling", "Leveraging Grammar Induction for Language Understanding and Generation", "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "MUSCLE: A Model Update Strategy for Compatible LLM Evolution", "Large Language Models are Students at Various Levels: Zero-shot Question Difficulty Estimation", "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models"], "Knowledge Graph": ["A Usage-centric Take on Intent Understanding in E-Commerce", "Efficient Performance Tracking: Leveraging Large Language Models for Automated Construction of Scientific Leaderboards", "Unsupervised Named Entity Disambiguation for Low Resource Domains", "TKGT: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs", "StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for Children's Story-Based Learning", "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering", "Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation", "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature", "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "CNEQ: Incorporating numbers into Knowledge Graph Reasoning", "Knowledge-based Consistency Testing of Large Language Models", "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models", "R\u00b3-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL", "NALA: an Effective and Interpretable Entity Alignment Method", "Mitigating Hallucination in Fictional Character Role-Play", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective", "MedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures"], "Ranking": ["Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing", "Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing", "Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation", "How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?", "Efficient LLM Comparative Assessment: A Product of Experts Framework for Pairwise Comparisons", "AGRAME: Any-Granularity Ranking with Multi-Vector Embeddings", "TROTR: A Framework for Evaluating the Recontextualization of Text", "VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models", "Entity Insertion in Multilingual Linked Corpora: The Case of Wikipedia", "Make Large Language Model a Better Ranker", "Make Large Language Model a Better Ranker", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering", "Exploring Automated Keyword Mnemonics Generation with Large Language Models via Overgenerate-and-Rank", "A Study of Implicit Ranking Unfairness in Large Language Models", "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems", "Enhancing Alignment using Curriculum Learning & Ranked Preferences", "R\u00b3-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL", "LEGOBENCH: Scientific Leaderboard Generation Benchmark"], "Emotion Recognition": ["A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "Visual Prompting in LLMs for Enhancing Emotion Recognition", "Visual Prompting in LLMs for Enhancing Emotion Recognition", "Towards Robust Speech Representation Learning for Thousands of Languages", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "Revisiting Supervised Contrastive Learning for Microblog Classification", "Worry Words: Norms of Anxiety Association for over 44k English Words", "PALM: Few-Shot Prompt Learning for Audio Language Models", "Emotion Granularity from Text: An Aggregate-Level Indicator of Mental Health", "BLSP-Emo: Towards Empathetic Large Speech-Language Models", "No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages", "Emosical: An Emotion-Annotated Musical Theatre Dataset", "Emosical: An Emotion-Annotated Musical Theatre Dataset", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion Cause", "DetectiveNN: Imitating Human Emotional Reasoning with a Recall-Detect-Predict Framework for Emotion Recognition in Conversations", "Vanessa : Visual Connotation and Aesthetic Attributes Understanding Network for Multimodal Aspect-based Sentiment Analysis"], "Translation": ["Mitigating the Alignment Tax of RLHF", "Using Language Models to Disambiguate Lexical Choices in Translation", "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "A Thorough Examination of Decoding Methods in the Era of LLMs", "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting", "Towards Fast Multilingual LLM Inference: Speculative Decoding and Specialized Drafters", "An Analysis and Mitigation of the Reversal Curse", "Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "MedINST: Meta Dataset of Biomedical Instructions", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "A Recipe to Train Powerful Romanian LLMs with English Instructions", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS", "Hop, skip, jump to Convergence: Dynamics of Learning Rate Transitions for Improved Training of Large Language Models"], "Planning": ["MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making", "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration", "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models", "ASSISTANTBENCH: Can Web Agents Solve Realistic and Time-Consuming Tasks?", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "Analysis of Plan-based Retrieval for Grounded Text Generation", "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models", "CAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans", "Thoughts to Target: Enhance Planning for Target-driven Conversation", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "On the Empirical Complexity of Reasoning and Planning in LLMs", "On the Empirical Complexity of Reasoning and Planning in LLMs", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context", "An Evaluation Mechanism of LLM-based Agents on Manipulating APIs", "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs", "Experience as Source for Anticipation and Planning: Experiential Policy Learning for Target-driven Recommendation Dialogues", "Explaining Mixtures of Sources in News Articles"], "Low-rank Adaptation": ["RoseLoRA: Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning", "On Mitigating Performance Disparities in Multilingual Speech Recognition", "Mixture-of-Subspaces in Low-Rank Adaptation", "Fast Forwarding Low-Rank Training", "LoRA-Guard: Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models", "Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models", "RepMatch: Quantifying Cross-Instance Similarities in Representation Space", "ApiQ: Finetuning of 2-Bit Quantized Large Language Model", "Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation", "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "LoRAN: Improved Low-Rank Adaptation by a Non-Linear Transformation", "LaRA: Large Rank Adaptation for Speech and Text Cross-Modal Learning in Large Language Models", "LaMDA: Large Model Fine-Tuning via Spectrally Decomposed Low-Dimensional Adaptation", "LoRASC: Expressive and Generalizable Low-rank Adaptation for Large Models via Slow Cascaded Learning", "Advancing Vision-Language Models with Adapter Ensemble Strategies", "A Study of Parameter Efficient Fine-tuning by Learning to Efficiently Fine-Tune", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning"], "Representation Learning": ["Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement", "PromptReps: Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval", "Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "Can Transformers Learn n-gram Language Models?", "Bridging Local Details and Global Context in Text-Attributed Graphs", "Leveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training", "Revisiting Supervised Contrastive Learning for Microblog Classification", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Automated Tone Transcription and Clustering with Tone2Vec", "MINERS: Multilingual Language Models as Semantic Retrievers", "From Text Segmentation to Enhanced Representation Learning: A Novel Approach to Multi-Label Classification for Long Texts", "Modeling Historical Relevant and Local Frequency Context for Representation-Based Temporal Knowledge Graph Forecasting", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs", "Gender Identity in Pretrained Language Models: An Inclusive Approach to Data Creation and Probing", "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation", "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation"], "Dialogue Systems": ["Be Helpful but Don't Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support", "Efficient Sequential Decision Making with Large Language Models", "Evaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "RA2FD: Distilling Faithfulness into Efficient Dialogue Systems", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "DC-Instruct: An Effective Framework for Generative Multi-intent Spoken Language Understanding", "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues", "Class Name Guided Out-of-Scope Intent Classification", "From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues", "From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues", "LLMs as Collaborator: Demands-Guided Collaborative Retrieval-Augmented Generation for Commonsense Knowledge-Grounded Open-Domain Dialogue Systems", "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions", "Large Language Models Know What To Say But Not When to Speak", "A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents"], "Event Extraction": ["Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training", "ADELIE: Aligning Large Language Models on Information Extraction", "Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments", "Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments", "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness", "SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness", "Media Attitude Detection via Framing Analysis with Events and their Relations", "General Collaborative Framework between Large Language Model and Experts for Universal Information Extraction", "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction", "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction", "Employing Glyphic Information for Chinese Event Extraction with Vision-Language Model", "Employing Glyphic Information for Chinese Event Extraction with Vision-Language Model", "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models", "Event-Keyed Summarization", "Event-Keyed Summarization", "MedINST: Meta Dataset of Biomedical Instructions", "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction", "Debate as Optimization: Adaptive Conformal Prediction and Diverse Retrieval for Event Extraction"], "Multi-hop Reasoning": ["Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large Language Models Attentive Readers?", "First Heuristic Then Rational: Dynamic Use of Heuristics in Language Model Reasoning", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models", "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation", "CNEQ: Incorporating numbers into Knowledge Graph Reasoning", "Augmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering", "Cross-Lingual Multi-Hop Knowledge Editing", "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models", "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective"], "Attention Mechanism": ["How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning", "External Knowledge-Driven Argument Mining: Leveraging Attention-Enhanced Multi-Network Models", "Position Engineering: Boosting Large Language Models through Positional Information Manipulation", "DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination", "Contextualized Sequence Likelihood: Enhanced Confidence Scores for Natural Language Generation", "MARE: Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction", "DKEC: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction", "Chain and Causal Attention for Efficient Entity Tracking", "ROCEL: Advancing Table Entity Linking through Distinctive Row and Column Contexts", "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions", "Varying Sentence Representations via Condition-Specified Routers", "Memorize Step by Step: Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk", "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs", "Stanceformer: Target-Aware Transformer for Stance Detection", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "Query-based Cross-Modal Projector Bolstering Mamba Multimodal LLM", "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression", "SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering"], "Automatic Speech Recognition": ["Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects", "SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "On Mitigating Performance Disparities in Multilingual Speech Recognition", "Optimized Speculative Sampling for GPU Hardware Accelerators", "Advancing Test-Time Adaptation in Wild Acoustic Test Settings", "Muting Whisper: A Universal Acoustic Adversarial Attack on Speech Foundation Models", "Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition", "Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights", "Unveiling the Role of Pretraining in Direct Speech Translation", "Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding", "Are Modern Neural ASR Architectures Robust for Polysynthetic Languages?", "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models", "Exploring the Potential of Multimodal LLM with Knowledge-Intensive Multimodal ASR", "Modeling Gender and Dialect Bias in Automatic Speech Recognition", "Modeling Gender and Dialect Bias in Automatic Speech Recognition", "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper", "Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper"], "Datasets": ["MEANT: Multimodal Encoder for Antecedent Information", "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery", "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback", "Puzzle Solving using Reasoning of Large Language Models: A Survey", "SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents", "MAIR: A Massive Benchmark for Evaluating Instructed Retrieval", "Distractor Generation in Multiple-Choice Tasks: A Survey of Methods, Datasets, and Evaluation", "Adaptive Question Answering: Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations", "Automated Essay Scoring: A Reflection on the State of the Art", "Let's discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment", "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "Rethinking Evaluation Methods for Machine Unlearning", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "Datasets for Multilingual Answer Sentence Selection", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "All You Need is Attention: Lightweight Attention-based Data Augmentation for Text Classification", "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization"], "Text-to-image Generation": ["ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "ImageInWords: Unlocking Hyper-Detailed Image Descriptions", "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation", "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation", "Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation", "Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation", "Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training", "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "GOME: Grounding-based Metaphor Binding With Conceptual Elaboration For Figurative Language Illustration", "Altogether: Image Captioning via Re-aligning Alt-text", "Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image Diffusion Model Training", "Precision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model", "Multimodal Procedural Planning via Dual Text-Image Prompting", "Repairing Catastrophic-Neglect in Text-to-Image Diffusion Models via Attention-Guided Feature Enhancement", "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation", "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation"], "Uncertainty": ["Uncertainty in Language Models: Assessment through Rank-Calibration", "Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences", "Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process", "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models", "Calibrating the Confidence of Large Language Models by Eliciting Fidelity", "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?", "Statistical Uncertainty in Word Embeddings: GloVe-V", "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning", "CURE: Context- and Uncertainty-Aware Mental Disorder Detection", "Characterizing LLM Abstention Behavior in Science QA with Context Perturbations", "On the Rigour of Scientific Writing: Criteria, Analysis, and Insights", "Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning", "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts", "Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation", "Uncertainty Calibration for Tool-Using Language Agents"], "Reward Model": ["Learning Planning-based Reasoning via Trajectories Collection and Process Reward Synthesizing", "Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "A Probability-Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors", "Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration", "Preference-Guided Reflective Sampling for Aligning Language Models", "Filtered Direct Preference Optimization", "Reward Difference Optimization For Sample Reweighting In Offline RLHF", "Reward Modeling Requires Automatic Adjustment Based on Data Quality", "Sing it, Narrate it: Quality Musical Lyrics Translation", "Semi-Supervised Reward Modeling via Iterative Self-Training", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "On Diversified Preferences of Large Language Model Alignment", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts"], "Gender Bias": ["On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models", "\"You Gotta be a Doctor, Lin\": An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations", "Humans or LLMs as the Judge? A Study on Judgement Bias", "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation", "Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective", "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment", "What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study", "Local Contrastive Editing of Gender Stereotypes", "A Unified Framework and Dataset for Assessing Societal Bias in Vision-Language Models", "JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models", "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework", "Gender Bias in Decision-Making with Large Language Models: A Study of Relationship Conflicts", "TWBias: A Benchmark for Assessing Social Bias in Traditional Chinese Large Language Models through a Taiwan Cultural Lens", "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "Does Context Help Mitigate Gender Bias in Neural Machine Translation?", "Modeling Gender and Dialect Bias in Automatic Speech Recognition", "Evaluating Gender Bias of LLMs in Making Morality Judgements"], "Math": ["Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks", "GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts", "Nash CoT: Multi-Path Inference with Preference Equilibrium", "DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging", "SCIAGENT: Tool-augmented Language Models for Scientific Reasoning", "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "Task Oriented In-Domain Data Augmentation", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "AGENTBANK: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories", "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision", "Weak-to-Strong Reasoning", "Skills-in-Context: Unlocking Compositionality in Large Language Models"], "Retrieval": ["LONGEMBED: Extending Embedding Models for Long Context Retrieval", "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation", "How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?", "Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval", "From RAG to RICHES: Retrieval Interlaced with Sequence Generation", "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach", "Analysis of Plan-based Retrieval for Grounded Text Generation", "KAR\u00b3L: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition", "IFCap: Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning", "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "ChartInsights: Evaluating Multimodal Large Language Models for Low-Level Chart Question Answering", "Lost in Translation: Chemical Language Models and the Misunderstanding of Molecule Structures", "R\u00b3-NL2GQL: A Model Coordination and Knowledge Graph Alignment Approach for NL2GQL", "Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision"], "Pruning": ["Rethinking Token Reduction for State Space Models", "FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping", "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging", "Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging", "Threshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval", "Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation", "Local Contrastive Editing of Gender Stereotypes", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "xCOMET-lite: Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics", "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection", "Pruning Foundation Models for High Accuracy without Retraining", "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy", "Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy", "Pruning Multilingual Large Language Models for Multilingual Inference", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "When Compression Meets Model Compression: Memory-Efficient Double Compression for Large Language Models"], "Fact Verification": ["\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization", "Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification", "Molecular Facts: Desiderata for Decontextualization in LLM Fact Verification", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context", "RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning", "Evidence Retrieval for Fact Verification using Multi-stage Reranking", "Evidence Retrieval for Fact Verification using Multi-stage Reranking", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "How Entangled is Factuality and Deception in German?", "Cross-Lingual Multi-Hop Knowledge Editing", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "Mitigating Hallucination in Fictional Character Role-Play", "Zero-Shot Fact Verification via Natural Logic and Large Language Models", "Zero-Shot Fact Verification via Natural Logic and Large Language Models"], "Medical QA": ["Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "LM\u00b2: A Simple Society of Language Models Solves Complex Reasoning", "CasiMedicos-Arg: A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures", "AMPO: Automatic Multi-Branched Prompt Optimization", "Few shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering", "Few shot chain-of-thought driven reasoning to prompt LLMs for open-ended medical question answering", "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering", "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration", "RoQLlama: A Lightweight Romanian Adapted Language Model", "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate", "Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs.", "MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks", "Large Language Models are In-context Teachers for Knowledge Reasoning", "Large Language Models are In-context Teachers for Knowledge Reasoning", "MedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures", "MedLogic-AQA: Enhancing Medical Question Answering with Abstractive Models Focusing on Logical Structures"], "Clustering": ["Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation", "Tracking the perspectives of interacting language models", "A Generic Method for Fine-grained Category Discovery in Natural Language Texts", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA", "Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning", "Latent Concept-based Explanation of NLP Models", "Voices in a Crowd: Searching for Clusters of Unique Perspectives", "Story Morals: Surfacing value-driven narrative schemas using large language models", "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG", "Leveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training", "Open-world Multi-label Text Classification with Extremely Weak Supervision", "DEFT-UCS: Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing", "FASTTRACK: Reliable Fact Tracing via Clustering and LLM-Powered Evidence Validation", "Unsupervised Hierarchical Topic Modeling via Anchor Word Clustering and Path Guidance", "Pseudo-Label Enhanced Prototypical Contrastive Learning for Uniformed Intent Discovery"], "Mixture Of Experts": ["Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models", "Integrating Plutchik's Theory with Mixture of Experts for Enhancing Emotion Classification", "LEMOE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models", "MODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning", "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models", "FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models", "MMOE: Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "Turn Waste into Worth: Rectifying Top-k Router of MoE", "MedCoT: Medical Chain of Thought via Hierarchical Expert", "AdaMOE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts", "OPEN-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models", "BiMediX: Bilingual Medical Mixture of Experts LLM", "Efficient and Interpretable Grammatical Error Correction with Mixture of Experts"], "Long Context": ["LONGEMBED: Extending Embedding Models for Long Context Retrieval", "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "Where am I? Large Language Models Wandering between Semantics and Structures in Long Contexts", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs", "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP", "LLOCO: Learning Long Contexts Offline", "A Simple and Effective L2 Norm-Based Strategy for KV Cache Compression", "Memorize Step by Step: Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk", "COMPACT: Compressing Retrieved Documents Actively for Question Answering", "DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?", "LongAlign: A Recipe for Long Context Alignment of Large Language Models", "KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches", "QRMeM: Unleash the Length Limitation through Question then Reflection Memory Mechanism", "LONGHEADS: Multi-Head Attention is Secretly a Long Context Processor", "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"], "ML": ["Large Language Models for Data Annotation and Synthesis: A Survey", "A Survey on In-context Learning", "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "Understanding \u201cDemocratization\u201d in NLP and ML Research", "C3PA: An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits", "Verba volant, scripta volant? Don't worry! There are computational solutions for protoword reconstruction", "Towards a Greek Proverb Atlas: Computational Spatial Exploration and Attribution of Greek Proverbs", "OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants", "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models", "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models", "Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions", "When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?", "On the Rigour of Scientific Writing: Criteria, Analysis, and Insights", "Knowledge Mechanisms in Large Language Models: A Survey and Perspective", "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling"], "Clip": ["EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models", "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification", "RWKV-CLIP: A Robust Vision-Language Representation Learner", "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP", "If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions", "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models", "Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification", "MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Updating CLIP to Prefer Descriptions Over Captions", "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought", "Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP", "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models", "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning", "VPL: Visual Proxy Learning Framework for Zero-Shot Medical Image Diagnosis", "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP"], "Question Generation": ["QUDSELECT: Selective Decoding for Questions Under Discussion Parsing", "Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages", "Paraphrase Types Elicit Prompt Engineering Capabilities", "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation", "Measuring the Robustness of NLP Models to Domain Shifts", "Translation of Multifaceted Data without Re-Training of Machine Translation Systems", "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain", "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "Evaluation of Question Answer Generation for Portuguese: Insights and Datasets", "Learning to Ask Denotative and Connotative Questions for Knowledge-based VQA", "Learning to Ask Denotative and Connotative Questions for Knowledge-based VQA", "Enable Fast Sampling for Seq2Seq Text Diffusion", "DisGeM: Distractor Generation for Multiple Choice Questions with Span Masking", "Reference-based Metrics Disprove Themselves in Question Generation", "Reference-based Metrics Disprove Themselves in Question Generation"], "Toxicity": ["Evaluating Psychological Safety of Large Language Models", "LLM See, LLM Do: Leveraging Active Inheritance to Target Non-Differentiable Objectives", "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models", "Data, Data Everywhere: A Guide for Pretraining Dataset Construction", "Style-Specific Neurons for Steering LLMs in Text Style Transfer", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal Mechanisms and the Superficial Hypothesis", "Style-Shifting Behaviour of the Manosphere on Reddit", "SAFETY-J: Evaluating Safety with Critique", "Promoting Constructive Deliberation: Reframing for Receptiveness", "BiasDora: Exploring Hidden Biased Associations in Vision-Language Models", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "Attribute Controlled Fine-tuning for Large Language Models: A Case Study on Detoxification", "Preference Tuning For Toxicity Mitigation Generalizes Across Languages", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression"], "Fact-checking": ["Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "\u201cImage, Tell me your story!\" Predicting the original meta-context of visual misinformation", "MisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"", "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents", "Unknown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data", "Unknown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data", "MIPD: Exploring Manipulation and Intention in A Novel Corpus of Polish Disinformation", "M\u00b3D: MultiModal MultiDocument Fine-Grained Inconsistency Detection", "M\u00b3D: MultiModal MultiDocument Fine-Grained Inconsistency Detection", "Generating Media Background Checks for Automated Source Critical Reasoning", "Multimodal Misinformation Detection by Learning from Synthetic Data with Multimodal LLMs", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers"], "Text-to-sql": ["I Need Help! Evaluating LLM\u2019s Ability to Ask for Users\u2019 Support: A Case Study on Text-to-SQL Generation", "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL", "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL", "Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities", "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL", "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL", "SYNTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA", "SYNTQA: Synergistic Table-based Question Answering via Mixture of Text-to-SQL and E2E TQA", "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization", "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL", "Learning from Imperfect Data: Towards Efficient Knowledge Distillation of Autoregressive Language Models for Text-to-SQL", "DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models", "DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models", "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning"], "Semantic Parsing": ["QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "QUITE: Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios", "Learning to Retrieve Iteratively for In-Context Learning", "Learning to Retrieve Iteratively for In-Context Learning", "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning", "Language-to-Code Translation with a Single Labeled Example", "Language-to-Code Translation with a Single Labeled Example", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations", "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing", "Generate-on-Graph: Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering", "Scope-enhanced Compositional Semantic Parsing for DRT", "Scope-enhanced Compositional Semantic Parsing for DRT", "Learning Semantic Structure through First-Order-Logic Translation", "Predicting generalization performance with correctness discriminators", "Augmenting Reasoning Capabilities of LLMs with Graph Structures in Knowledge Base Question Answering"], "Vqa": ["DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis", "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information", "CommVQA: Situating Visual Question Answering in Communicative Contexts", "CommVQA: Situating Visual Question Answering in Communicative Contexts", "ERVQA: A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments", "In-Context Compositional Generalization for Large Vision-Language Models", "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM", "VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning", "Visual Question Decomposition on Multimodal Large Language Models", "A Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect", "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent", "Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models", "MobileVLM: A Vision-Language Model for Better Intra- and Inter-UI Understanding", "Towards One-to-Many Visual Question Answering", "Improving Adversarial Robustness in Vision-Language Models with Architecture and Prompt Design"], "Security": ["Seeing the Forest through the Trees: Data Leakage from Partial Transformer Gradients", "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model", "Taylor Unswift: Secured Weight Release for Large Language Models via Taylor Expansion", "CLEANGEN: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models", "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models", "Where Am I From? Identifying Origin of LLM-generated Content", "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks", "BaitAttack: Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting", "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking", "Jailbreaking LLMs with Arabic Transliteration and Arabizi", "GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation", "SecureSQL: Evaluating Data Leakage of Large Language Models as Natural Language Interfaces to Databases", "Securing Multi-turn Conversational Language Models From Distributed Backdoor Triggers", "Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models", "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLMs Jailbreakers", "Look Who's Talking Now: Covert Channels From Biased LLMs"], "Benchmarks Dataset": ["NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian", "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction", "DATATALES: A Benchmark for Real-World Intelligent Data Narration", "GLOBESUMM: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization", "Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights", "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering", "DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting", "FAME: Towards Factual Multi-Task Model Editing", "Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis", "Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues", "Do LLMs Know to Respect Copyright Notice?", "Holistic Evaluation for Interleaved Text-and-Image Generation", "Editing Conceptual Knowledge for Large Language Models", "BOOLQUESTIONS: Does Dense Retrieval Understand Boolean Logic in Language?", "Who's Who: Large Language Models Meet Knowledge Conflicts in Practice", "Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?"], "Continual Pre-training": ["Pretraining Language Models Using Translationese", "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale", "A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models", "LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-Training", "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models", "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models", "Task Oriented In-Domain Data Augmentation", "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models", "BSharedRAG: Backbone Shared Retrieval-Augmented Generation for the E-commerce Domain", "Improving Referring Ability for Biomedical Language Models", "Improving Referring Ability for Biomedical Language Models", "Unlocking Continual Learning Abilities in Language Models", "Unlocking the Potential of Model Merging for Low-Resource Languages", "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages", "A Recipe to Train Powerful Romanian LLMs with English Instructions"], "Evaluation Metric": ["Learning to Extract Structured Entities Using Language Models", "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models", "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation", "RaTEScore: A Metric for Radiology Report Generation", "RaTEScore: A Metric for Radiology Report Generation", "Holistic Evaluation for Interleaved Text-and-Image Generation", "A Novel Metric for Measuring the Robustness of Large Language Models in Non-adversarial Scenarios", "LONG2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall", "FAITHSCORE: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models", "Can LLMs Recognize Toxicity? A Structured Investigation Framework and Toxicity Metric", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "Unveiling the Invisible: Captioning Videos with Metaphors", "Introducing Spatial Information and a Novel Evaluation Scheme for Open-Domain Live Commentary Generation", "Reference-based Metrics Disprove Themselves in Question Generation", "Reference-based Metrics Disprove Themselves in Question Generation", "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation"], "Data Generation": ["Table Question Answering for Low-resourced Indic Languages", "Table Question Answering for Low-resourced Indic Languages", "Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors", "FuseGen: PLM Fusion for Data-generation based Zero-shot Learning", "DocKD: Knowledge Distillation from LLMs for Open-World Document Understanding Models", "Incubating Text Classifiers Following User Instruction with Nothing but LLM", "Leveraging pre-trained language models for linguistic analysis: A case of argument structure constructions", "MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs", "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese", "Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts", "Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "Diverse and Effective Synthetic Data Generation for Adaptable Zero-Shot Dialogue State Tracking", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification"], "Debiasing": ["\u201cThinking\u201d Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models", "Overcome Noise and Bias: Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue", "Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias", "Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference", "Decoding Matters: Addressing Amplification Bias and Homogeneity Issue for LLM-based Recommendation", "Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization", "Efficient Overshadowed Entity Disambiguation by Mitigating Shortcut Learning", "Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion", "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models", "Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding", "FAIRFLOW: Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding", "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "A Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect", "Recent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models", "Recent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models"], "Factual Knowledge": ["Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?", "A Thorough Examination of Decoding Methods in the Era of LLMs", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", ">Why Does New Knowledge Create Messy Ripple Effects in LLMs?", "FAC2E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition", "On the Robustness of Editing Large Language Models", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "Knowledge Editing in Language Models via Adapted Direct Preference Optimization", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models", "To Know or Not to Know? Analyzing Self-Consistency of Large Language Models under Ambiguity"], "Explainable Ai": ["Evaluating Readability and Faithfulness of Concept-based Explanations", "Enhancing Advanced Visual Reasoning Ability of Large Language Models", "Generative Models for Automatic Medical Decision Rule Extraction from Text", "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "FINDVER: Explainable Claim Verification over Long and Hybrid-Content Financial Documents", "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments", "Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting", "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning", "The Illusion of Competence: Evaluating the Effect of Explanations on Users' Mental Models of Visual Question Answering Systems", "Adaptable Moral Stances of Large Language Models on Sexist Content: Implications for Society and Gender Discourse", "CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems", "Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach", "Multilingual Fine-Grained News Headline Hallucination Detection", "Improving LLM Attributions with Randomized Path-Integration", "The Language of Trauma: Modeling Traumatic Event Descriptions Across Domains with Explainable AI"], "Multi-modal Learning": ["GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation", "MatchTime: Towards Automatic Soccer Game Commentary Generation", "Enhancing Advanced Visual Reasoning Ability of Large Language Models", "Fine-Grained Prediction of Reading Comprehension from Eye Movements", "GAMA: A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities", "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models", "MOSEL: Inference Serving Using Dynamic Modality Selection", "Multi-Level Information Retrieval Augmented Generation for Knowledge-based Visual Question Answering", "Preserving Multi-Modal Capabilities of Pre-trained VLMS for Improving Vision-Linguistic Compositionality", "RECANTFormer: Referring Expression Comprehension with Varying Numbers of Targets", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "Creative Problem Solving in Large Language and Vision Models \u2013 What Would it Take?", "SaSR-Net: Source-Aware Semantic Representation Network for Enhancing Audio-Visual Question Answering", "Towards One-to-Many Visual Question Answering"], "Asr": ["LLMs Are Zero-Shot Context-Aware Simultaneous Translators", "Towards Robust Speech Representation Learning for Thousands of Languages", "What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations", "Unveiling the Role of Pretraining in Direct Speech Translation", "Self-Powered LLM Modality Expansion for Large Speech-Text Models", "Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech", "Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding", "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR", "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based ASR", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "Are Modern Neural ASR Architectures Robust for Polysynthetic Languages?", "PolyWER: A Holistic Evaluation Framework for Code-Switched Speech Recognition", "Beyond Common Words: Enhancing ASR Cross-Lingual Proper Noun Recognition Using Large Language Models", "STTATTS: Unified Speech-To-Text And Text-To-Speech Model", "Modeling Gender and Dialect Bias in Automatic Speech Recognition"], "Dense Retrieval": ["ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval", "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval", "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment", "Large Language Models as Foundations for Next-Gen Dense Retrieval: A Comprehensive Empirical Assessment", "Unifying Multimodal Retrieval via Document Screenshot Embedding", "Taxonomy-guided Semantic Indexing for Academic Paper Search", "Improve Dense Passage Retrieval with Entailment Tuning", "Language Concept Erasure for Language-invariant Dense Retrieval", "LitSearch: A Retrieval Benchmark for Scientific Literature Search", "Dense X Retrieval: What Retrieval Granularity Should We Use?", "PAIRDISTILL: Pairwise Relevance Distillation for Dense Retrieval", "PAIRDISTILL: Pairwise Relevance Distillation for Dense Retrieval", "Exploring the Best Practices of Query Expansion with Large Language Models", "BOOLQUESTIONS: Does Dense Retrieval Understand Boolean Logic in Language?", "LumberChunker: Long-Form Narrative Document Segmentation"], "Self-consistency": ["GLaPE: Gold Label-agnostic Prompt Evaluation for Large Language Models", "Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison", "Python is Not Always the Best Choice: Embracing Multilingual Program of Thoughts", "Empowering Multi-step Reasoning across Languages via Program-Aided Language Models", "Atomic Self-Consistency for Better Long Form Generations", "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models", "Tree of Problems: Improving structured problem solving with compositionality", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction", "Self-Consistency Boosts Calibration for Math Reasoning", "ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees", "Calibrating Long-form Generations from Large Language Models", "Regression-aware Inference with LLMs", "To Know or Not to Know? Analyzing Self-Consistency of Large Language Models under Ambiguity"], "Prompt Tuning": ["Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "Divide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning", "StablePrompt: Automatic Prompt Tuning using Reinforcement Learning for Large Language Models", "Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment", "IntCoOp: Interpretability-Aware Vision-Language Prompt Tuning", "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators", "Understanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs", "BiKT: Enabling Bidirectional Knowledge Transfer Between Pretrained Models and Sequential Downstream Tasks", "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning", "Achieving Stronger Generation via Simple Contrastive Tuning", "StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation", "LLM as a metric critic for low resource relation identification", "Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization", "ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning"], "Math Reasoning": ["Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models", "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors", "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "Fewer is More: Boosting Math Reasoning with Reinforced Context Pruning", "SCIAGENT: Tool-augmented Language Models for Scientific Reasoning", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "Can LLMs Learn From Mistakes? An Empirical Study on Reasoning Tasks", "MathFish: Evaluating Language Model Math Reasoning via Grounding in Educational Curricula", "Self-Consistency Boosts Calibration for Math Reasoning", "MUSCLE: A Model Update Strategy for Compatible LLM Evolution", "Unlocking the Potential of Model Merging for Low-Resource Languages", "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning"], "Knowledge Retrieval": ["EfficientRAG: Efficient Retriever for Multi-Hop Question Answering", "Dissecting Fine-Tuning Unlearning in Large Language Models", "Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System", "Relevance Is a Guiding Light: Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System", "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling", "SCIPROMPT: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics", "Atomic Self-Consistency for Better Long Form Generations", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature", "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance", "Large Language Models are Limited in Out-of-Context Knowledge Reasoning", "QPaug: Question and Passage Augmentation for Open-Domain Question Answering of LLMs", "Learning to Match Representations is Better for End-to-End Task-Oriented Dialog System", "An LLM-Enabled Knowledge Elicitation and Retrieval Framework for Zero-Shot Cross-Lingual Stance Identification", "H-LegalKI: A Hierarchical Legal Knowledge Integration Framework for Legal Community Question Answering"], "Personalization": ["Unsupervised Human Preference Learning", "Unsupervised Human Preference Learning", "PERSONALIZED PIECES: Efficient Personalized Large Language Models through Collaborative Efforts", "Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning", "MisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"", "MisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"", "Are Large Language Models In-Context Personalized Summarizers? Get an iCOPERNICUS Test Done!", "Jump Starting Bandits with LLM-Generated Prior Knowledge", "Jump Starting Bandits with LLM-Generated Prior Knowledge", "EDEN: Empathetic Dialogues for English Learning", "Guided Profile Generation Improves Personalization with LLMs", "BAPO: Base-Anchored Preference Optimization for Overcoming Forgetting in Large Language Models Personalization", "Train Once, Use Flexibly: A Modular Framework for Multi-Aspect Neural News Recommendation", "Can LLM be a Personalized Judge?", "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization"], "Knowledge Transfer": ["Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting", "Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment", "McCrolin: Multi-consistency Cross-lingual Training for Retrieval Question Answering", "ICL: Iterative Continual Learning for Multi-domain Neural Machine Translation", "Infrared-LLaVA: Enhancing Understanding of Infrared Images in Multi-Modal Large Language Models", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "Data-Centric AI in the Age of Large Language Models", "Data-Centric AI in the Age of Large Language Models", "An LLM-Enabled Knowledge Elicitation and Retrieval Framework for Zero-Shot Cross-Lingual Stance Identification", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "DLORA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model", "Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling"], "Visual Grounding": ["VIMI: Grounding Video Generation through Multi-modal Instruction", "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering", "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering", "UOUO: Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models", "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling", "FineCops-Ref: A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension", "Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models", "Show and Guide: Instructional-Plan Grounded Vision and Language Model", "PropTest: Automatic Property Testing for Improved Visual Programming", "PropTest: Automatic Property Testing for Improved Visual Programming", "VDebugger: Harnessing Execution Feedback for Debugging Visual Programs", "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition", "Large Language Models Are Challenged by Habitat-Centered Reasoning"], "Pretraining": ["VIMI: Grounding Video Generation through Multi-modal Instruction", "Unveiling the Role of Pretraining in Direct Speech Translation", "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients", "GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients", "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection", "Getting The Most Out of Your Training Data: Exploring Unsupervised Tasks for Morphological Inflection", "BLSP-Emo: Towards Empathetic Large Speech-Language Models", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "Is Child-Directed Speech Effective Training Data for Language Models?", "Tending Towards Stability: Convergence Challenges in Small Language Models", "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models", "Data-Centric AI in the Age of Large Language Models", "Gradient Localization Improves Lifelong Pretraining of Language Models", "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging"], "Topic Classification": ["SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages", "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning", "Revisiting Supervised Contrastive Learning for Microblog Classification", "CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs", "SYNTHESIZRR: Generating Diverse Datasets with Retrieval Augmentation", "Multilingual Topic Classification in X: Dataset and Analysis", "CoverICL: Selective Annotation for In-Context Learning via Active Graph Coverage", "Transfer Learning for Text Classification via Model Risk Analysis", "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "In-Context Learning with Iterative Demonstration Selection", "Dual-Phase Accelerated Prompt Optimization", "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons", "Inference and Verbalization Functions During In-Context Learning"], "Truthfulness": ["Personas as a Way to Model Truthfulness in Language Models", "Personas as a Way to Model Truthfulness in Language Models", "On the Relationship between Truth and Political Bias in Language Models", "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "On the Universal Truthfulness Hyperplane Inside LLMS", "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models", "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models", "Evaluating Language Model Character Traits", "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully", "Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression", "Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning", "Student Data Paradox and Curious Case of Single Student-Tutor Model: Regressive Side Effects of Training LLMs for Personalized Learning"], "Neural Machine Translation": ["Domain adapted machine translation: What does catastrophic forgetting forget and why?", "Domain adapted machine translation: What does catastrophic forgetting forget and why?", "Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation", "Granularity is crucial when applying differential privacy to text: An investigation for neural machine translation", "Granularity is crucial when applying differential privacy to text: An investigation for neural machine translation", "Contrastive Token Learning with Similarity Decay for Repetition Suppression in Machine Translation", "Dual-teacher Knowledge Distillation for Low-frequency Word Translation", "Together We Can: Multilingual Automatic Post-Editing for Low-Resource Languages", "DICTDIS: Dictionary Constrained Disambiguation for Improved NMT", "DICTDIS: Dictionary Constrained Disambiguation for Improved NMT", "Finding the Optimal Byte-Pair Encoding Merge Operations for Neural Machine Translation in a Low-Resource Setting", "Finding the Optimal Byte-Pair Encoding Merge Operations for Neural Machine Translation in a Low-Resource Setting", "Does Context Help Mitigate Gender Bias in Neural Machine Translation?", "Does Context Help Mitigate Gender Bias in Neural Machine Translation?", "Gloss2Text: Sign Language Gloss translation using LLMs and Semantically Aware Label Smoothing"], "Temporal Reasoning": ["Temporally Consistent Factuality Probing for Large Language Models", "Temporally Consistent Factuality Probing for Large Language Models", "CAT-BENCH: Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans", "Video-Text Prompting for Weakly Supervised Spatio-Temporal Video Grounding", "A Simple LLM Framework for Long-Range Video Question-Answering", "MIBench: Evaluating Multimodal Large Language Models over Multiple Images", "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs", "Exploring Question Guidance and Answer Calibration for Visually Grounded Video Question Answering", "DyKnow: Dynamically Verifying Time-Sensitive Factual Knowledge in LLMs", "Updating Large Language Models' Memories with Time Constraints", "Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question Answering", "Remember This Event That Year? Assessing Temporal Information and Understanding in Large Language Models", "Remember This Event That Year? Assessing Temporal Information and Understanding in Large Language Models", "NARRATIVE-OF-THOUGHT: Improving Temporal Reasoning of Large Language Models via Recounted Narratives", "NARRATIVE-OF-THOUGHT: Improving Temporal Reasoning of Large Language Models via Recounted Narratives"], "Factual Consistency": ["FIZZ: Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document", "EVEDIT: Event-based Knowledge Editing for Deterministic Knowledge Propagation", "ECON: On the Detection and Resolution of Evidence Conflicts", "Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works", "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios", "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "Rationale-Aware Answer Verification by Pairwise Self-Evaluation", "SummaCoz: A Dataset for Improving the Interpretability of Factual Consistency Detection for Summarization", "Learning from Implicit User Feedback, Emotions and Demographic Information in Task-Oriented and Document-Grounded Dialogues", "Learning from Implicit User Feedback, Emotions and Demographic Information in Task-Oriented and Document-Grounded Dialogues", "Improving Factual Consistency of News Summarization by Contrastive Preference Optimization", "Improving Factual Consistency of News Summarization by Contrastive Preference Optimization", "Learning to Refine with Fine-Grained Natural Language Feedback"], "Transferability": ["Prompts have evil twins", "DA\u00b3: A Distribution-Aware Adversarial Attack against Language Models", "ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings", "Concept-skill Transferability-based Data Selection for Large Vision-Language Models", "When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models", "Demystifying Verbatim Memorization in Large Language Models", "Rethinking the Evaluation of In-Context Learning for LLMs", "MIXTURE-OF-SKILLS: Learning to Optimize Data Usage for Fine-Tuning Large Language Models", "Extracting Prompts by Inverting LLM Outputs", "Instruction Matters: A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks", "On the Fragility of Active Learners for Text Classification", "Transferability of Syntax-Aware Graph Neural Networks in Zero-Shot Cross-Lingual Semantic Role Labeling", "Adversarial Math Word Problem Generation", "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for Faithfulness and Transferability"], "Relevance": ["Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing", "Be Helpful but Don't Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support", "Evaluating D-MERIT of Partial-annotation on Information Retrieval", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models", "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "Improve Dense Passage Retrieval with Entailment Tuning", "QGEval: Benchmarking Multi-dimensional Evaluation for Question Generation", "Topic-Oriented Open Relation Extraction with A Priori Seed Generation", "Learning to Rank Salient Content for Query-focused Summarization", "Model-based Preference Optimization in Abstractive Summarization without Human Feedback", "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics", "Factuality of Large Language Models: A Survey", "Threshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval"], "Gpt-4": ["GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation", "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models", "Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use", "Leveraging pre-trained language models for linguistic analysis: A case of argument structure constructions", "Annotation alignment: Comparing LLM and human annotations of conversational safety", "Story Morals: Surfacing value-driven narrative schemas using large language models", "LitSearch: A Retrieval Benchmark for Scientific Literature Search", "Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4", "The Death and Life of Great Prompts: Analyzing the Evolution of LLM Prompts from the Structural Perspective", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "SODA-EVAL: Open-Domain Dialogue Evaluation in the age of LLMs", "Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?", "Large Language Models for Propaganda Span Annotation", "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMS"], "Evaluation Benchmarks": ["SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation", "SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation", "AKEW: Assessing Knowledge Editing in the Wild", "AKEW: Assessing Knowledge Editing in the Wild", "MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models", "LONGGENBENCH: Long-context Generation Benchmark", "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification", "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs", "Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts", "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style Transfer", "TWBias: A Benchmark for Assessing Social Bias in Traditional Chinese Large Language Models through a Taiwan Cultural Lens", "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-Checkers"], "Mechanistic Interpretability": ["Neuron-Level Knowledge Attribution in Large Language Models", "Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis", "Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models", "Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning", "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective", "Towards Interpretable Sequence Continuation: Analyzing Shared Circuits in Large Language Models", "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis", "Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions", "Information Flow Routes: Automatically Interpreting Language Models at Scale", "Deeper Insights Without Updates: The Power of In-Context Learning Over Fine-Tuning", "Mechanistic Understanding and Mitigation of Language Model Non-Factual Hallucinations", "Activation Scaling for Steering and Interpreting Language Models", "On the Similarity of Circuits across Languages: A Case Study on the Subject-verb Agreement Task", "Preference Tuning For Toxicity Mitigation Generalizes Across Languages"], "Preference Alignment": ["CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering", "Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model", "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment", "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective", "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a Feature Decorrelation Perspective", "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering", "ORPO: Monolithic Preference Optimization without Reference Model", "Themis: A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability", "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization", "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "Self-Renewal Prompt Optimizing with Implicit Reasoning", "Make Compound Sentences Simple to Analyze: Learning to Split Sentences for Aspect-based Sentiment Analysis", "Self-supervised Preference Optimization: Enhance Your Language Model with Preference Degree Awareness"], "Knowledge Representation": ["CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering", "SATYRN: A Platform for Analytics Augmented Generation", "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models", "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "Unraveling Babel: Exploring Multilingual Activation Patterns of LLMs and Their Applications", "KAR\u00b3L: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students", "Inference Helps PLMs' Conceptual Understanding: Improving the Abstract Inference Ability with Hierarchical Conceptual Entailment Graphs", "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs", "Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study", "Enhancing Incremental Summarization with Structured Representations", "Enhancing Incremental Summarization with Structured Representations", "CONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules", "Dense Passage Retrieval: Is it Retrieving?", "Explicit Inductive Inference using Large Language Models"], "Video QA": ["Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting", "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection", "Encoding and Controlling Global Semantics for Long-form Video Question Answering", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge", "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning", "A Simple LLM Framework for Long-Range Video Question-Answering", "Training-free Deep Concept Injection Enables Language Models for Video Question Answering", "Enhancing Temporal Modeling of Video LLMs via Time Gating", "Enhancing Temporal Modeling of Video LLMs via Time Gating", "Exploring Question Guidance and Answer Calibration for Visually Grounded Video Question Answering", "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs", "ALANAVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding", "ALANAVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding"], "Multilingual Llms": ["Teaching LLMs to Abstain across Languages via Multilingual Feedback", "An Analysis of Multilingual FActScore", "Concept Space Alignment in Multilingual LLMs", "Understanding and Mitigating Language Confusion in LLMs", "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs", "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?", "Exploring Multilingual Concepts of Human Values in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?", "Pruning Multilingual Large Language Models for Multilingual Inference", "Preference Tuning For Toxicity Mitigation Generalizes Across Languages", "Representational Isomorphism and Alignment of Multilingual Large Language Models", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language", "How Does Quantization Affect Multilingual LLMs?", "How Does Quantization Affect Multilingual LLMs?"], "Generative Models": ["Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP", "Generative Models for Automatic Medical Decision Rule Extraction from Text", "Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension", "An image speaks a thousand words, but can everyone listen? On image transcreation for cultural relevance", "Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments", "Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems", "AudioVSR: Enhancing Video Speech Recognition with Audio Data", "GRIZAL: Generative Prior-guided Zero-Shot Temporal Action Localization", "Altogether: Image Captioning via Re-aligning Alt-text", "GREEN: Generative Radiology Report Evaluation and Error Notation", "MINERS: Multilingual Language Models as Semantic Retrievers", "Compare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY", "Generate then Refine: Data Augmentation for Zero-shot Intent Detection", "Enhancing Short-Text Topic Modeling with LLM-Driven Context Expansion and Prefix-Tuned VAEs"], "Grammatical Error Correction": ["A Survey of AMR Applications", "Multi-pass Decoding for Grammatical Error Correction", "Multi-pass Decoding for Grammatical Error Correction", "LLM-based Code-Switched Text Generation for Grammatical Error Correction", "LLM-based Code-Switched Text Generation for Grammatical Error Correction", "To Err Is Human, but Llamas Can Learn It Too", "To Err Is Human, but Llamas Can Learn It Too", "Towards Explainable Chinese Native Learner Essay Fluency Assessment: Dataset, Tasks, and Method", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language", "To Ask LLMs about English Grammaticality, Prompt Them in a Different Language", "Gazelle: An Instruction Dataset for Arabic Writing Assistance", "Gazelle: An Instruction Dataset for Arabic Writing Assistance", "Efficient and Interpretable Grammatical Error Correction with Mixture of Experts", "Efficient and Interpretable Grammatical Error Correction with Mixture of Experts"], "Watermarking": ["POSTMARK: A Robust Blackbox Watermark for Large Language Models", "POSTMARK: A Robust Blackbox Watermark for Large Language Models", "Where Am I From? Identifying Origin of LLM-generated Content", "Where Am I From? Identifying Origin of LLM-generated Content", "Revisiting the Robustness of Watermarking to Paraphrasing Attacks", "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models", "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models", "GuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack", "GuardEmb: Dynamic Watermark for Safeguarding Large Language Model Embedding Service Against Model Stealing Attack", "CODEIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code", "CODEIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code", "A Survey on Detection of LLMs-Generated Content", "Downstream Trade-offs of a Family of Text Watermarks", "Look Who's Talking Now: Covert Channels From Biased LLMs"], "Link Prediction": ["MQuinE: a Cure for \"Z-paradox\" in Knowledge Graph Embedding", "MQuinE: a Cure for \"Z-paradox\" in Knowledge Graph Embedding", "MoCoKGC: Momentum Contrast Entity Encoding for Knowledge Graph Completion", "ATAP: Automatic Template-Augmented Commonsense Knowledge Graph Completion via Pre-Trained Language Models", "Can Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction", "Can Large Language Models Enhance Predictions of Disease Progression? Investigating Through Disease Network Link Prediction", "Predictive Multiplicity of Knowledge Graph Embeddings in Link Prediction", "Predictive Multiplicity of Knowledge Graph Embeddings in Link Prediction", "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs", "Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs", "Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning", "OpenGraph: Towards Open Graph Foundation Models", "Llamipa: An Incremental Discourse Parser", "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding"], "Mmlu": ["Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs", "MODULA: Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning", "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement", "Reasoning Robustness of LLMs to Adversarial Typographical Errors", "Turn Waste into Worth: Rectifying Top-k Router of MoE", "LLM-Evolve: Evaluation for LLM's Evolving Capability on Benchmarks", "Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling", "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance", "The Base-Rate Effect on LLM Benchmark Performance: Disambiguating Test-Taking Strategies from Benchmark Performance", "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization", "LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement", "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning"], "Perplexity": ["A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers", "ScalingFilter: Assessing Data Quality through Inverse Utilization of Scaling Laws", "Evaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding", "I Learn Better If You Speak My Language: Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses", "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models", "Social Bias Probing: Fairness Benchmarking for Language Models", "How to Compute the Probability of a Word", "Who is better at math, Jenny or Jingzhen? Uncovering Stereotypes in Large Language Models", "Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?", "Scalable Data Ablation Approximations for Language Models through Modular Training and Merging", "Adversarial Text Generation using Large Language Models for Dementia Detection", "Monotonic Paraphrasing Improves Generalization of Language Model Prompting", "MoE-I2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition"], "Meta-evaluation": ["Evaluating Readability and Faithfulness of Concept-based Explanations", "Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments", "APPLS: Evaluating Evaluation Metrics for Plain Language Summarization", "Revisiting Automated Evaluation for Long-form Table Question Answering", "Revisiting Automated Evaluation for Long-form Table Question Answering", "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists", "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "OffsetBias: Leveraging Debiased Data for Tuning Evaluators", "SAFETY-J: Evaluating Safety with Critique", "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?", "Compare without Despair: Reliable Preference Evaluation with Generation SEPARABILITY", "A Critical Look at Meta-evaluating Summarisation Evaluation Metrics", "A Critical Look at Meta-evaluating Summarisation Evaluation Metrics"], "Dataset Creation": ["COCOLOFA: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds", "Susu Box or Piggy Bank: Assessing Cultural Commonsense Knowledge between Ghana and the U.S.", "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures", "Casablanca: Data and Models for Multidialectal Arabic Speech Recognition", "Emosical: An Emotion-Annotated Musical Theatre Dataset", "Forecasting Future International Events: A Reliable Dataset for Text-Based Event Modeling", "Detecting Temporal Ambiguity in Questions", "From Pixels to Personas: Investigating and Modeling Self-Anthropomorphism in Human-Robot Dialogues", "Ask the experts: Sourcing a high-quality nutrition counseling dataset through Human-AI collaboration", "Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation", "CACTUS: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory", "ARTS: Assessing Readability & Text Simplicity", "CoCoHD: Congress Committee Hearing Dataset"], "Red Teaming": ["FLIRT: Feedback Loop In-context Red Teaming", "FLIRT: Feedback Loop In-context Red Teaming", "Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models", "Red Teaming Language Models for Processing Contradictory Dialogues", "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction", "Distract Large Language Models for Automatic Jailbreak Attack", "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference", "STAR: SocioTechnical Approach to Red Teaming Language Models", "STAR: SocioTechnical Approach to Red Teaming Language Models", "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming", "Be a Multitude to Itself: A Prompt Evolution Framework for Red Teaming", "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors"], "Error Detection": ["Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections", "ADASWITCH: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning", "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors", "Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors", "Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing", "Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing", "STORYSUMM: Evaluating Faithfulness in Story Summarization", "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors", "Can Automatic Metrics Assess High-Quality Translations?", "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check", "Bi-DCSpell: A Bi-directional Detector-Corrector Interactive Framework for Chinese Spelling Check", "Knowledge-based Consistency Testing of Large Language Models", "Learning to Refine with Fine-Grained Natural Language Feedback"], "Large Multimodal Models": ["Mitigating Language Bias of LMMs in Social Intelligence Understanding with Virtual Counterfactual Calibration", "Towards Low-Resource Harmful Meme Detection with LMM Agents", "TraveLER: A Modular Multi-LMM Agent Framework for Video Question-Answering", "Towards Probing Speech-Specific Risks in Large Multimodal Models: A Taxonomy, Benchmark, and Insights", "UNICORN: A Unified Causal Video-Oriented Language-Modeling Framework for Temporal Video-Language Tasks", "DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding", "Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant", "An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models", "MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification", "MULTISKILL: Evaluating Large Multimodal Models for Fine-grained Alignment Skills", "M5 \u2013 A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks", "Recent Advances in Online Hate Speech Moderation: Multimodality and the Role of Large Models", "Navigating Hallucinations for Reasoning of Unintentional Activities"], "Causal Inference": ["Mitigating Language Bias of LMMs in Social Intelligence Understanding with Virtual Counterfactual Calibration", "Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference", "LLMs Are Prone to Fallacies in Causal Inference", "LLMs Are Prone to Fallacies in Causal Inference", "SLANG: New Concept Comprehension of Large Language Models", "SLANG: New Concept Comprehension of Large Language Models", "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments", "AutoPersuade: A Framework for Evaluating and Explaining Persuasive Arguments", "The Odyssey of Commonsense Causality: From Foundational Benchmarks to Cutting-Edge Reasoning", "Dual-oriented Disentangled Network with Counterfactual Intervention for Multimodal Intent Detection", "A Robust Dual-debiasing VQA Model based on Counterfactual Causal Effect", "Do LLMs Think Fast and Slow? A Causal Study on Sentiment Analysis", "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective"], "Trustworthiness": ["Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?", "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation", "Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?", "Knowledge Conflicts for LLMs: A Survey", "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation", "Factuality of Large Language Models: A Survey", "Enhancing Healthcare LLM Trust with Atypical Presentations Recalibration", "Generating Media Background Checks for Automated Source Critical Reasoning", "In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models", "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate", "Knowledge Mechanisms in Large Language Models: A Survey and Perspective", "TrustAgent: Towards Safe and Trustworthy LLM-based Agents", "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection"], "Chinese": ["Do We Need Language-Specific Fact-Checking Models? The Case of Chinese", "ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations", "Gold Panning in Vocabulary: An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs", "Re-Evaluating Evaluation for Multilingual Summarization", "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction", "Employing Glyphic Information for Chinese Event Extraction with Vision-Language Model", "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models", "OEE-CFC: A Dataset for Open Event Extraction from Chinese Financial Commentary", "Evaluating Moral Beliefs across LLMs through a Pluralistic Framework", "CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays", "CEAMC: Corpus and Empirical Study of Argument Analysis in Education via LLMs", "Creative and Context-Aware Translation of East Asian Idioms with GPT-4", "CHAmbi: A New Benchmark on Chinese Ambiguity Challenges for Large Language Models"], "Scalability": ["Optimizing Code Retrieval: High-Quality and Scalable Dataset Annotation through Large Language Models", "MisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"", "Bridging Local Details and Global Context in Text-Attributed Graphs", "Working Memory Identifies Reasoning Limits in Language Models", "Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies", "Waterfall: Scalable Framework for Robust Text Watermarking and Provenance for LLMs", "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval", "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning", "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts", "TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models", "Efficient Pointwise-Pairwise Learning-to-Rank for News Recommendation", "HyQE: Ranking Contexts with Hypothetical Query Embeddings", "Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation"], "Generalizability": ["Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models", "On Training Data Influence of GPT Models", "What is \u201cTypological Diversity\u201d in NLP?", "Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing", "VLEU: a Method for Automatic Evaluation for Generalizability of Text-to-Image Models", "SecCoder: Towards Generalizable and Robust Secure Code Generation", "Text Fluoroscopy: Detecting LLM-Generated Text through Intrinsic Features", "LM\u00b2: A Simple Society of Language Models Solves Complex Reasoning", "Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding", "Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets", "Navigating the Shortcut Maze: A Comprehensive Analysis of Shortcut Learning in Text Classification by Language Models", "PROTRIX: Building Models for Planning and Reasoning over Tables with Sentence Context", "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models"], "Automatic Evaluation": ["Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course", "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course", "Is Safer Better? The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering", "ARXIVDIGESTABLES: Synthesizing Scientific Literature into Tables using Language Models", "More Insightful Feedback for Tutoring: Enhancing Generation Mechanisms and Automatic Evaluation", "What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study", "Error Analysis of Multilingual Language Models in Machine Translation: A Case Study of English-Amharic Translation", "Can AI Relate: Testing Large Language Model Response for Mental Health Support", "Evaluating Automatic Metrics with Incremental Machine Translation Systems", "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?", "ALIGNSUM: Data Pyramid Hierarchical Fine-tuning for Aligning with Human Summarization Preference", "Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?", "CACTUS: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory"], "Downstream Tasks": ["Collaborative Performance Prediction for Large Language Models", "TEMA: Token Embeddings Mapping for Enriching Low-Resource Language Models", "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention", "A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives", "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "Moral Foundations of Large Language Models", "Memory-Efficient Fine-Tuning of Transformers via Token Selection", "Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings", "Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models", "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning?", "Textual Dataset Distillation via Language Model Embedding", "Downstream Trade-offs of a Family of Text Watermarks", "TransferCVLM: Transferring Cross-Modal Knowledge for Vision-Language Modeling"], "Semantic Textual Similarity": ["Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese", "Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss", "Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Scaling Sentence Embeddings with Large Language Models", "Variational Language Concepts for Interpreting Foundation Language Models", "Hit the Nail on the Head: Parameter-Efficient Multi-task Tuning via Human Language Intervention", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity", "MEDCARE: Advancing Medical LLMs through Decoupling Clinical Alignment and Knowledge Aggregation", "Regression-aware Inference with LLMs", "Representational Isomorphism and Alignment of Multilingual Large Language Models"], "Attribution": ["PhiloGPT: A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study", "Advancing Large Language Model Attribution through Self-Improving", "Attribute or Abstain: Large Language Models as Long Document Assistants", "AGRAME: Any-Granularity Ranking with Multi-Vector Embeddings", "Where Am I From? Identifying Origin of LLM-generated Content", "Latent Concept-based Explanation of NLP Models", "Analysis of Plan-based Retrieval for Grounded Text Generation", "Information Flow Routes: Automatically Interpreting Language Models at Scale", "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity", "Improving LLM Attributions with Randomized Path-Integration", "LLM Explainability via Attributive Masking Learning", "LLM Explainability via Attributive Masking Learning", "Analyzing Context Contributions in LLM-based Machine Translation"], "Inference": ["QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models", "QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models", "EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees", "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance", "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy", "LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference", "Normalized Narrow Jump To Conclusions: Normalized Narrow Shortcuts for Parameter Efficient Early Exit Transformer Prediction", "Fast Matrix Multiplications for Lookup Table-Quantized LLMs", "QEFT: Quantization for Efficient Fine-Tuning of LLMs", "QEFT: Quantization for Efficient Fine-Tuning of LLMs", "XC-CACHE: Cross-Attending to Cached Context for Efficient LLM Inference", "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression", "Inference and Verbalization Functions During In-Context Learning"], "Misinformation": ["F2RL: Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation", "ECON: On the Detection and Resolution of Evidence Conflicts", "MisinfoEval: Generative AI in the Era of \u201cAlternative Facts\"", "\"Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models", "On the Relationship between Truth and Political Bias in Language Models", "Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation", "Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach", "Can Large Language Models Identify Authorship?", "SAFETY-J: Evaluating Safety with Critique", "Can Language Models Recognize Convincing Arguments?", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs", "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation"], "Self-training": ["LogicST: A Logical Self-Training Framework for Document-Level Relation Extraction with Incomplete Annotations", "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models", "Re-ReST: Reflection-Reinforced Self-Training for Language Agents", "Self-Training Large Language and Vision Assistant for Medical Question-Answering", "Can LLMs Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner", "SELF-EXPLORE: Enhancing Mathematical Reasoning in Language Models with Fine-grained Rewards", "Semi-Supervised Reward Modeling via Iterative Self-Training", "Self-training Language Models for Arithmetic Reasoning", "Self-training Language Models for Arithmetic Reasoning", "Self-training Large Language Models through Knowledge Detection", "Self-training Large Language Models through Knowledge Detection"], "Open-domain Qa": ["REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering", "Towards Verifiable Text Generation with Evolving Memory and Self-Reflection", "AGRAME: Any-Granularity Ranking with Multi-Vector Embeddings", "Improve Dense Passage Retrieval with Entailment Tuning", "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models", "Dense X Retrieval: What Retrieval Granularity Should We Use?", "Dense X Retrieval: What Retrieval Granularity Should We Use?", "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation", "RaFe: Ranking Feedback Improves Query Rewriting for RAG", "Learning to Paraphrase for Alignment with LLM Preference", "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts", "Detecting Temporal Ambiguity in Questions", "ConTReGen: Context-driven Tree-structured Retrieval for Open-domain Long-form Text Generation"], "Reranking": ["FIRST: Faster Improved Listwise Reranking with Single Token Decoding", "FIRST: Faster Improved Listwise Reranking with Single Token Decoding", "Enhancing High-order Interaction Awareness in LLM-based Recommender Model", "Pcc-tuning: Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity", "Searching for Best Practices in Retrieval-Augmented Generation", "PAIRDISTILL: Pairwise Relevance Distillation for Dense Retrieval", "Comparing Neighbors Together Makes it Easy: Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval", "EchoSight: Advancing Visual-Language Models with Wiki Knowledge", "Toolken+: Improving LLM Tool Usage with Reranking and a Reject Option", " FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking ", "Exploring Hint Generation Approaches in Open-Domain Question Answering", "A Decoding Algorithm for Length-Control Summarization Based on Directed Acyclic Transformers", "TINYSTYLER: Efficient Few-Shot Text Style Transfer with Authorship Embeddings"], "Stance Detection": ["The Lou Dataset: Exploring the Impact of Gender-Fair Language in German Text Classification", "Diversity Over Size: On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets", "Enhancing Data Quality through Simple De-duplication: Navigating Responsible Computational Social Science Research", "MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification", "I love pineapple on pizza != I hate pineapple on pizza: Stance-Aware Sentence Transformers for Opinion Mining", "I love pineapple on pizza != I hate pineapple on pizza: Stance-Aware Sentence Transformers for Opinion Mining", "Stanceformer: Target-Aware Transformer for Stance Detection", "Stanceformer: Target-Aware Transformer for Stance Detection", "Toeing the Party Line: Election Manifestos as a Key to Understand Political Discourse on Twitter", "Toeing the Party Line: Election Manifestos as a Key to Understand Political Discourse on Twitter", "An LLM-Enabled Knowledge Elicitation and Retrieval Framework for Zero-Shot Cross-Lingual Stance Identification", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models", "How to Train Your Fact Verifier: Knowledge Transfer with Multimodal Open Models"]};
    window.addEventListener('DOMContentLoaded', () => {
        const plotDiv = document.querySelector(".plotly-graph-div");

        const titleBox = document.createElement("div");
        titleBox.id = "titleBox";
        titleBox.style.width = "90%";
        titleBox.style.maxHeight = "300px";
        titleBox.style.margin = "20px auto";
        titleBox.style.border = "1px solid gray";
        titleBox.style.overflowY = "scroll";
        titleBox.style.padding = "10px";
        titleBox.style.fontSize = "16px";
        titleBox.innerHTML = "Click the Bars! You can see papers of the specific keyword";
        document.body.appendChild(titleBox);

        plotDiv.on('plotly_click', function(data) {
            const displayLabel = data.points[0].x;
            const titles = ngramMap[displayLabel] || [];
            let content = `<b style='font-size:18px;'>${displayLabel}</b><br><br>`;
            titles.forEach((t, i) => {
                content += `${i+1}. ${t}<br>`;
            });
            titleBox.innerHTML = content;
        });
    });
    </script>
    </body></html>
